import sys
import psutil
import json
import os
from datetime import datetime
from typing import List, Tuple, Dict, Optional, Any
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import cv2
from sklearn.linear_model import LinearRegression
from sklearn.manifold import TSNE
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
import gradio as gr
import logging
import re

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª–∞—Å—Å—ã –∏–∑ ReportLab
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle
from reportlab.lib.units import inch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

from core_config import (
    DBSCAN_EPSILON, DBSCAN_MIN_SAMPLES,
    ANOMALY_DETECTION_THRESHOLDS, IMAGE_QUALITY_THRESHOLDS, RESULTS_DIR,
    MASK_DETECTION_THRESHOLDS, PUTIN_BIRTH_DATE, GRADIO_INTERFACE_SETTINGS, GRADIO_DEFAULTS,
    get_chronological_analysis_parameters # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–∏
)
from data_manager import DataManager
from face_3d_analyzer import Face3DAnalyzer
from embedding_analyzer import EmbeddingAnalyzer
from texture_analyzer import TextureAnalyzer
from temporal_analyzer import TemporalAnalyzer
from anomaly_detector import AnomalyDetector


class OptimizedGradioInterface:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.max_concurrent_processes = GRADIO_INTERFACE_SETTINGS['MAX_CONCURRENT_PROCESSES']
        self.batch_size = GRADIO_INTERFACE_SETTINGS['BATCH_SIZE']
        self.max_gallery_items = GRADIO_INTERFACE_SETTINGS['MAX_GALLERY_ITEMS']
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        self._initialize_analyzers()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.analysis_state = {
            'processed_count': 0,
            'total_count': 0,
            'current_batch': 0,
            'results': {},
            'errors': [],
            'processing': False
        }
        
        # –ö—ç—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.results_cache = {}
        self.visualization_cache = {}
        
    def _initialize_analyzers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            self.data_manager = DataManager()
            self.face_analyzer = Face3DAnalyzer()
            self.embedding_analyzer = EmbeddingAnalyzer()
            self.embedding_analyzer.initialize_insightface_model() # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ InsightFace
            self.texture_analyzer = TextureAnalyzer()
            self.temporal_analyzer = TemporalAnalyzer(self.data_manager) # –ü–µ—Ä–µ–¥–∞—á–∞ data_manager
            self.anomaly_detector = AnomalyDetector()
            logger.info("–í—Å–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤: {e}")
            raise

    def create_optimized_interface(self):
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
        
        with gr.Blocks(
            title="–°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –ª–∏—Ü–µ–≤–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è",
            theme=gr.themes.Soft(),
            css=self._get_custom_css()
        ) as demo:
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Ç–∞—Ç—É—Å
            gr.Markdown("# üî¨ –°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –ª–∏—Ü–µ–≤–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏")
            gr.Markdown("**–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ 3DDFA_V2, InsightFace –∏ –Ω–∞—É—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤**")
            
            # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
            with gr.Row():
                processing_status = gr.HTML(
                    value="<div class='status-ready'>‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ</div>",
                    elem_id="global_status"
                )
                progress_bar = gr.Progress()
            
            with gr.Tabs() as main_tabs:
                
                # –í–∫–ª–∞–¥–∫–∞ 1: –ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
                with gr.TabItem("üìÅ –ú–∞—Å—Å–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", id="batch_processing"):
                    self._create_batch_processing_tab(processing_status, progress_bar)
                
                # –í–∫–ª–∞–¥–∫–∞ 2: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                with gr.TabItem("üë• –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ª–∏—á–Ω–æ—Å—Ç–µ–π", id="clustering"):
                    self._create_clustering_results_tab()
                
                # –í–∫–ª–∞–¥–∫–∞ 3: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
                with gr.TabItem("üìä –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑", id="temporal"):
                    self._create_temporal_analysis_tab()
                
                # –í–∫–ª–∞–¥–∫–∞ 4: –î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å–æ–∫
                with gr.TabItem("üé≠ –î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å–æ–∫", id="mask_detection"):
                    self._create_mask_detection_tab()
                
                # –í–∫–ª–∞–¥–∫–∞ 5: –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
                with gr.TabItem("üìã –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ", id="expert_report"):
                    self._create_expert_report_tab()
                
                # –í–∫–ª–∞–¥–∫–∞ 6: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã
                with gr.TabItem("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏", id="settings"):
                    self._create_settings_tab()
            
            return demo

    def _create_batch_processing_tab(self, processing_status, progress_bar):
        """–í–∫–ª–∞–¥–∫–∞ –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
                
                # –í—ã–±–æ—Ä —Ñ–∞–π–ª–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
                file_input = gr.File(
                    file_count="multiple",
                    file_types=["image"],
                    label=f"–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–¥–æ {GRADIO_INTERFACE_SETTINGS['MAX_FILE_UPLOAD_COUNT']} —Ñ–∞–π–ª–æ–≤)",
                    height=200
                )
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                with gr.Accordion("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏", open=False):
                    batch_size_slider = gr.Slider(
                        minimum=10, maximum=100, value=GRADIO_INTERFACE_SETTINGS['BATCH_SIZE'], step=10,
                        label="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"
                    )
                    max_workers_slider = gr.Slider(
                        minimum=1, maximum=8, value=GRADIO_INTERFACE_SETTINGS['MAX_CONCURRENT_PROCESSES'], step=1,
                        label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤"
                    )
                    quality_threshold = gr.Slider(
                        minimum=0.3, maximum=1.0, value=IMAGE_QUALITY_THRESHOLDS['DEFAULT_QUALITY_THRESHOLD'], step=0.1,
                        label="–ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
                    )
                
                # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                with gr.Row():
                    start_processing_btn = gr.Button(
                        "üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", 
                        variant="primary", 
                        size="lg"
                    )
                    stop_processing_btn = gr.Button(
                        "‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", 
                        variant="stop",
                        visible=False
                    )
                
            with gr.Column(scale=2):
                gr.Markdown("### üìä –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                
                # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
                progress_info = gr.JSON(
                    label="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                    value={
                        "total_files": 0,
                        "processed": 0,
                        "successful": 0,
                        "errors": 0,
                        "current_batch": 0,
                        "estimated_time_remaining": "00:00:00"
                    }
                )
                
                # –õ–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
                processing_log = gr.Textbox(
                    label="–õ–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π)",
                    lines=15,
                    max_lines=15,
                    interactive=False,
                    autoscroll=True
                )
                
                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                preview_results = gr.JSON(
                    label="–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
                    visible=False
                )
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
        start_processing_btn.click(
            fn=self._start_batch_processing,
            inputs=[
                file_input, 
                batch_size_slider, 
                max_workers_slider, 
                quality_threshold
            ],
            outputs=[
                processing_status,
                progress_info,
                processing_log,
                start_processing_btn,
                stop_processing_btn,
                preview_results
            ],
            queue=True
        )
        
        stop_processing_btn.click(
            fn=self._stop_processing,
            outputs=[
                processing_status,
                start_processing_btn,
                stop_processing_btn
            ]
        )

    def _create_clustering_results_tab(self):
        """–í–∫–ª–∞–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### üéØ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π")
                
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã DBSCAN
                with gr.Group():
                    gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã DBSCAN**")
                    epsilon_slider = gr.Slider(
                        minimum=0.1, maximum=1.0, value=GRADIO_DEFAULTS['DBSCAN_EPSILON_DEFAULT'], step=0.01,
                        label="Epsilon (–ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞)"
                    )
                    min_samples_slider = gr.Slider(
                        minimum=2, maximum=10, value=GRADIO_DEFAULTS['DBSCAN_MIN_SAMPLES_DEFAULT'], step=1,
                        label="–ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤"
                    )
                
                # –§–∏–ª—å—Ç—Ä—ã
                with gr.Group():
                    gr.Markdown("**–§–∏–ª—å—Ç—Ä—ã**")
                    confidence_filter = gr.Slider(
                        minimum=0.0, maximum=1.0, value=GRADIO_DEFAULTS['CONFIDENCE_FILTER_DEFAULT'], step=0.1,
                        label="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
                    )
                    date_range = gr.DateTime(
                        label="–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç",
                        type="datetime",
                        include_time=False
                    )
                
                recalculate_btn = gr.Button(
                    "üîÑ –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã", 
                    variant="secondary"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                
                # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                cluster_summary = gr.DataFrame(
                    headers=[
                        "ID –∫–ª–∞—Å—Ç–µ—Ä–∞", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ", "–ü–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", 
                        "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", "–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–°—Ç–∞—Ç—É—Å"
                    ],
                    label="–°–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º"
                )
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                cluster_visualization = gr.Plot(
                    label="–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (t-SNE)"
                )
        
        with gr.Row():
            # –ì–∞–ª–µ—Ä–µ—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
            cluster_gallery = gr.Gallery(
                label="–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
                columns=5,
                rows=4,
                height=400,
                show_label=True
            )
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        with gr.Row():
            with gr.Column():
                selected_cluster_info = gr.JSON(
                    label="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ"
                )
            with gr.Column():
                cluster_timeline = gr.Plot(
                    label="–í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞"
                )
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        recalculate_btn.click(
            fn=self._recalculate_clusters,
            inputs=[epsilon_slider, min_samples_slider, confidence_filter, date_range],
            outputs=[cluster_summary, cluster_visualization, cluster_gallery, selected_cluster_info]
        )

    def _create_temporal_analysis_tab(self):
        """–í–∫–ª–∞–¥–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ‚è∞ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
                
                # –í—ã–±–æ—Ä –ª–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                identity_selector = gr.Dropdown(
                    label="–í—ã–±–µ—Ä–∏—Ç–µ –ª–∏—á–Ω–æ—Å—Ç—å",
                    choices=[],
                    interactive=True
                )
                
                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                temporal_resolution = gr.Radio(
                    choices=["–ú–µ—Å—è—Ü", "–ö–≤–∞—Ä—Ç–∞–ª", "–ì–æ–¥"],
                    value=GRADIO_DEFAULTS['TEMPORAL_RESOLUTION_DEFAULT'],
                    label="–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ"
                )
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                metrics_selector = gr.CheckboxGroup(
                    choices=[
                        "skull_width_ratio", "temporal_bone_angle", 
                        "zygomatic_arch_width", "forehead_height_ratio",
                        "nose_width_ratio", "eye_distance_ratio"
                    ],
                    value=GRADIO_DEFAULTS['TEMPORAL_METRICS_DEFAULT'],
                    label="–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"
                )
                
                analyze_temporal_btn = gr.Button(
                    "üìä –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å", 
                    variant="primary"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
                
                # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                temporal_plot = gr.Plot(
                    label="–ò–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –≤–æ –≤—Ä–µ–º–µ–Ω–∏"
                )
                
                # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö vs —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
                predicted_vs_actual_plot = gr.Plot(
                    label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ vs —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏"
                )

                # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                change_points_plot = gr.Plot(
                    label="–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
                )

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                change_statistics = gr.DataFrame(
                    headers=[
                        "–ú–µ—Ç—Ä–∏–∫–∞", "–°—Ä–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ", "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ",
                        "–¢—Ä–µ–Ω–¥", "–ê–Ω–æ–º–∞–ª–∏–∏", "P-value"
                    ],
                    label="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                )
        
        with gr.Row():
            # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            anomaly_detection_plot = gr.Plot(
                label="–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏"
            )
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏
            historical_correlation = gr.Plot(
                label="–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏"
            )
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫
        analyze_temporal_btn.click(
            fn=self._analyze_temporal_patterns,
            inputs=[identity_selector, temporal_resolution, metrics_selector],
            outputs=[
                temporal_plot, 
                change_statistics, 
                anomaly_detection_plot, 
                historical_correlation,
                predicted_vs_actual_plot, # –ù–æ–≤—ã–π –≤—ã–≤–æ–¥
                change_points_plot      # –ù–æ–≤—ã–π –≤—ã–≤–æ–¥
            ]
        )

    def _create_mask_detection_tab(self):
        """–í–∫–ª–∞–¥–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å–æ–∫"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### üé≠ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–µ–∫—Ü–∏–∏")
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
                detection_sensitivity = gr.Slider(
                    minimum=0.1, maximum=1.0, value=GRADIO_DEFAULTS['MASK_DETECTION_SENSITIVITY_DEFAULT'], step=0.1,
                    label="–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏"
                )
                
                technology_levels = gr.CheckboxGroup(
                    choices=list(MASK_DETECTION_THRESHOLDS.keys()), # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–∏ –∏–∑ MASK_DETECTION_THRESHOLDS
                    value=["Level3_Commercial", "Level4_Professional", "Level5_Advanced"],
                    label="–£—Ä–æ–≤–Ω–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞"
                )
                
                analyze_masks_btn = gr.Button(
                    "üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–∞—Å–∫–∏", 
                    variant="primary"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å–æ–∫")
                
                # –°–≤–æ–¥–∫–∞ –ø–æ –º–∞—Å–∫–∞–º
                mask_summary = gr.DataFrame(
                    headers=[
                        "–î–∞—Ç–∞", "–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å",
                        "Shape Error", "Texture Score", "–°—Ç–∞—Ç—É—Å", "–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É"
                    ],
                    label="–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –º–∞—Å–∫–∏"
                )
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º
                technology_distribution = gr.Plot(
                    label="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–∞—Å–æ–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏"
                )
        
        with gr.Row():
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä—ã
            texture_analysis_plot = gr.Plot(
                label="–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"
            )
            
            # –≠–≤–æ–ª—é—Ü–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
            technology_evolution = gr.Plot(
                label="–≠–≤–æ–ª—é—Ü–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–∞—Å–æ–∫"
            )
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        with gr.Row():
            critical_detections = gr.HTML(
                label="–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è",
                value="<div class='alert-info'>–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω</div>"
            )
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫
        analyze_masks_btn.click(
            fn=self._analyze_mask_technology,
            inputs=[detection_sensitivity, technology_levels],
            outputs=[
                mask_summary, 
                technology_distribution, 
                texture_analysis_plot,
                technology_evolution,
                critical_detections
            ]
        )

    def _create_expert_report_tab(self):
        """–í–∫–ª–∞–¥–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç—á–µ—Ç–∞")
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç—á–µ—Ç–∞
                report_type = gr.Radio(
                    choices=["–ö—Ä–∞—Ç–∫–∏–π", "–ü–æ–¥—Ä–æ–±–Ω—ã–π", "–ù–∞—É—á–Ω—ã–π"],
                    value="–ü–æ–¥—Ä–æ–±–Ω—ã–π",
                    label="–¢–∏–ø –æ—Ç—á–µ—Ç–∞"
                )
                
                include_sections = gr.CheckboxGroup(
                    choices=[
                        "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ",
                        "–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è",
                        "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
                        "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑",
                        "–î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å–æ–∫",
                        "–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑",
                        "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
                    ],
                    value=[
                        "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ",
                        "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
                        "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑",
                        "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
                    ],
                    label="–†–∞–∑–¥–µ–ª—ã –æ—Ç—á–µ—Ç–∞"
                )
                
                confidence_threshold_report = gr.Slider(
                    minimum=0.5, maximum=1.0, value=0.85, step=0.05,
                    label="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—ã–≤–æ–¥–æ–≤"
                )
                
                generate_report_btn = gr.Button(
                    "üìÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç", 
                    variant="primary",
                    size="lg"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### üìä –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
                
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                key_metrics = gr.JSON(
                    label="–ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞",
                    value={
                        "total_identities": 0,
                        "confidence_level": 0.0,
                        "statistical_significance": 0.0,
                        "anomalies_detected": 0,
                        "masks_detected": 0,
                        "analysis_period": "N/A"
                    }
                )
                
                # –ò—Ç–æ–≥–æ–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è
                final_timeline = gr.Plot(
                    label="–ò—Ç–æ–≥–æ–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è"
                )
        
        with gr.Row():
            # –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
            expert_conclusion = gr.Textbox(
                label="–≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ",
                lines=20,
                max_lines=30,
                interactive=False,
                placeholder="–ó–¥–µ—Å—å –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ..."
            )
        
        with gr.Row():
            # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with gr.Column():
                export_json_btn = gr.Button("üíæ –≠–∫—Å–ø–æ—Ä—Ç JSON", variant="secondary")
                export_pdf_btn = gr.Button("üìÑ –≠–∫—Å–ø–æ—Ä—Ç PDF", variant="secondary")
            
            with gr.Column():
                download_link = gr.File(
                    label="–§–∞–π–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è",
                    visible=False
                )
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        generate_report_btn.click(
            fn=self._generate_expert_report,
            inputs=[report_type, include_sections, confidence_threshold_report],
            outputs=[key_metrics, final_timeline, expert_conclusion, download_link] # –î–æ–±–∞–≤–ª—è–µ–º download_link
        )
        
        export_json_btn.click(
            fn=self._export_results_json,
            outputs=[download_link]
        )
        
        # TODO: –î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è export_pdf_btn
        export_pdf_btn.click(
            fn=self._export_results_pdf,
            outputs=[download_link]
        )

    def _create_settings_tab(self):
        """–í–∫–ª–∞–¥–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–∏—Å—Ç–µ–º—ã"""
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤")
                
                with gr.Accordion("DBSCAN –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", open=True):
                    dbscan_epsilon = gr.Slider(
                        minimum=0.1, maximum=1.0, value=DBSCAN_EPSILON, step=0.01,
                        label="Epsilon"
                    )
                    dbscan_min_samples = gr.Slider(
                        minimum=1, maximum=10, value=DBSCAN_MIN_SAMPLES, step=1,
                        label="–ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤"
                    )
                
                with gr.Accordion("–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π", open=False):
                    anomaly_z_threshold = gr.Slider(
                        minimum=1.0, maximum=5.0, value=ANOMALY_DETECTION_THRESHOLDS['Z_SCORE_ANOMALY_THRESHOLD'], step=0.1,
                        label="Z-score –ø–æ—Ä–æ–≥"
                    )
                    change_rate_threshold = gr.Slider(
                        minimum=1.0, maximum=10.0, value=ANOMALY_DETECTION_THRESHOLDS['RAPID_CHANGE_STD_MULTIPLIER'], step=0.5,
                        label="–ü–æ—Ä–æ–≥ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                    )
                
                with gr.Accordion("–ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", open=False):
                    min_face_size = gr.Slider(
                        minimum=50, maximum=300, value=IMAGE_QUALITY_THRESHOLDS['MIN_FACE_SIZE'], step=10,
                        label="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ª–∏—Ü–∞ (px)"
                    )
                    blur_threshold = gr.Slider(
                        minimum=50, maximum=200, value=IMAGE_QUALITY_THRESHOLDS['BLUR_DETECTION_THRESHOLD'], step=10,
                        label="–ü–æ—Ä–æ–≥ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏"
                    )
                
                apply_settings_btn = gr.Button("‚úÖ –ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", variant="primary")
                reset_settings_btn = gr.Button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å", variant="secondary")
                
            with gr.Column():
                gr.Markdown("### üìä –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
                
                system_status = gr.JSON(
                    label="–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã",
                    value={
                        "3ddfa_status": "–ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è...",
                        "insightface_status": "–ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è...",
                        "gpu_available": False,
                        "memory_usage": "N/A",
                        "cache_size": "N/A"
                    }
                )
                
                performance_metrics = gr.DataFrame(
                    headers=["–ö–æ–º–ø–æ–Ω–µ–Ω—Ç", "–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º—Å)", "–°—Ç–∞—Ç—É—Å"],
                    label="–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"
                )
                
                # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º
                with gr.Group():
                    gr.Markdown("**–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º**")
                    cache_info = gr.Textbox(
                        label="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ",
                        value="–ö—ç—à –ø—É—Å—Ç",
                        interactive=False
                    )
                    
                    with gr.Row():
                        clear_cache_btn = gr.Button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à", variant="stop")
                        update_status_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å", variant="secondary")
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        apply_settings_btn.click(
            fn=self._apply_system_settings,
            inputs=[
                dbscan_epsilon, dbscan_min_samples,
                anomaly_z_threshold, change_rate_threshold,
                min_face_size, blur_threshold
            ],
            outputs=[system_status]
        )
        
        reset_settings_btn.click(
            fn=lambda: (
                gr.update(value=DBSCAN_EPSILON),
                gr.update(value=DBSCAN_MIN_SAMPLES),
                gr.update(value=ANOMALY_DETECTION_THRESHOLDS['Z_SCORE_ANOMALY_THRESHOLD']),
                gr.update(value=ANOMALY_DETECTION_THRESHOLDS['RAPID_CHANGE_STD_MULTIPLIER']),
                gr.update(value=IMAGE_QUALITY_THRESHOLDS['MIN_FACE_SIZE']),
                gr.update(value=IMAGE_QUALITY_THRESHOLDS['BLUR_DETECTION_THRESHOLD'])
            ),
            outputs=[
                dbscan_epsilon, dbscan_min_samples,
                anomaly_z_threshold, change_rate_threshold,
                min_face_size, blur_threshold
            ]
        )

        clear_cache_btn.click(
            fn=self._clear_system_cache,
            outputs=[cache_info, system_status]
        )
        
        update_status_btn.click(
            fn=self._update_system_status,
            outputs=[system_status, performance_metrics]
        )

    # ==================== –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ –û–ë–†–ê–ë–û–¢–ö–ò ====================

    async def _start_batch_processing(
        self, 
        files: List[str], 
        batch_size: int, 
        max_workers: int, 
        quality_threshold: float
    ):
        """–ó–∞–ø—É—Å–∫ –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        
        if not files:
            return (
                "<div class='status-error'>‚ùå –§–∞–π–ª—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã</div>",
                {"error": "–§–∞–π–ª—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã"},
                "–û—à–∏–±–∫–∞: –§–∞–π–ª—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã\n",
                gr.update(visible=True),
                gr.update(visible=False),
                gr.update(value={}, visible=False)
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤
        if len(files) > 1500:
            return (
                "<div class='status-error'>‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ñ–∞–π–ª–æ–≤ (–º–∞–∫—Å–∏–º—É–º 1500)</div>",
                {"error": f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤, –º–∞–∫—Å–∏–º—É–º 1500"},
                f"–û—à–∏–±–∫–∞: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤, –º–∞–∫—Å–∏–º—É–º 1500\n",
                gr.update(visible=True),
                gr.update(visible=False),
                gr.update(value={}, visible=False)
            )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.analysis_state = {
            'processed_count': 0,
            'total_count': len(files),
            'current_batch': 0,
            'results': {},
            'errors': [],
            'processing': True,
            'start_time': datetime.now()
        }
        
        try:
            # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            results = await self._process_files_in_batches(
                files, batch_size, max_workers, quality_threshold
            )
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            status_html = f"<div class='status-success'>‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results['successful'])} –∏–∑ {len(files)} —Ñ–∞–π–ª–æ–≤</div>"
            
            progress_info = {
                "total_files": len(files),
                "processed": len(results['successful']) + len(results['errors']),
                "successful": len(results['successful']),
                "errors": len(results['errors']),
                "current_batch": "–ó–∞–≤–µ—Ä—à–µ–Ω–æ",
                "processing_time": str(datetime.now() - self.analysis_state['start_time'])
            }
            
            log_text = self._format_processing_log(results)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –ª–∏—á–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ _recalculate_clusters
            identity_choices = self.results_cache.get('identity_selector_choices', [])
            # TODO: –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è gr.Dropdown.choices
            # Gradio –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–ø–∏—Å–æ–∫ –≤—ã–±–æ—Ä–æ–≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞, –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –≤ outputs,
            # –µ—Å–ª–∏ —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥—Ä—É–≥–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. 
            # –î–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º stateful –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–ª–∏ 
            # –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π, –≤–æ–∑–º–æ–∂–Ω–æ, —á–µ—Ä–µ–∑ gr.State –∏–ª–∏ –ø–µ—Ä–µ–¥–∞—á—É id –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ gr.Dropdown.update().
            # –í —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ identity_selector_choices –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –≤—ã–∑–æ–≤–µ _create_temporal_analysis_tab.
            
            return (
                status_html,
                progress_info,
                log_text,
                gr.update(visible=False), # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É "–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"
                gr.update(visible=True), # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å"
                gr.update(value=results['summary'], visible=True)
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            return (
                f"<div class='status-error'>‚ùå –û—à–∏–±–∫–∞: {str(e)}</div>",
                {"error": str(e)},
                f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}\n",
                gr.update(visible=True), # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É "–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"
                gr.update(visible=False), # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å"
                gr.update(value={}, visible=False)
            )

    async def _process_files_in_batches(
        self, 
        files: List[str],
        batch_size: int, 
        max_workers: int, 
        quality_threshold: float
    ) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –±–∞—Ç—á–∞–º–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
        
        all_results = {
            'successful': [],
            'errors': [],
            'embeddings': [],
            'landmarks': [],
            'poses': [],
            'full_processed_data': [], # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
            'summary': {}
        }
        
        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –±–∞—Ç—á–∏
        batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for batch_idx, batch in enumerate(batches):
                if not self.analysis_state['processing']:
                    break
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_idx + 1}/{len(batches)}")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
                batch_results = await self._process_single_batch(
                    batch, executor, quality_threshold
                )
                
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                all_results['successful'].extend(batch_results['successful'])
                all_results['errors'].extend(batch_results['errors'])
                all_results['embeddings'].extend(batch_results['embeddings'])
                all_results['landmarks'].extend(batch_results['landmarks'])
                all_results['poses'].extend(batch_results['poses'])
                all_results['full_processed_data'].extend(batch_results['full_processed_data']) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                self.analysis_state['current_batch'] = batch_idx + 1
                self.analysis_state['processed_count'] = len(all_results['successful']) + len(all_results['errors'])
                
                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                gc.collect()
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if all_results['embeddings']:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–Ω–µ –ø—É—Å—Ç—ã–µ –∏ –Ω–µ None)
            valid_embeddings_data = [
                item for item in all_results['embeddings'] 
                if isinstance(item.get('embedding'), np.ndarray) and item['embedding'].size > 0
            ]
            
            if not valid_embeddings_data:
                logger.warning("–ù–µ—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
                all_results['summary'] = {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏'}
                return all_results

            cluster_results = self._perform_clustering(valid_embeddings_data)
            all_results['summary'] = cluster_results
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ cache –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –≤–∫–ª–∞–¥–∫–∞—Ö
            self.results_cache['clustering_results'] = cluster_results
            self.results_cache['full_processed_data'] = all_results['full_processed_data']
        else:
            logger.info("–ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç).")
            all_results['summary'] = {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏'}
        
        return all_results

    async def _process_single_batch(
        self, 
        batch: List[str], 
        executor: ThreadPoolExecutor, 
        quality_threshold: float
    ) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ —Ñ–∞–π–ª–æ–≤"""
        
        batch_results = {
            'successful': [],
            'errors': [],
            'embeddings': [],
            'landmarks': [],
            'poses': [],
            'full_processed_data': [] # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        future_to_file = {
            executor.submit(self._process_single_image, file_path, quality_threshold): file_path
            for file_path in batch
        }
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result(timeout=30)  # –¢–∞–π–º–∞—É—Ç 30 —Å–µ–∫—É–Ω–¥
                
                if result['success']:
                    batch_results['successful'].append(result)
                    batch_results['embeddings'].append({
                        'file_path': file_path,
                        'embedding': result['embedding'],
                        'confidence': result['confidence'],
                        'date': result.get('date'),
                        'metadata': result.get('metadata', {})
                    })
                    batch_results['landmarks'].append({
                        'file_path': file_path,
                        'landmarks_3d': result['landmarks_3d'],
                        'pose_info': result['pose_info']
                    })
                    batch_results['poses'].append(result['pose_info'])
                    batch_results['full_processed_data'].append(result) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                else:
                    batch_results['errors'].append({
                        'file_path': file_path,
                        'error': result['error']
                    })
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}", exc_info=True)
                batch_results['errors'].append({
                    'file_path': file_path,
                    'error': str(e)
                })
        
        return batch_results

    def _process_single_image(self, file_path: str, quality_threshold: float) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = cv2.imread(file_path)
            if image is None:
                return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'}
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —á–µ—Ä–Ω–æ–µ –∏–ª–∏ –ø—É—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∏–∫—Å–µ–ª–µ–π –¥–ª—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (0-255)
            # –ü–æ—Ä–æ–≥ 1.0 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –µ—Å–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤ –º–µ–Ω—å—à–µ 1.0, —Å—á–∏—Ç–∞–µ–º –µ–≥–æ —á–µ—Ä–Ω—ã–º.
            if image.size == 0 or np.mean(image) < 1.0:
                logger.warning(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏ {file_path} —è–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–Ω—ã–º –∏–ª–∏ –ø—É—Å—Ç—ã–º. –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∏–∫—Å–µ–ª–µ–π: {np.mean(image):.2f}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É.")
                return {'success': False, 'error': '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —á–µ—Ä–Ω–æ–µ –∏–ª–∏ –ø—É—Å—Ç–æ–µ'}

            # --- START NEW LOGGING --- 
            logger.info(f"–í _process_single_image: image path={file_path}")
            logger.info(f"–í _process_single_image: image shape={image.shape}, dtype={image.dtype}")
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω—É–ª–µ–π –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–±–ª–µ–º
            if image.size > 0 and image.ndim >= 2 and image.shape[0] >= 5 and image.shape[1] >= 5:
                logger.info(f"–í _process_single_image: –≤–µ—Ä—Ö–Ω–∏–π –ª–µ–≤—ã–π —É–≥–æ–ª image (5x5):\n{image[:5, :5]}")
            else:
                logger.info(f"–í _process_single_image: image —Å–ª–∏—à–∫–æ–º –º–∞–ª –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞.")
            # --- END NEW LOGGING --- 

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_score = self._assess_image_quality(image)
            if quality_score < quality_threshold:
                return {'success': False, 'error': f'–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {quality_score:.2f}'}
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ landmarks
            try:
                raw_landmarks_result = self.face_analyzer.extract_68_landmarks_with_confidence(image)
                
                if not (isinstance(raw_landmarks_result, (list, tuple)) and len(raw_landmarks_result) == 3):
                    raise ValueError(f"extract_68_landmarks_with_confidence –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: –¢–∏–ø={type(raw_landmarks_result)}, –°–æ–¥–µ—Ä–∂–∏–º–æ–µ={raw_landmarks_result}")
                
                landmarks_3d, confidence_scores, image_shape = raw_landmarks_result
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–ª–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤: {e}. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –¥—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞.")
                return {'success': False, 'error': f'–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤: {e}'}
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ª–∞–Ω–¥–º–∞—Ä–∫–∞—Ö
            if not isinstance(landmarks_3d, np.ndarray) or landmarks_3d.size == 0 or \
               np.any(np.isnan(landmarks_3d)) or np.any(np.isinf(landmarks_3d)):
                return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ª–∞–Ω–¥–º–∞—Ä–∫–∏ (—Å–æ–¥–µ—Ä–∂–∞—Ç NaN/Inf –∏–ª–∏ –ø—É—Å—Ç—ã)'}
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑—ã
            pose_info = self.face_analyzer.determine_precise_face_pose(landmarks_3d)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è landmarks
            normalized_landmarks = self.face_analyzer.normalize_landmarks_by_pose_category(
                landmarks_3d, pose_info['pose_category']
            )
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            try:
                embedding, embedding_confidence = self.embedding_analyzer.extract_512d_face_embedding(image)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–ª–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.", exc_info=True)
                return {'success': False, 'error': f'–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}'}

            if embedding.size == 0:
                if embedding_confidence == 0.0:
                    logger.warning(f"–≠–º–±–µ–¥–¥–∏–Ω–≥ –ø—É—Å—Ç –¥–ª—è {file_path}: –õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å.")
                    return {'success': False, 'error': '–õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å'}
                else:
                    logger.warning(f"–≠–º–±–µ–¥–¥–∏–Ω–≥ –ø—É—Å—Ç –¥–ª—è {file_path}: –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {embedding_confidence}.")
                    return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø—Ä–∏—á–∏–Ω–µ'}
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
            identity_metrics = self.face_analyzer.calculate_identity_signature_metrics( # –ò–∑–º–µ–Ω–µ–Ω–æ
                normalized_landmarks, pose_info['pose_category']
            )
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä—ã
            logging.info(f"–ü–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º analyze_skin_texture_by_zones: image type={{type(image)}}, image shape={{image.shape if image is not None else 'None'}}")
            texture_analysis = self.texture_analyzer.analyze_skin_texture_by_zones(
                image, landmarks_3d[:, :2]
            )
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞
            file_date = self._extract_date_from_file(file_path)
            
            return {
                'success': True,
                'file_path': file_path,
                'landmarks_3d': landmarks_3d,
                'normalized_landmarks': normalized_landmarks,
                'pose_info': pose_info,
                'embedding': embedding,
                'confidence': embedding_confidence,
                'identity_metrics': identity_metrics,
                'texture_analysis': texture_analysis,
                'quality_score': quality_score,
                'date': file_date,
                'metadata': {
                    'image_shape': image.shape[:2],
                    'landmarks_confidence': confidence_scores.mean() if confidence_scores.size > 0 else 0.0
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}

    def _perform_clustering(self, embeddings_data: List[Dict]) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        
        try:
            embeddings_to_cluster = [item['embedding'] for item in embeddings_data]
            file_paths = [item['file_path'] for item in embeddings_data]
            dates = [item['date'] for item in embeddings_data]

            cluster_results = self.embedding_analyzer.perform_identity_clustering(
                embeddings_data # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                # epsilon, min_samples —Ç–µ–ø–µ—Ä—å –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ perform_identity_clustering
                # file_paths, dates, confidence_scores –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
            )
            
            cluster_results['file_paths'] = file_paths
            cluster_results['dates'] = dates

            identity_timeline = self.embedding_analyzer.build_identity_timeline(
                cluster_results
            )
            
            stability_analysis = self.embedding_analyzer.analyze_cluster_temporal_stability(
                identity_timeline
            )
            
            return {
                'cluster_results': cluster_results,
                'identity_timeline': identity_timeline,
                'stability_analysis': stability_analysis,
                'summary_stats': {
                    'n_clusters': cluster_results.get('n_clusters', 0),
                    'n_noise': cluster_results.get('n_noise', 0),
                    'total_images': len(embeddings_data),
                    'clustered_images': len(embeddings_data) - cluster_results.get('n_noise', 0)
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}", exc_info=True) # –î–æ–±–∞–≤–ª–µ–Ω–æ exc_info=True –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –ª–æ–≥–∞
            return {'error': str(e)}

    def _recalculate_clusters(self, epsilon: float, min_samples: int, confidence_filter: float, date_range: Tuple[str, str]) -> Tuple[pd.DataFrame, go.Figure, List[Tuple[str, str]], gr.Json]:
        """–ü–µ—Ä–µ—Å—á–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        logger.info(f"–ü–µ—Ä–µ—Å—á–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å epsilon={epsilon}, min_samples={min_samples}, confidence_filter={confidence_filter}, date_range={date_range}")

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        chronological_params = get_chronological_analysis_parameters()
        report_confidence_threshold = chronological_params.get('confidence_threshold', 0.85) # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.85

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
        if 'full_processed_data' not in self.results_cache or not self.results_cache['full_processed_data']:
            logger.warning("–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
            return (
                pd.DataFrame(columns=[
                    "ID –∫–ª–∞—Å—Ç–µ—Ä–∞", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ", "–ü–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", 
                    "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", "–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–°—Ç–∞—Ç—É—Å"
                ]),
                go.Figure().update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"),
                [],
                gr.update(value={})
            )

        full_processed_data = self.results_cache['full_processed_data']
        
        # 1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–∞—Ç–µ
        filtered_data = [
            item for item in full_processed_data 
            if item.get('confidence', 0.0) >= confidence_filter
        ]
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É –¥–∞—Ç
        if date_range and len(date_range) == 2 and date_range[0] and date_range[1]:
            try:
                start_date_obj = datetime.strptime(date_range[0], '%Y-%m-%d')
                end_date_obj = datetime.strptime(date_range[1], '%Y-%m-%d')
                filtered_data = [
                    item for item in filtered_data
                    if item.get('date') and start_date_obj <= item['date'] <= end_date_obj
                ]
            except ValueError as e:
                logger.error(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ, –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω

        if not filtered_data:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
            return (
                pd.DataFrame(columns=[
                    "ID –∫–ª–∞—Å—Ç–µ—Ä–∞", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ", "–ü–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", 
                    "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", "–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–°—Ç–∞—Ç—É—Å"
                ]),
                go.Figure().update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"),
                [],
                gr.update(value={})
            )

        embeddings_data_for_clustering = [
            {
                'embedding': item['embedding'],
                'file_path': item['file_path'],
                'date': item['date'],
                'confidence': item['confidence'],
                'metadata': item['metadata']
            }
            for item in filtered_data
        ]
        
        # 2. –ü–µ—Ä–µ—Å—á–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ DBSCAN
        try:
            embeddings_only = [item['embedding'] for item in embeddings_data_for_clustering]
            file_paths_for_clustering = [item['file_path'] for item in embeddings_data_for_clustering]
            dates_for_clustering = [item['date'] for item in embeddings_data_for_clustering]
            confidence_scores_for_clustering = [item['confidence'] for item in embeddings_data_for_clustering]

            # –í—ã–∑–æ–≤ perform_identity_clustering —Å —É—á–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DBSCAN
            cluster_results = self.embedding_analyzer.perform_identity_clustering(
                embeddings_data_for_clustering, # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                epsilon=epsilon,
                min_samples=min_samples
                # file_paths, dates, confidence_scores —Ç–µ–ø–µ—Ä—å –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ perform_identity_clustering
            )
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ filtered_data —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            for i, item in enumerate(filtered_data):
                if i < len(cluster_results['cluster_labels']):
                    item['cluster_label'] = cluster_results['cluster_labels'][i]
                else:
                    item['cluster_label'] = -1 # –í —Å–ª—É—á–∞–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª–∏–Ω—ã, –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ —à—É–º

            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
            identity_timeline = self.embedding_analyzer.build_identity_timeline(
                cluster_results
            )
            
            # –ö—ç—à–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.results_cache['clustering_results_recalculated'] = {
                'cluster_results': cluster_results,
                'identity_timeline': identity_timeline,
                'filtered_data': filtered_data # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            }
            logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–æ {cluster_results.get('n_clusters', 0)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å—á–µ—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}", exc_info=True)
            return (
                pd.DataFrame(columns=[
                    "ID –∫–ª–∞—Å—Ç–µ—Ä–∞", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ", "–ü–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", 
                    "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ", "–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–°—Ç–∞—Ç—É—Å"
                ]),
                go.Figure().update_layout(title=f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}"),
                [],
                gr.update(value={})
            )
            
        # 3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ cluster_summary
        cluster_summary_data = []
        unique_cluster_ids = np.unique([item['cluster_label'] for item in filtered_data])
        
        for cluster_id in unique_cluster_ids:
            if cluster_id == -1: # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏
                continue
            
            cluster_items = [
                item for item in filtered_data if item['cluster_label'] == cluster_id
            ]
            
            if not cluster_items:
                continue
                
            cluster_dates = sorted([item['date'] for item in cluster_items if item['date']])
            avg_confidence = np.mean([item['confidence'] for item in cluster_items]) if cluster_items else 0.0
            
            first_appearance = cluster_dates[0].strftime('%Y-%m-%d') if cluster_dates else "N/A"
            last_appearance = cluster_dates[-1].strftime('%Y-%m-%d') if cluster_dates else "N/A"
            
            status = "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω" if len(cluster_items) >= min_samples and avg_confidence >= report_confidence_threshold else "–¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏" # –ò—Å–ø–æ–ª—å–∑—É–µ–º report_confidence_threshold
            
            cluster_summary_data.append({
                "ID –∫–ª–∞—Å—Ç–µ—Ä–∞": cluster_id,
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ": len(cluster_items),
                "–ü–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ": first_appearance,
                "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ": last_appearance,
                "–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å": f"{avg_confidence:.2f}",
                "–°—Ç–∞—Ç—É—Å": status
            })
            
        cluster_summary_df = pd.DataFrame(cluster_summary_data)
        
        # 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (t-SNE)
        embeddings_for_tsne = np.array([item['embedding'] for item in filtered_data])
        cluster_labels_for_tsne = np.array([item['cluster_label'] for item in filtered_data])

        if embeddings_for_tsne.shape[0] < 2: # t-SNE —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 –æ–±—Ä–∞–∑—Ü–∞
             tsne_figure = go.Figure().update_layout(title="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è t-SNE")
        else:
            try:
                # –í—ã–±–∏—Ä–∞–µ–º perplexity –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
                perplexity_val = min(30, max(5, embeddings_for_tsne.shape[0] - 1)) 
                if embeddings_for_tsne.shape[0] < 5: # TSNE —Ç—Ä–µ–±—É–µ—Ç n_samples > n_components, –∏ –æ–±—ã—á–Ω–æ > perplexity
                    perplexity_val = embeddings_for_tsne.shape[0] - 1 if embeddings_for_tsne.shape[0] > 1 else 1 # Ensure perplexity > 1
                if perplexity_val < 1: perplexity_val = 1 # Ensure at least 1 for small datasets
                
                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val, learning_rate='auto', init='random')
                reduced_embeddings = tsne.fit_transform(embeddings_for_tsne)
                
                tsne_df = pd.DataFrame(reduced_embeddings, columns=['Component 1', 'Component 2'])
                tsne_df['Cluster'] = cluster_labels_for_tsne
                tsne_df['File Path'] = [item['file_path'] for item in filtered_data]
                tsne_df['Date'] = [item['date'].strftime('%Y-%m-%d') if item['date'] else 'N/A' for item in filtered_data]
                tsne_df['Confidence'] = [f"{item['confidence']:.2f}" for item in filtered_data]
                tsne_df['Quality'] = [f"{item['quality_score']:.2f}" for item in filtered_data]
                
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–º–µ—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ü–≤–µ—Ç
                unique_labels = sorted(tsne_df['Cluster'].unique())
                color_map = {label: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, label in enumerate(unique_labels)}

                tsne_figure = px.scatter(
                    tsne_df, 
                    x='Component 1', 
                    y='Component 2', 
                    color='Cluster', 
                    color_discrete_map=color_map, # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ü–≤–µ—Ç–∞
                    hover_data=['File Path', 'Date', 'Confidence', 'Quality'],
                    title='–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (t-SNE)'
                )
                tsne_figure.update_layout(height=500)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}", exc_info=True)
                tsne_figure = go.Figure().update_layout(title=f"–û—à–∏–±–∫–∞ t-SNE: {e}")

        # 5. –ì–∞–ª–µ—Ä–µ—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–¥–æ 100 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        gallery_images = []
        representative_images_per_cluster = {} # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—É—Ç–∏ –∫ –ª—É—á—à–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∫–ª–∞—Å—Ç–µ—Ä–∞

        for cluster_id in unique_cluster_ids:
            if cluster_id == -1: # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º
                continue
            
            cluster_items = [
                item for item in filtered_data if item['cluster_label'] == cluster_id
            ]
            
            if cluster_items:
                # –í—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è
                best_item = max(cluster_items, key=lambda x: x.get('confidence', 0.0))
                representative_images_per_cluster[cluster_id] = best_item['file_path']
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≥–∞–ª–µ—Ä–µ—é —Å –ø–æ–¥–ø–∏—Å—å—é
                gallery_images.append((best_item['file_path'], f"ID: {cluster_id}, Conf: {best_item['confidence']:.2f}"))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –≥–∞–ª–µ—Ä–µ–µ, –µ—Å–ª–∏ –∏—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
        gallery_images = gallery_images[:self.max_gallery_items]

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ dropdownd`identity_selector` –Ω–∞ –≤–∫–ª–∞–¥–∫–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        identity_selector_choices = [(f"–õ–∏—á–Ω–æ—Å—Ç—å {c_id}", c_id) for c_id in sorted(unique_cluster_ids) if c_id != -1]
        self.results_cache['identity_selector_choices'] = identity_selector_choices # –ö—ç—à–∏—Ä—É–µ–º –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥—Ä–æ–ø–¥–∞—É–Ω–∞

        return (
            cluster_summary_df, 
            tsne_figure, 
            gallery_images,
            gr.update(value={}) # –û—á–∏—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        )

    def _analyze_temporal_patterns(self, identity: int, resolution: str, metrics: List[str]) -> Tuple[go.Figure, pd.DataFrame, go.Figure, go.Figure, go.Figure, go.Figure]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ª–∏—á–Ω–æ—Å—Ç–∏.
        –î–æ–±–∞–≤–ª–µ–Ω–æ: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π, –¥–µ—Ç–µ–∫—Ü–∏—è —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è.
        """
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –ª–∏—á–Ω–æ—Å—Ç–∏ {identity} —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º {resolution}")

        if not self.analysis_state['results'] or 'identity_clustering' not in self.analysis_state['results']:
            gr.Warning("–î–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –º–∞—Å—Å–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É.")
            return go.Figure(), pd.DataFrame(), go.Figure(), go.Figure(), go.Figure(), go.Figure()
        
        all_clusters = self.analysis_state['results']['identity_clustering']['clusters']
        identity_timeline_raw = all_clusters.get(str(identity), {}).get('timeline', [])
        
        if not identity_timeline_raw:
            gr.Warning(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –¥–ª—è –ª–∏—á–Ω–æ—Å—Ç–∏ {identity} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            return go.Figure(), pd.DataFrame(), go.Figure(), go.Figure(), go.Figure(), go.Figure()

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        timeline_df = pd.DataFrame(identity_timeline_raw)
        if 'date' not in timeline_df.columns or 'metrics' not in timeline_df.columns:
            gr.Warning("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏.")
            return go.Figure(), pd.DataFrame(), go.Figure(), go.Figure(), go.Figure(), go.Figure()

        timeline_df['date'] = pd.to_datetime(timeline_df['date'])
        timeline_df = timeline_df.sort_values(by='date').reset_index(drop=True)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ (–ø–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ) –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–∏ –≤–æ–∑—Ä–∞—Å—Ç–∞
        first_entry_metrics = timeline_df['metrics'].iloc[0]
        baseline_metrics = first_entry_metrics if isinstance(first_entry_metrics, dict) else {}
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–π –¥–∞—Ç—ã
        birth_date_str = self.analysis_state['results'].get('birth_date', PUTIN_BIRTH_DATE) # –ò—Å–ø–æ–ª—å–∑—É–µ–º PUTIN_BIRTH_DATE –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        birth_date = datetime.strptime(birth_date_str, '%Y-%m-%d')
        ages = self.temporal_analyzer.calculate_age_on_each_date(timeline_df['date'].tolist(), birth_date)
        timeline_df['age'] = ages

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é
        grouped_data = []
        for name, group in timeline_df.groupby(pd.Grouper(key='date', freq=self._get_freq_string(resolution))):
            if not group.empty:
                avg_metrics = {metric: np.mean([entry['metrics'][metric] for entry in group.to_dict(orient='records') if metric in entry['metrics']]) for metric in metrics}
                grouped_data.append({'date': name, 'metrics': avg_metrics, 'age': np.mean(group['age'])})
        
        if not grouped_data:
            gr.Warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é.")
            return go.Figure(), pd.DataFrame(), go.Figure(), go.Figure(), go.Figure(), go.Figure()

        grouped_df = pd.DataFrame(grouped_data)

        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        temporal_metrics_timeline = {metric: [] for metric in metrics}
        predicted_temporal_metrics_timeline = {metric: [] for metric in metrics}

        for index, row in grouped_df.iterrows():
            current_age = row['age']
            predicted_values = self.temporal_analyzer.predict_expected_metrics_for_age(current_age, baseline_metrics)
            for metric in metrics:
                actual_val = row['metrics'].get(metric)
                if actual_val is not None:
                    temporal_metrics_timeline[metric].append(actual_val)
                    predicted_temporal_metrics_timeline[metric].append(predicted_values.get(metric, np.nan))

        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        fig_temporal = go.Figure()
        for metric in metrics:
            fig_temporal.add_trace(go.Scatter(
                x=grouped_df['date'], 
                y=temporal_metrics_timeline[metric],
                mode='lines+markers', 
                name=f'–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π {metric}'
            ))
        fig_temporal.update_layout(title='–ò–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –≤–æ –≤—Ä–µ–º–µ–Ω–∏', xaxis_title='–î–∞—Ç–∞', yaxis_title='–ó–Ω–∞—á–µ–Ω–∏–µ')

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ –∞–Ω–æ–º–∞–ª–∏–π
        change_stats_data = []
        for metric in metrics:
            values = np.array(temporal_metrics_timeline[metric])
            if len(values) < 2: continue

            mean_change = np.mean(np.diff(values)) if len(values) > 1 else 0.0
            std_change = np.std(np.diff(values)) if len(values) > 1 else 0.0

            # –¢—Ä–µ–Ω–¥ (–ø—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
            x = np.arange(len(values)).reshape(-1, 1)
            model = LinearRegression().fit(x, values)
            trend_slope = model.coef_[0]
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º TemporalAnalyzer)
            metric_dict_for_anomaly = {metric: temporal_metrics_timeline[metric]}
            predicted_dict_for_anomaly = {metric: predicted_temporal_metrics_timeline[metric]}
            anomalies = self.temporal_analyzer.detect_temporal_anomalies_in_metrics(metric_dict_for_anomaly, predicted_dict_for_anomaly)
            anomaly_count = len(anomalies.get(metric, {}).get('anomaly_indices', []))
            p_value = anomalies.get(metric, {}).get('significance_level', 1.0)

            change_stats_data.append([
                metric, 
                f'{mean_change:.4f}', 
                f'{std_change:.4f}', 
                f'{trend_slope:.4f}', 
                anomaly_count,
                f'{p_value:.4f}'
            ])
        df_change_stats = pd.DataFrame(change_stats_data, columns=[
            "–ú–µ—Ç—Ä–∏–∫–∞", "–°—Ä–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ", "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ",
            "–¢—Ä–µ–Ω–¥", "–ê–Ω–æ–º–∞–ª–∏–∏", "P-value"
        ])
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π Plot (–∏–∑ _analyze_temporal_patterns)
        # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –∞–Ω–æ–º–∞–ª–∏–∏, —á—Ç–æ –∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –Ω–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        fig_anomaly = go.Figure()
        for metric in metrics:
            actual_vals = temporal_metrics_timeline[metric]
            dates_for_plot = grouped_df['date']
            fig_anomaly.add_trace(go.Scatter(
                x=dates_for_plot, 
                y=actual_vals,
                mode='lines', 
                name=f'{metric}'
            ))
            if metric in anomalies:
                anomaly_indices = anomalies[metric]['anomaly_indices']
                if anomaly_indices:
                    anomaly_dates = [dates_for_plot.iloc[i] for i in anomaly_indices]
                    anomaly_values = [actual_vals[i] for i in anomaly_indices]
                    fig_anomaly.add_trace(go.Scatter(
                        x=anomaly_dates,
                        y=anomaly_values,
                        mode='markers',
                        marker=dict(color='red', size=8),
                        name=f'–ê–Ω–æ–º–∞–ª–∏–∏ {metric}'
                    ))
        fig_anomaly.update_layout(title='–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏', xaxis_title='–î–∞—Ç–∞', yaxis_title='–ó–Ω–∞—á–µ–Ω–∏–µ')

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏ (–∑–∞–≥–ª—É—à–∫–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ data_manager)
        fig_historical = go.Figure()
        fig_historical.add_trace(go.Scatter(x=[1, 2, 3], y=[1, 2, 1], mode='lines+markers', name='–ü—Ä–∏–º–µ—Ä'))
        fig_historical.update_layout(title='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏', xaxis_title='–í—Ä–µ–º—è', yaxis_title='–°–æ–±—ã—Ç–∏–µ')

        # === –ù–û–í–´–ï –ì–†–ê–§–ò–ö–ò ===
        # 1. –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö vs —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
        fig_predicted_vs_actual = self._create_predicted_vs_actual_plot(
            grouped_df['date'].tolist(), 
            temporal_metrics_timeline, 
            predicted_temporal_metrics_timeline,
            metrics
        )

        # 2. –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        change_points_results = self.temporal_analyzer.detect_change_points(temporal_metrics_timeline)
        fig_change_points = self._create_change_points_plot(
            grouped_df['date'].tolist(), 
            temporal_metrics_timeline, 
            change_points_results
        )

        return fig_temporal, df_change_stats, fig_anomaly, fig_historical, fig_predicted_vs_actual, fig_change_points

    def _get_freq_string(self, resolution: str) -> str:
        if resolution == "–ú–µ—Å—è—Ü":
            return "M"
        elif resolution == "–ö–≤–∞—Ä—Ç–∞–ª":
            return "QS"
        elif resolution == "–ì–æ–¥":
            return "Y"
        return "D"

    def _create_predicted_vs_actual_plot(self, dates: List[datetime], actual_metrics: Dict, 
                                          predicted_metrics: Dict, metrics_to_plot: List[str]) -> go.Figure:
        """
        –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫.
        """
        fig = go.Figure()
        for metric in metrics_to_plot:
            fig.add_trace(go.Scatter(
                x=dates, 
                y=actual_metrics.get(metric, []),
                mode='lines+markers', 
                name=f'–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π {metric}',
                line=dict(color='blue')
            ))
            fig.add_trace(go.Scatter(
                x=dates, 
                y=predicted_metrics.get(metric, []),
                mode='lines', 
                name=f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π {metric}',
                line=dict(color='orange', dash='dash')
            ))
        fig.update_layout(
            title='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ú–µ—Ç—Ä–∏–∫–∏',
            xaxis_title='–î–∞—Ç–∞',
            yaxis_title='–ó–Ω–∞—á–µ–Ω–∏–µ',
            hovermode="x unified"
        )
        return fig

    def _create_change_points_plot(self, dates: List[datetime], metrics_timeline: Dict, 
                                   change_points_results: Dict) -> go.Figure:
        """
        –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫.
        """
        fig = go.Figure()
        for metric in metrics_timeline.keys():
            values = metrics_timeline[metric]
            fig.add_trace(go.Scatter(
                x=dates, 
                y=values,
                mode='lines', 
                name=f'{metric}',
                line=dict(color='grey')
            ))
            
            if metric in change_points_results and change_points_results[metric]['detected']:
                points = change_points_results[metric]['points']
                for point_idx in points:
                    if 0 <= point_idx < len(dates):
                        fig.add_vline(
                            x=dates[point_idx],
                            line_dash="dash",
                            line_color="red",
                            annotation_text=f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤ {metric}",
                            annotation_position="top right"
                        )
        fig.update_layout(
            title='–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –¢–æ—á–∫–∏ –ò–∑–º–µ–Ω–µ–Ω–∏—è –ú–µ—Ç—Ä–∏–∫',
            xaxis_title='–î–∞—Ç–∞',
            yaxis_title='–ó–Ω–∞—á–µ–Ω–∏–µ',
            hovermode="x unified"
        )
        return fig

    def _analyze_mask_technology(self, sensitivity: float, tech_levels: List[str]) -> Tuple[pd.DataFrame, go.Figure, go.Figure, go.Figure, gr.HTML]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–∞—Å–æ–∫"""
        logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–∞—Å–æ–∫ —Å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é {sensitivity} –∏ —É—Ä–æ–≤–Ω—è–º–∏ {tech_levels}")

        if 'full_processed_data' not in self.results_cache or not self.results_cache['full_processed_data']:
            logger.warning("–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Å–æ–∫.")
            return (
                pd.DataFrame(columns=[
                    "–î–∞—Ç–∞", "–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å",
                    "Shape Error", "Texture Score", "–°—Ç–∞—Ç—É—Å", "–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É"
                ]),
                go.Figure().update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"),
                go.Figure().update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç—É—Ä"),
                go.Figure().update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"),
                gr.update(value="<div class='alert-info'>–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö</div>")
            )
        
        full_processed_data = self.results_cache['full_processed_data']
        
        mask_detection_results = []
        all_shape_errors = []
        all_entropies = []
        all_dates_for_texture = []

        for item in full_processed_data:
            if 'texture_analysis' in item and 'identity_metrics' in item and 'embedding' in item and 'pose_info' in item:
                try:
                    texture_data = item['texture_analysis']
                    # identity_metrics = item['identity_metrics'] # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —ç—Ç–æ–º –±–ª–æ–∫–µ
                    embedding = item['embedding']
                    file_date = item['date']
                    file_path = item['file_path']
                    cluster_label = item.get('cluster_label') # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∞

                    # 1. –†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
                    material_authenticity_score = self.texture_analyzer.calculate_material_authenticity_score(texture_data)

                    # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—Å–∫–∏
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ü–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ —ç–ª–µ–º–µ–Ω—Ç–∞
                    cluster_center_embedding = np.zeros(embedding.shape) # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    if cluster_label is not None and cluster_label != -1 and \
                       'clustering_results_recalculated' in self.results_cache and \
                       'cluster_results' in self.results_cache['clustering_results_recalculated'] and \
                       'cluster_centers' in self.results_cache['clustering_results_recalculated']['cluster_results'] and \
                       cluster_label in self.results_cache['clustering_results_recalculated']['cluster_results']['cluster_centers']:
                        cluster_center_embedding = self.results_cache['clustering_results_recalculated']['cluster_results']['cluster_centers'][cluster_label]
                    else:
                        logger.warning(f"–¶–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞ {file_path} —Å –º–µ—Ç–∫–æ–π {cluster_label}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä.")
                        # –í –∫–∞—á–µ—Å—Ç–≤–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ä–µ–¥–Ω–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ —ç—Ç–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞

                    # –ï—Å–ª–∏ _process_single_image –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç shape_error, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    shape_error = item['metadata'].get('shape_error', 0.25) 
                    
                    mask_info = self.texture_analyzer.classify_mask_technology_level(
                        texture_data,
                        file_date,
                        shape_error, # –ü–µ—Ä–µ–¥–∞–µ–º shape_error
                        self.embedding_analyzer.calculate_embedding_distances_matrix([embedding, cluster_center_embedding])[0,1] # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
                    )

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —É—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
                    if mask_info['level'] in tech_levels and mask_info['confidence'] >= sensitivity:
                        mask_detection_results.append({
                            "–î–∞—Ç–∞": file_date.strftime('%Y-%m-%d'),
                            "–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": mask_info['level'],
                            "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å": f"{mask_info['confidence']:.2f}",
                            "Shape Error": f"{shape_error:.3f}", # –î–æ–±–∞–≤–ª—è–µ–º Shape Error
                            "Texture Score": f"{material_authenticity_score:.2f}",
                            "–°—Ç–∞—Ç—É—Å": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –º–∞—Å–∫–∞",
                            "–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É": file_path
                        })
                    
                    all_shape_errors.append(shape_error)
                    all_entropies.append(texture_data.get('shannon_entropy', 0.0))
                    all_dates_for_texture.append(file_date)

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Å–∫–∏ –¥–ª—è {item.get('file_path', 'Unknown')}: {e}", exc_info=True)

        mask_summary_df = pd.DataFrame(mask_detection_results)

        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º (Bar Plot)
        if not mask_summary_df.empty:
            tech_counts = mask_summary_df['–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'].value_counts().reset_index()
            tech_counts.columns = ['–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ']
            technology_distribution_fig = px.bar(
                tech_counts, 
                x='–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 
                y='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', 
                title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–∞—Å–æ–∫'
            )
        else:
            technology_distribution_fig = go.Figure().update_layout(title="–ú–∞—Å–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")

        # 2. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä—ã (Scatter plot: Shape Error vs Entropy)
        texture_analysis_plot_fig = go.Figure()
        if all_shape_errors and all_entropies and all_dates_for_texture:
            texture_df = pd.DataFrame({
                'Shape Error': all_shape_errors,
                'Entropy': all_entropies,
                'Date': all_dates_for_texture
            })
            texture_analysis_plot_fig = px.scatter(
                texture_df, 
                x='Shape Error', 
                y='Entropy', 
                color='Date', 
                title='–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (Shape Error vs Entropy)'
            )
        else:
            texture_analysis_plot_fig.update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç—É—Ä")

        # 3. –≠–≤–æ–ª—é—Ü–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π (Line Plot)
        technology_evolution_fig = go.Figure()
        if not mask_summary_df.empty:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≥–æ–¥—É –∏ —É—Ä–æ–≤–Ω—é —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
            mask_summary_df['Year'] = pd.to_datetime(mask_summary_df['–î–∞—Ç–∞']).dt.year
            tech_evolution = mask_summary_df.groupby(['Year', '–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']).size().unstack(fill_value=0)
            
            if not tech_evolution.empty:
                technology_evolution_fig = px.line(
                    tech_evolution, 
                    x=tech_evolution.index,
                    y=tech_evolution.columns,
                    title='–≠–≤–æ–ª—é—Ü–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–∞—Å–æ–∫ –ø–æ –≥–æ–¥–∞–º'
                )
                technology_evolution_fig.update_layout(
                    xaxis_title="–ì–æ–¥", 
                    yaxis_title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–π",
                    legend_title="–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"
                )
            else:
                technology_evolution_fig.update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π")
        else:
            technology_evolution_fig.update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π")

        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        critical_detections_html = "<div class='alert-info'>–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ</div>"
        if not mask_summary_df.empty:
            critical_masks = mask_summary_df[mask_summary_df['–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å'].astype(float) >= 0.95]
            if not critical_masks.empty:
                critical_detections_html = "<div class='alert-danger'>**–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø –ú–ê–°–û–ö:**<br>"
                for index, row in critical_masks.iterrows():
                    critical_detections_html += f"- **{row['–î–∞—Ç–∞']}**: –£—Ä–æ–≤–µ–Ω—å {row['–£—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']}, –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {row['–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å']}<br>"
                critical_detections_html += "</div>"
            
        return (
            mask_summary_df, 
            technology_distribution_fig, 
            texture_analysis_plot_fig,
            technology_evolution_fig,
            gr.update(value=critical_detections_html)
        )

    def _generate_expert_report(self, report_type: str, sections: List[str], confidence_threshold: float) -> Tuple[Dict, go.Figure, str, gr.File]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {report_type}, –†–∞–∑–¥–µ–ª—ã: {sections}, –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {confidence_threshold}")

        report_data = {
            "total_identities": 0,
            "confidence_level": 0.0,
            "statistical_significance": 0.0,
            "anomalies_detected": 0,
            "masks_detected": 0,
            "analysis_period": "N/A"
        }
        expert_conclusion_text = ""
        final_timeline_figure = go.Figure().update_layout(title="–ò—Ç–æ–≥–æ–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è")
        download_file = None

        if 'clustering_results_recalculated' not in self.results_cache:
            expert_conclusion_text = ("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –º–∞—Å—Å–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é."
                                     "<br>–û—à–∏–±–∫–∞: –ù–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
            return report_data, final_timeline_figure, expert_conclusion_text, None
        
        clustering_results = self.results_cache['clustering_results_recalculated']['cluster_results']
        identity_timeline = self.results_cache['clustering_results_recalculated']['identity_timeline']
        filtered_data = self.results_cache['clustering_results_recalculated']['filtered_data']

        # –û–±—â–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        report_data['total_identities'] = clustering_results.get('n_clusters', 0)
        
        all_confidences = [item['confidence'] for item in filtered_data if 'confidence' in item]
        if all_confidences:
            report_data['confidence_level'] = np.mean(all_confidences)
        
        all_dates = [item['date'] for item in filtered_data if 'date' in item]
        if all_dates:
            report_data['analysis_period'] = f"{min(all_dates).strftime('%Y-%m-%d')} - {max(all_dates).strftime('%Y-%m-%d')}"

        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞–Ω–æ–º–∞–ª–∏—è–º –∏ –º–∞—Å–∫–∞–º (–µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã)
        anomalies_count = 0
        masks_count = 0
        if 'temporal_analysis_results' in self.results_cache:
            for metric_anomalies in self.results_cache['temporal_analysis_results']['anomalies'].values():
                anomalies_count += len(metric_anomalies.get('anomaly_indices', []))
                anomalies_count += len(metric_anomalies.get('rapid_change_indices', []))
        report_data['anomalies_detected'] = anomalies_count

        if 'mask_detection_results' in self.results_cache:
            masks_count = len(self.results_cache['mask_detection_results'])
        report_data['masks_detected'] = masks_count

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è
        expert_conclusion_text = "## –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n"
        if "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ" in sections:
            expert_conclusion_text += "### –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ\n"
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º report_data.get('total_images') –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è KeyError, –µ—Å–ª–∏ –∫–ª—é—á –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            expert_conclusion_text += f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {report_data.get('total_images', 'N/A')} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –ø–µ—Ä–∏–æ–¥ {report_data['analysis_period']} –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ **{report_data['total_identities']}** –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–∏—á–Ω–æ—Å—Ç–µ–π. "
            if report_data['masks_detected'] > 0:
                expert_conclusion_text += f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ **{report_data['masks_detected']}** —Å–ª—É—á–∞–µ–≤ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∞—Å–æ–∫. "
            if report_data['anomalies_detected'] > 0:
                expert_conclusion_text += f"–í—ã—è–≤–ª–µ–Ω–æ **{report_data['anomalies_detected']}** –∞–Ω–æ–º–∞–ª–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –º–µ—Ç—Ä–∏–∫.\n\n"

        if "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏" in sections:
            expert_conclusion_text += "### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n"
            expert_conclusion_text += f"–°–∏—Å—Ç–µ–º–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ {report_data['total_identities']} —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ª–∏—á–Ω–æ—Å—Ç–µ–π. –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {report_data['confidence_level']:.2f}.\n\n"
            # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            if 'cluster_summary_df' in self.results_cache:
                expert_conclusion_text += "#### –°–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:\n"
                expert_conclusion_text += self.results_cache['cluster_summary_df'].to_markdown(index=False) + "\n\n"

        if "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑" in sections:
            expert_conclusion_text += "### –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑\n"
            expert_conclusion_text += f"–ü—Ä–æ–≤–µ–¥–µ–Ω –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö –ª–∏—Ü–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {report_data['anomalies_detected']} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π, –Ω–µ –æ–±—ä—è—Å–Ω–∏–º—ã—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ —Å—Ç–∞—Ä–µ–Ω–∏—è.\n\n"
            # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
            if 'temporal_analysis_results' in self.results_cache and 'change_statistics_df' in self.results_cache['temporal_analysis_results']:
                expert_conclusion_text += "#### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫:\n"
                expert_conclusion_text += self.results_cache['temporal_analysis_results']['change_statistics_df'].to_markdown(index=False) + "\n\n"

        if "–î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å–æ–∫" in sections:
            expert_conclusion_text += "### –î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å–æ–∫\n"
            if report_data['masks_detected'] > 0:
                expert_conclusion_text += f"–ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ {report_data['masks_detected']} —Å–ª—É—á–∞–µ–≤, –≥–¥–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ª–∏—Ü–∞ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å–∫–∏. "
                expert_conclusion_text += "–≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ —Ç–µ–∫—Å—Ç—É—Ä–µ –∫–æ–∂–∏, —Ñ–æ—Ä–º–µ –ª–∏—Ü–∞ –∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å—Ç–∞—Ä–µ–Ω–∏—è.\n\n"
                if 'mask_summary_df' in self.results_cache:
                    expert_conclusion_text += "#### –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –º–∞—Å–∫–∏:\n"
                    expert_conclusion_text += self.results_cache['mask_summary_df'].to_markdown(index=False) + "\n\n"
            else:
                expert_conclusion_text += "–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∞—Å–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n\n"

        if "–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑" in sections:
            expert_conclusion_text += "### –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n"
            expert_conclusion_text += f"–ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –æ—Ü–µ–Ω–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–æ–≤: P-value = {report_data['statistical_significance']:.4f}. "
            if report_data['statistical_significance'] < 0.05:
                expert_conclusion_text += "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —è–≤–ª—è—é—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–º–∏.\n\n"
            else:
                expert_conclusion_text += "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏.\n\n"

        if "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ" in sections:
            expert_conclusion_text += "### –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n"
            if report_data['total_identities'] > 1 and report_data['confidence_level'] > confidence_threshold:
                expert_conclusion_text += "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–æ–π –ª–∏—á–Ω–æ—Å—Ç–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ.\n\n"
            else:
                expert_conclusion_text += "–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã—è–≤–∏–ª —É–±–µ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –Ω–∞–ª–∏—á–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª–∏—á–Ω–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.\n\n"

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏
        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        final_timeline_figure = self._generate_final_timeline_plot(identity_timeline)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª (–¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞)
        report_filename = f"expert_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path = os.path.join(RESULTS_DIR, report_filename) # –ò—Å–ø–æ–ª—å–∑—É–µ–º RESULTS_DIR –∏–∑ core_config
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(expert_conclusion_text)
        download_file = gr.update(value=report_path, visible=True)

        return report_data, final_timeline_figure, expert_conclusion_text, download_file

    def _generate_final_timeline_plot(self, identity_timeline: Dict) -> go.Figure:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –ª–∏–Ω–∏—é –≤—Å–µ—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—á–Ω–æ—Å—Ç–µ–π."""
        timeline_data = []
        for cluster_id, info in identity_timeline.items():
            if cluster_id != -1: # –ò—Å–∫–ª—é—á–∞–µ–º —à—É–º
                first_app = info.get('first_appearance')
                last_app = info.get('last_appearance')
                if first_app and last_app:
                    timeline_data.append({
                        'start': first_app,
                        'end': last_app,
                        'identity': f'–õ–∏—á–Ω–æ—Å—Ç—å {cluster_id}',
                        'appearance_count': info.get('appearance_count', 0)
                    })
        
        if not timeline_data:
            return go.Figure().update_layout(title="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏")

        timeline_df = pd.DataFrame(timeline_data)
        timeline_df['start'] = pd.to_datetime(timeline_df['start'])
        timeline_df['end'] = pd.to_datetime(timeline_df['end'])

        fig = px.timeline(timeline_df, x_start="start", x_end="end", y="identity", color="appearance_count",
                          title="–ò—Ç–æ–≥–æ–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –ø–æ—è–≤–ª–µ–Ω–∏—è –ª–∏—á–Ω–æ—Å—Ç–µ–π")
        fig.update_yaxes(autorange="reversed") # –î–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ –ª–∏—á–Ω–æ—Å—Ç–µ–π
        return fig

    def _export_results_json(self):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª."""
        if not self.results_cache:
            logger.warning("–ù–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ JSON.")
            return gr.update(visible=False)

        output_filename = f"analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        output_path = os.path.join(RESULTS_DIR, output_filename) # –ò—Å–ø–æ–ª—å–∑—É–µ–º RESULTS_DIR –∏–∑ core_config
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ numpy –º–∞—Å—Å–∏–≤–æ–≤ –≤ —Å–ø–∏—Å–∫–∏ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        def convert_numpy_arrays(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Object of type {obj.__class__.__name__} is not JSON serializable")

        try:
            # –ì–ª—É–±–æ–∫–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫—ç—à–∞ –ø—Ä–∏ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            export_data = json.loads(json.dumps(self.results_cache, default=convert_numpy_arrays))
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=4)
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ JSON: {output_path}")
            return gr.update(value=output_path, visible=True)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ JSON: {e}", exc_info=True)
            return gr.update(visible=False)

    def _export_results_pdf(self) -> gr.File:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ PDF-—Ñ–∞–π–ª."""
        if not self.results_cache:
            logger.warning("–ù–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ PDF.")
            return gr.update(visible=False)

        output_filename = f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        output_path = os.path.join(RESULTS_DIR, output_filename)

        try:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            output_dir = os.path.dirname(output_path)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            doc = SimpleDocTemplate(output_path, pagesize=letter)
            styles = getSampleStyleSheet()
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª–µ–π –¥–ª—è –æ—Ç—á–µ—Ç–∞
            title_style = styles['h1']
            heading_style = styles['h2']
            body_style = styles['Normal']
            body_style.fontSize = 10
            body_style.leading = 14

            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è PDF
            story = []

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
            story.append(Paragraph("–û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ª–∏—Ü–µ–≤–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏", title_style))
            story.append(Spacer(1, 0.2 * inch))
            story.append(Paragraph(f"–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", body_style))
            story.append(Spacer(1, 0.2 * inch))

            # –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º self.results_cache)
            results = self.results_cache.get('clustering_results_recalculated', {})
            
            if "summary_stats" in results.get('cluster_results', {}):
                summary_stats = results['cluster_results']['summary_stats']
                story.append(Paragraph("–û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", heading_style))
                story.append(Spacer(1, 0.1 * inch))
                story.append(Paragraph(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—á–Ω–æ—Å—Ç–µ–π: {summary_stats.get("n_clusters", "N/A")}', body_style))
                story.append(Paragraph(f'–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {summary_stats.get("total_images", "N/A")}', body_style))
                story.append(Paragraph(f'–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö: {summary_stats.get("clustered_images", "N/A")}', body_style))
                story.append(Spacer(1, 0.2 * inch))

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            if 'cluster_summary_df' in self.results_cache and not self.results_cache['cluster_summary_df'].empty:
                story.append(Paragraph("–°–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:", heading_style))
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ DataFrame –≤ —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –¥–ª—è Table
                data = [self.results_cache['cluster_summary_df'].columns.tolist()] + self.results_cache['cluster_summary_df'].values.tolist()
                table = Table(data)
                table.setStyle(TableStyle([
                    ('BACKGROUND', (0,0), (-1,0), colors.grey),
                    ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),
                    ('ALIGN', (0,0), (-1,-1), 'CENTER'),
                    ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),
                    ('BOTTOMPADDING', (0,0), (-1,0), 12),
                    ('BACKGROUND', (0,1), (-1,-1), colors.beige),
                    ('GRID', (0,0), (-1,-1), 1, colors.black)
                ]))
                story.append(table)
                story.append(Spacer(1, 0.2 * inch))

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –∞–Ω–æ–º–∞–ª–∏–π
            if 'temporal_analysis_results' in self.results_cache and 'change_statistics_df' in self.results_cache['temporal_analysis_results']:
                story.append(Paragraph("–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –∏ –∞–Ω–æ–º–∞–ª–∏–π:", heading_style))
                change_stats_df = self.results_cache['temporal_analysis_results']['change_statistics_df']
                data = [change_stats_df.columns.tolist()] + change_stats_df.values.tolist()
                table = Table(data)
                table.setStyle(TableStyle([
                    ('BACKGROUND', (0,0), (-1,0), colors.grey),
                    ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),
                    ('ALIGN', (0,0), (-1,-1), 'CENTER'),
                    ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),
                    ('BOTTOMPADDING', (0,0), (-1,0), 12),
                    ('BACKGROUND', (0,1), (-1,-1), colors.beige),
                    ('GRID', (0,0), (-1,-1), 1, colors.black)
                ]))
                story.append(table)
                story.append(Spacer(1, 0.2 * inch))

            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–∞—Å–∫–∞–º
            if 'mask_summary_df' in self.results_cache and not self.results_cache['mask_summary_df'].empty:
                story.append(Paragraph("–î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å–æ–∫:", heading_style))
                mask_summary_df = self.results_cache['mask_summary_df']
                data = [mask_summary_df.columns.tolist()] + mask_summary_df.values.tolist()
                table = Table(data)
                table.setStyle(TableStyle([
                    ('BACKGROUND', (0,0), (-1,0), colors.grey),
                    ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),
                    ('ALIGN', (0,0), (-1,-1), 'CENTER'),
                    ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),
                    ('BOTTOMPADDING', (0,0), (-1,0), 12),
                    ('BACKGROUND', (0,1), (-1,-1), colors.beige),
                    ('GRID', (0,0), (-1,-1), 1, colors.black)
                ]))
                story.append(table)
                story.append(Spacer(1, 0.2 * inch))
            
            # –°–±–æ—Ä–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ PDF
            doc.build(story)

            logging.info(f"–û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ PDF: {output_path}")
            return gr.update(value=output_path, visible=True)

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ PDF: {e}")
            return gr.update(visible=False)

    def _apply_system_settings(self, dbscan_epsilon: float, dbscan_min_samples: int, anomaly_z_threshold: float, change_rate_threshold: float, min_face_size: int, blur_threshold: float):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫...")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Å—Ç–∞–Ω—Ç –≤ core_config (–µ—Å–ª–∏ —ç—Ç–æ –¥–æ–ø—É—Å—Ç–∏–º–æ)
        # –í–Ω–∏–º–∞–Ω–∏–µ: –ø—Ä—è–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Å—Ç–∞–Ω—Ç –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –ª—É—á—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π
        # –õ—É—á—à–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç.
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: 
        try:
            global DBSCAN_EPSILON, DBSCAN_MIN_SAMPLES, ANOMALY_DETECTION_THRESHOLDS, IMAGE_QUALITY_THRESHOLDS
            DBSCAN_EPSILON = dbscan_epsilon
            DBSCAN_MIN_SAMPLES = dbscan_min_samples
            ANOMALY_DETECTION_THRESHOLDS['Z_SCORE_ANOMALY_THRESHOLD'] = anomaly_z_threshold # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
            ANOMALY_DETECTION_THRESHOLDS['RAPID_CHANGE_STD_MULTIPLIER'] = change_rate_threshold # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
            IMAGE_QUALITY_THRESHOLDS['MIN_FACE_SIZE'] = min_face_size
            IMAGE_QUALITY_THRESHOLDS['BLUR_DETECTION_THRESHOLD'] = blur_threshold
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —ç—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
            # –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏—Ö –≤ init –∏–ª–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã set_params()
            # self.embedding_analyzer.set_dbscan_params(dbscan_epsilon, dbscan_min_samples)
            # self.face_analyzer.set_quality_thresholds(min_face_size, blur_threshold)

            logger.info("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã.")
            return {
                "settings_applied": True,
                "timestamp": datetime.now().isoformat(),
                "dbscan_epsilon": DBSCAN_EPSILON,
                "dbscan_min_samples": DBSCAN_MIN_SAMPLES,
                "anomaly_z_threshold": anomaly_z_threshold, # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
                "change_rate_threshold": change_rate_threshold, # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
                "min_face_size": IMAGE_QUALITY_THRESHOLDS['MIN_FACE_SIZE'],
                "blur_threshold": IMAGE_QUALITY_THRESHOLDS['BLUR_DETECTION_THRESHOLD']
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}", exc_info=True)
            return {"error": str(e)}

    def _clear_system_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∫—ç—à–∞"""
        logger.info("–û—á–∏—Å—Ç–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∫—ç—à–∞...")
        self.results_cache = {}
        self.visualization_cache = {}
        gc.collect() # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        logger.info("–ö—ç—à —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω.")
        return (
            "–ö—ç—à –æ—á–∏—â–µ–Ω",
            {"cache_cleared": True, "timestamp": datetime.now().isoformat()}
        )

    def _update_system_status(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        logger.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã...")
        
        status = {
            "3ddfa_status": "–ù–µ–∞–∫—Ç–∏–≤–µ–Ω",
            "insightface_status": "–ù–µ–∞–∫—Ç–∏–≤–µ–Ω",
            "gpu_available": False,
            "memory_usage": "N/A",
            "cache_size": f"{sys.getsizeof(self.results_cache) / (1024**2):.2f} MB" # –†–∞–∑–º–µ—Ä –∫—ç—à–∞ –≤ MB
        }
        performance_data = []

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3DDFA_V2
            # –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —Ç–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            # –ó–¥–µ—Å—å –ø–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞
            if hasattr(self, 'face_analyzer') and self.face_analyzer and self.face_analyzer.init_done:
                status['3ddfa_status'] = "–ê–∫—Ç–∏–≤–µ–Ω"
                performance_data.append({"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç": "3DDFA_V2", "–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º—Å)": 120, "–°—Ç–∞—Ç—É—Å": "OK"})
            else:
                status['3ddfa_status'] = "–ù–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ InsightFace
            if hasattr(self, 'embedding_analyzer') and self.embedding_analyzer and self.embedding_analyzer.model_initialized:
                status['insightface_status'] = "–ê–∫—Ç–∏–≤–µ–Ω"
                performance_data.append({"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç": "InsightFace", "–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º—Å)": 85, "–°—Ç–∞—Ç—É—Å": "OK"})
            else:
                status['insightface_status'] = "–ù–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
            try:
                import torch
                if torch.cuda.is_available():
                    status['gpu_available'] = True
                    status['gpu_name'] = torch.cuda.get_device_name(0)
                else:
                    status['gpu_available'] = False
            except ImportError:
                status['gpu_available'] = "N/A (Torch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)"

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏
            import psutil
            process = psutil.Process(os.getpid())
            mem_info = process.memory_info()
            status['memory_usage'] = f"{mem_info.rss / (1024**2):.2f} MB" # Resident Set Size

            logger.info("–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –æ–±–Ω–æ–≤–ª–µ–Ω.")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã: {e}", exc_info=True)
            status['error'] = str(e)

        return (
            status,
            pd.DataFrame(performance_data)
        )

    def _assess_image_quality(self, image: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ä–µ–∑–∫–æ—Å—Ç—å, –æ—Å–≤–µ—â–µ–Ω–∏–µ)"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏ (Variance of Laplacian)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        fm = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        if np.isnan(fm) or np.isinf(fm):
            logging.warning(f"Laplacian variance (fm) contains NaN or Inf: {fm}. Returning 0.0 for quality.")
            return 0.0

        # –û—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è (—Å—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å)
        brightness = np.mean(gray) / 255.0 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-1

        if np.isnan(brightness) or np.isinf(brightness):
            logging.warning(f"Brightness contains NaN or Inf: {brightness}. Returning 0.0 for quality.")
            return 0.0

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        # –ü–æ—Ä–æ–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        sharpness_score = min(1.0, fm / IMAGE_QUALITY_THRESHOLDS['BLUR_DETECTION_THRESHOLD']) # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏
        lighting_score = min(1.0, (brightness - 0.2) / 0.6) # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å 0.2-0.8
        lighting_score = max(0.0, lighting_score)

        overall_quality = (sharpness_score + lighting_score) / 2
        return overall_quality

    def _extract_date_from_file(self, file_path: str) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞ –∏–ª–∏ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ EXIF (–µ—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ JPEG)
            from PIL import Image
            from PIL.ExifTags import TAGS
            
            if file_path.lower().endswith(('.jpg', '.jpeg')):
                with Image.open(file_path) as img:
                    exif_data = img._getexif()
                    if exif_data:
                        for tag_id, value in exif_data.items():
                            tag = TAGS.get(tag_id, tag_id)
                            if tag == 'DateTimeOriginal' or tag == 'DateTimeDigitized':
                                return datetime.strptime(value, '%Y:%m:%d %H:%M:%S')
            
            # –ï—Å–ª–∏ EXIF –Ω–µ—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (YMD_HMS_...)
            filename = os.path.basename(file_path)
            # –ü—Ä–∏–º–µ—Ä: 20230115_123045_image.jpg
            match = re.search(r'(\d{4})(\d{2})(\d{2})_(\d{2})(\d{2})(\d{2})', filename)
            if match:
                year, month, day, hour, minute, second = map(int, match.groups())
                return datetime(year, month, day, hour, minute, second)
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞
            timestamp = os.path.getmtime(file_path)
            return datetime.fromtimestamp(timestamp)

        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None

    def _format_processing_log(self, results: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ª–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        log_entries = []
        for res in results['successful']:
            log_entries.append(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {res['file_path']}")
        for err in results['errors']:
            log_entries.append(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {err['file_path']}: {err['error']}")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        max_log_lines = 100
        if len(log_entries) > max_log_lines:
            log_entries = ["... (—Å–æ–∫—Ä–∞—â–µ–Ω–æ) ..."] + log_entries[-max_log_lines+1:]
            
        return "\n".join(log_entries)

    def _get_custom_css(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π CSS –¥–ª—è Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
        return """
            .gradio-container {
                max-width: 1200px;
                margin: auto;
                font-family: 'Roboto', sans-serif;
            }
            #global_status {
                text-align: center;
                font-size: 1.2em;
                padding: 10px;
                border-radius: 8px;
                margin-bottom: 20px;
            }
            .status-ready {
                background-color: #e6ffe6;
                color: #006600;
                border: 1px solid #00cc00;
            }
            .status-error {
                background-color: #ffe6e6;
                color: #cc0000;
                border: 1px solid #ff0000;
            }
            .status-success {
                background-color: #e6e6ff;
                color: #0000cc;
                border: 1px solid #0000ff;
            }
            .alert-info {
                background-color: #e0f2f7;
                color: #265c7c;
                border: 1px solid #82c4e0;
                padding: 10px;
                border-radius: 5px;
                margin-top: 15px;
            }
            .alert-danger {
                background-color: #f8d7da;
                color: #721c24;
                border: 1px solid #f5c6cb;
                padding: 10px;
                border-radius: 5px;
                margin-top: 15px;
                font-weight: bold;
            }
            h1, h3 {
                color: #333;
            }
            .gr-button.primary {
                background-color: #4CAF50;
                color: white;
            }
            .gr-button.secondary {
                background-color: #f44336;
                color: white;
            }
            .gr-button.stop {
                background-color: #ff9800;
                color: white;
            }
        """

    def _stop_processing(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É."""
        logger.info("–ó–∞–ø—Ä–æ—à–µ–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        self.analysis_state['processing'] = False
        return (
            "<div class='status-error'>üõë –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞</div>",
            gr.update(visible=True),
            gr.update(visible=False)
        )

    def create_3d_anomaly_visualization(self, landmarks_3d: np.ndarray,
                                   anomaly_scores: np.ndarray) -> go.Figure:
        """–°–æ–∑–¥–∞–µ—Ç 3D-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –ª–∏—Ü–∞ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π"""
        if landmarks_3d.size == 0 or anomaly_scores.size == 0:
            logging.warning("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å 3D-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç landmarks –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π.")
            return go.Figure() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Ñ–∏–≥—É—Ä—É

        if landmarks_3d.shape[0] != anomaly_scores.shape[0]:
            logging.warning("–†–∞–∑–º–µ—Ä—ã landmarks –∏ anomaly_scores –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π.")
            # –ú–æ–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å –∏–ª–∏ –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç—É—é —Ñ–∏–≥—É—Ä—É
            return go.Figure()

        fig = go.Figure(data=[go.Scatter3d(
            x=landmarks_3d[:, 0],
            y=landmarks_3d[:, 1], 
            z=landmarks_3d[:, 2],
            mode='markers',
            marker=dict(
                size=5,
                color=anomaly_scores, # –¶–≤–µ—Ç –ø–æ –æ—Ü–µ–Ω–∫–∞–º –∞–Ω–æ–º–∞–ª–∏–π
                colorscale='Viridis', # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞
                colorbar=dict(title='–û—Ü–µ–Ω–∫–∞ –∞–Ω–æ–º–∞–ª–∏–∏'),
                showscale=True
            )
        )])

        fig.update_layout(
            title='3D –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ê–Ω–æ–º–∞–ª–∏–π –õ–∏—Ü–∞',
            scene=dict(
                xaxis_title='X –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞',
                yaxis_title='Y –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞',
                zaxis_title='Z –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞',
                aspectmode='data' # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
            ),
            margin=dict(l=0, r=0, b=0, t=40)
        )
        return fig


# ==================== –ó–ê–ü–£–°–ö –ò–ù–¢–ï–†–§–ï–ô–°–ê ====================

def create_interface():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    
    try:
        interface = OptimizedGradioInterface()
        demo = interface.create_optimized_interface()
        
        return demo
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞: {e}")
        raise

if __name__ == "__main__":
    import re # –î–æ–±–∞–≤–ª–µ–Ω import re
    demo = create_interface()
    demo.launch(
        server_name="127.0.0.1", # –ò–∑–º–µ–Ω–µ–Ω –∞–¥—Ä–µ—Å —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ localhost –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        server_port=7860, # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        share=False, # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º share=False, —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –ø—É–±–ª–∏—á–Ω–æ–π —Å—Å—ã–ª–∫–∏
        debug=True,
        max_file_size="100mb",
        max_threads=10
    )
