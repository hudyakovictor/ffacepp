# data_manager.py
import os
import re
import json
import logging
import hashlib
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from collections import OrderedDict, defaultdict
import numpy as np
import cv2
from PIL import Image
import psutil

from core_config import get_config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# === –ö–û–ù–°–¢–ê–ù–¢–´ –ò –†–ï–ì–£–õ–Ø–†–ù–´–ï –í–´–†–ê–ñ–ï–ù–ò–Ø ===

# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ DD_MM_YY[-SEQ].jpg
FILENAME_PATTERN = re.compile(r'^(\d{2})_(\d{2})_(\d{2})(?:-(\d+))?\.jpe?g$', re.IGNORECASE)

# –î–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è –í–ª–∞–¥–∏–º–∏—Ä–∞ –ü—É—Ç–∏–Ω–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞
PUTIN_BIRTH_DATE = datetime.date(1952, 10, 7)

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
SUPPORTED_FORMATS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}

# –ü–æ—Ä–æ–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
QUALITY_THRESHOLDS = {
    'min_resolution': (400, 400),
    'max_resolution': (2000, 2000),
    'min_brightness': 50,
    'max_brightness': 200,
    'min_contrast': 0.3,
    'blur_threshold': 100.0,
    'noise_threshold': 0.1,
    'min_file_size': 10240,  # 10 KB
    'max_file_size': 50 * 1024 * 1024  # 50 MB
}

# === –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===

@dataclass
class ImageMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    filepath: str
    filename: str
    date: datetime.date
    age_on_date: float
    sequence: Optional[int]
    file_size: int
    file_hash: str
    resolution: Tuple[int, int]
    quality_score: float
    quality_flags: List[str]
    processing_status: str = "pending"
    error_message: Optional[str] = None

@dataclass
class QualityAssessment:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    overall_score: float
    brightness_score: float
    contrast_score: float
    sharpness_score: float
    noise_score: float
    resolution_score: float
    flags: List[str]
    is_valid: bool

@dataclass
class ChronologicalIndex:
    """–•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    images_by_date: OrderedDict[datetime.date, List[ImageMetadata]]
    images_by_year: Dict[int, List[ImageMetadata]]
    total_images: int
    date_range: Tuple[datetime.date, datetime.date]
    age_range: Tuple[float, float]
    quality_stats: Dict[str, float]
    gaps_analysis: List[Dict[str, Any]]

@dataclass
class HistoricalEvent:
    """–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å–æ–±—ã—Ç–∏–µ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
    date: datetime.date
    title: str
    category: str
    importance: int  # 1-5
    description: str

# === –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ú–ï–ù–ï–î–ñ–ï–†–ê –î–ê–ù–ù–´–• ===

class DataManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self):
        self.config = get_config()
        self.cache_dir = Path("./cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.chronological_index: Optional[ChronologicalIndex] = None
        self.historical_events: List[HistoricalEvent] = []
        self.processing_stats = {
            'total_processed': 0,
            'valid_images': 0,
            'invalid_images': 0,
            'parsing_errors': 0,
            'quality_failures': 0
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
        self._load_historical_events()
        
        logger.info("DataManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def parse_date_from_filename(self, filename: str) -> Optional[Tuple[datetime.date, Optional[int]]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD_MM_YY[-SEQ].jpg
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–¥–∞—Ç–∞, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            match = FILENAME_PATTERN.match(filename)
            if not match:
                logger.warning(f"–ò–º—è —Ñ–∞–π–ª–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É: {filename}")
                return None
            
            day, month, year, sequence = match.groups()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–≤—É–∑–Ω–∞—á–Ω–æ–≥–æ –≥–æ–¥–∞ –≤ —á–µ—Ç—ã—Ä–µ—Ö–∑–Ω–∞—á–Ω—ã–π
            year_int = int(year)
            if year_int >= 50:  # 50-99 -> 1950-1999
                full_year = 1900 + year_int
            else:  # 00-49 -> 2000-2049
                full_year = 2000 + year_int
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –¥–∞—Ç—ã
            date_obj = datetime.date(full_year, int(month), int(day))
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç—ã (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ—Å–ª–µ —Ä–æ–∂–¥–µ–Ω–∏—è –ü—É—Ç–∏–Ω–∞)
            if date_obj < PUTIN_BIRTH_DATE:
                logger.warning(f"–î–∞—Ç–∞ {date_obj} —Ä–∞–Ω—å—à–µ –¥–∞—Ç—ã —Ä–æ–∂–¥–µ–Ω–∏—è –ü—É—Ç–∏–Ω–∞: {filename}")
                return None
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç—ã (–Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –±—É–¥—É—â–µ–º)
            if date_obj > datetime.date.today():
                logger.warning(f"–î–∞—Ç–∞ {date_obj} –≤ –±—É–¥—É—â–µ–º: {filename}")
                return None
            
            seq = int(sequence) if sequence else None
            
            logger.debug(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –¥–∞—Ç–∞ {date_obj}, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å {seq} –∏–∑ —Ñ–∞–π–ª–∞ {filename}")
            return date_obj, seq
            
        except ValueError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞ {filename}: {e}")
            return None
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {filename}: {e}")
            return None

    def calculate_putin_age_on_date(self, date: datetime.date) -> float:
        """
        –†–∞—Å—á–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞ –ü—É—Ç–∏–Ω–∞ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∞—Ç—É
        
        Args:
            date: –î–∞—Ç–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞
            
        Returns:
            –í–æ–∑—Ä–∞—Å—Ç –≤ –≥–æ–¥–∞—Ö (—Å —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–æ –¥–Ω—è)
        """
        try:
            delta = date - PUTIN_BIRTH_DATE
            age_years = delta.days / 365.25  # –£—á–µ—Ç –≤–∏—Å–æ–∫–æ—Å–Ω—ã—Ö –ª–µ—Ç
            return round(age_years, 2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ –¥–ª—è –¥–∞—Ç—ã {date}: {e}")
            return 0.0

    def validate_image_quality_for_analysis(self, filepath: str) -> QualityAssessment:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –û–±—ä–µ–∫—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        flags = []
        scores = {}
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            if not os.path.exists(filepath):
                return QualityAssessment(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
                                       ["file_not_found"], False)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
            file_size = os.path.getsize(filepath)
            if file_size < QUALITY_THRESHOLDS['min_file_size']:
                flags.append("file_too_small")
            elif file_size > QUALITY_THRESHOLDS['max_file_size']:
                flags.append("file_too_large")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            try:
                image = cv2.imread(filepath)
                if image is None:
                    return QualityAssessment(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
                                           ["cannot_read_image"], False)
                
                height, width = image.shape[:2]
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
                min_w, min_h = QUALITY_THRESHOLDS['min_resolution']
                max_w, max_h = QUALITY_THRESHOLDS['max_resolution']
                
                if width < min_w or height < min_h:
                    flags.append("resolution_too_low")
                    scores['resolution_score'] = 0.3
                elif width > max_w or height > max_h:
                    flags.append("resolution_too_high")
                    scores['resolution_score'] = 0.8
                else:
                    scores['resolution_score'] = 1.0
                
                # –û—Ü–µ–Ω–∫–∞ —è—Ä–∫–æ—Å—Ç–∏
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                mean_brightness = np.mean(gray)
                
                if mean_brightness < QUALITY_THRESHOLDS['min_brightness']:
                    flags.append("too_dark")
                    scores['brightness_score'] = mean_brightness / QUALITY_THRESHOLDS['min_brightness']
                elif mean_brightness > QUALITY_THRESHOLDS['max_brightness']:
                    flags.append("too_bright")
                    scores['brightness_score'] = 1.0 - (mean_brightness - QUALITY_THRESHOLDS['max_brightness']) / 55
                else:
                    scores['brightness_score'] = 1.0
                
                # –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
                contrast = np.std(gray) / 255.0
                if contrast < QUALITY_THRESHOLDS['min_contrast']:
                    flags.append("low_contrast")
                    scores['contrast_score'] = contrast / QUALITY_THRESHOLDS['min_contrast']
                else:
                    scores['contrast_score'] = min(1.0, contrast / 0.5)
                
                # –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏ (Variance of Laplacian)
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                if laplacian_var < QUALITY_THRESHOLDS['blur_threshold']:
                    flags.append("blurry")
                    scores['sharpness_score'] = laplacian_var / QUALITY_THRESHOLDS['blur_threshold']
                else:
                    scores['sharpness_score'] = min(1.0, laplacian_var / 300.0)
                
                # –û—Ü–µ–Ω–∫–∞ —à—É–º–∞
                noise_level = np.std(cv2.GaussianBlur(gray, (5, 5), 0) - gray)
                if noise_level > QUALITY_THRESHOLDS['noise_threshold'] * 255:
                    flags.append("noisy")
                    scores['noise_score'] = max(0.0, 1.0 - noise_level / (QUALITY_THRESHOLDS['noise_threshold'] * 255))
                else:
                    scores['noise_score'] = 1.0
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {filepath}: {e}")
                return QualityAssessment(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
                                       ["analysis_error"], False)
            
            # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞
            overall_score = np.mean(list(scores.values()))
            is_valid = overall_score >= 0.5 and len([f for f in flags if f in 
                                                   ["file_not_found", "cannot_read_image", "analysis_error"]]) == 0
            
            return QualityAssessment(
                overall_score=overall_score,
                brightness_score=scores.get('brightness_score', 0.0),
                contrast_score=scores.get('contrast_score', 0.0),
                sharpness_score=scores.get('sharpness_score', 0.0),
                noise_score=scores.get('noise_score', 0.0),
                resolution_score=scores.get('resolution_score', 0.0),
                flags=flags,
                is_valid=is_valid
            )
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ {filepath}: {e}")
            return QualityAssessment(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
                                   ["critical_error"], False)

    def create_master_chronological_index(self, image_paths: List[str]) -> ChronologicalIndex:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Ç–µ—Ä-–∏–Ω–¥–µ–∫—Å–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏
        
        Args:
            image_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            
        Returns:
            –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å
        """
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(image_paths)} —Ñ–∞–π–ª–æ–≤")
        
        images_by_date = OrderedDict()
        images_by_year = defaultdict(list)
        valid_images = []
        
        self.processing_stats = {
            'total_processed': 0,
            'valid_images': 0,
            'invalid_images': 0,
            'parsing_errors': 0,
            'quality_failures': 0
        }
        
        for filepath in image_paths:
            self.processing_stats['total_processed'] += 1
            
            try:
                filename = os.path.basename(filepath)
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                date_result = self.parse_date_from_filename(filename)
                if date_result is None:
                    self.processing_stats['parsing_errors'] += 1
                    self.processing_stats['invalid_images'] += 1
                    continue
                
                date_obj, sequence = date_result
                
                # –†–∞—Å—á–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞
                age = self.calculate_putin_age_on_date(date_obj)
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞
                try:
                    with open(filepath, 'rb') as f:
                        file_hash = hashlib.sha256(f.read()).hexdigest()
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ö–µ—à–∞ –¥–ª—è {filepath}: {e}")
                    file_hash = "unknown"
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
                file_size = os.path.getsize(filepath)
                
                try:
                    with Image.open(filepath) as img:
                        resolution = img.size
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ {filepath}: {e}")
                    resolution = (0, 0)
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_assessment = self.validate_image_quality_for_analysis(filepath)
                
                if not quality_assessment.is_valid:
                    self.processing_stats['quality_failures'] += 1
                    self.processing_stats['invalid_images'] += 1
                    logger.warning(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ—à–ª–æ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞: {filepath}")
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                metadata = ImageMetadata(
                    filepath=filepath,
                    filename=filename,
                    date=date_obj,
                    age_on_date=age,
                    sequence=sequence,
                    file_size=file_size,
                    file_hash=file_hash,
                    resolution=resolution,
                    quality_score=quality_assessment.overall_score,
                    quality_flags=quality_assessment.flags,
                    processing_status="valid" if quality_assessment.is_valid else "quality_failed"
                )
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å—ã
                if date_obj not in images_by_date:
                    images_by_date[date_obj] = []
                images_by_date[date_obj].append(metadata)
                
                images_by_year[date_obj.year].append(metadata)
                
                if quality_assessment.is_valid:
                    valid_images.append(metadata)
                    self.processing_stats['valid_images'] += 1
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                self.processing_stats['invalid_images'] += 1
                continue
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–∞–º
        images_by_date = OrderedDict(sorted(images_by_date.items()))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –¥–∞—Ç—ã –ø–æ sequence
        for date_key in images_by_date:
            images_by_date[date_key].sort(key=lambda x: x.sequence or 0)
        
        # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if valid_images:
            dates = [img.date for img in valid_images]
            ages = [img.age_on_date for img in valid_images]
            quality_scores = [img.quality_score for img in valid_images]
            
            date_range = (min(dates), max(dates))
            age_range = (min(ages), max(ages))
            
            quality_stats = {
                'mean_quality': np.mean(quality_scores),
                'min_quality': np.min(quality_scores),
                'max_quality': np.max(quality_scores),
                'std_quality': np.std(quality_scores)
            }
        else:
            date_range = (datetime.date.today(), datetime.date.today())
            age_range = (0.0, 0.0)
            quality_stats = {'mean_quality': 0.0, 'min_quality': 0.0, 'max_quality': 0.0, 'std_quality': 0.0}
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        gaps_analysis = self._analyze_temporal_gaps(list(images_by_date.keys()))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        chronological_index = ChronologicalIndex(
            images_by_date=images_by_date,
            images_by_year=dict(images_by_year),
            total_images=len(valid_images),
            date_range=date_range,
            age_range=age_range,
            quality_stats=quality_stats,
            gaps_analysis=gaps_analysis
        )
        
        self.chronological_index = chronological_index
        
        logger.info(f"–°–æ–∑–¥–∞–Ω —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å: {len(valid_images)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π "
                   f"–∏–∑ {len(image_paths)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö")
        logger.info(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {date_range[0]} - {date_range[1]}")
        logger.info(f"–î–∏–∞–ø–∞–∑–æ–Ω –≤–æ–∑—Ä–∞—Å—Ç–æ–≤: {age_range[0]:.1f} - {age_range[1]:.1f} –ª–µ—Ç")
        
        return chronological_index

    def _analyze_temporal_gaps(self, dates: List[datetime.date]) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            dates: –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–∞—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if len(dates) < 2:
            return []
        
        gaps = []
        gap_threshold = datetime.timedelta(days=180)  # 6 –º–µ—Å—è—Ü–µ–≤
        
        for i in range(1, len(dates)):
            gap = dates[i] - dates[i-1]
            if gap > gap_threshold:
                gaps.append({
                    'start_date': dates[i-1],
                    'end_date': dates[i],
                    'duration_days': gap.days,
                    'duration_months': gap.days / 30.44,
                    'severity': 'major' if gap.days > 365 else 'minor'
                })
        
        return gaps

    def _load_historical_events(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        # –ü—Ä–∏–º–µ—Ä—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–∞–≥—Ä—É–∂–∞–ª–∏—Å—å –±—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö)
        mock_events = [
            HistoricalEvent(datetime.date(1999, 12, 31), "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏.–æ. –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞", "political", 5, 
                          "–ë–æ—Ä–∏—Å –ï–ª—å—Ü–∏–Ω –æ–±—ä—è–≤–∏–ª –æ–± –æ—Ç—Å—Ç–∞–≤–∫–µ"),
            HistoricalEvent(datetime.date(2000, 3, 26), "–ò–∑–±—Ä–∞–Ω–∏–µ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º", "political", 5,
                          "–ü–µ—Ä–≤–æ–µ –∏–∑–±—Ä–∞–Ω–∏–µ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–§"),
            HistoricalEvent(datetime.date(2004, 3, 14), "–ü–µ—Ä–µ–∏–∑–±—Ä–∞–Ω–∏–µ", "political", 4,
                          "–í—Ç–æ—Ä–æ–µ –∏–∑–±—Ä–∞–Ω–∏–µ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–§"),
            HistoricalEvent(datetime.date(2008, 5, 8), "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–µ–º—å–µ—Ä-–º–∏–Ω–∏—Å—Ç—Ä–æ–º", "political", 4,
                          "–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –ø–æ—Å—Ç –ø—Ä–µ–º—å–µ—Ä-–º–∏–Ω–∏—Å—Ç—Ä–∞"),
            HistoricalEvent(datetime.date(2012, 5, 7), "–í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –Ω–∞ –ø–æ—Å—Ç –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞", "political", 5,
                          "–¢—Ä–µ—Ç—å–µ –∏–∑–±—Ä–∞–Ω–∏–µ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–§"),
            HistoricalEvent(datetime.date(2018, 5, 7), "–ß–µ—Ç–≤–µ—Ä—Ç—ã–π —Å—Ä–æ–∫", "political", 4,
                          "–ß–µ—Ç–≤–µ—Ä—Ç–æ–µ –∏–∑–±—Ä–∞–Ω–∏–µ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–§"),
            HistoricalEvent(datetime.date(2024, 5, 7), "–ü—è—Ç—ã–π —Å—Ä–æ–∫", "political", 4,
                          "–ü—è—Ç–æ–µ –∏–∑–±—Ä–∞–Ω–∏–µ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–§")
        ]
        
        self.historical_events = mock_events
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.historical_events)} –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π")

    def correlate_with_historical_events(self, date: datetime.date, 
                                       tolerance_days: int = 30) -> List[HistoricalEvent]:
        """
        –ü–æ–∏—Å–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π —Ä—è–¥–æ–º —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç–æ–π
        
        Args:
            date: –î–∞—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            tolerance_days: –î–æ–ø—É—Å—Ç–∏–º–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –¥–Ω—è—Ö
            
        Returns:
            –°–ø–∏—Å–æ–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ–±—ã—Ç–∏–π
        """
        nearby_events = []
        tolerance = datetime.timedelta(days=tolerance_days)
        
        for event in self.historical_events:
            if abs((event.date - date).days) <= tolerance_days:
                nearby_events.append(event)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ –¥–∞—Ç–µ
        nearby_events.sort(key=lambda e: abs((e.date - date).days))
        
        return nearby_events

    def get_images_by_date_range(self, start_date: datetime.date, 
                                end_date: datetime.date) -> List[ImageMetadata]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –¥–∞—Ç
        
        Args:
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        if self.chronological_index is None:
            logger.warning("–•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω")
            return []
        
        result = []
        for date, images in self.chronological_index.images_by_date.items():
            if start_date <= date <= end_date:
                result.extend(images)
        
        return result

    def get_images_by_year(self, year: int) -> List[ImageMetadata]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –≥–æ–¥
        
        Args:
            year: –ì–æ–¥
            
        Returns:
            –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        if self.chronological_index is None:
            logger.warning("–•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω")
            return []
        
        return self.chronological_index.images_by_year.get(year, [])

    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = self.processing_stats.copy()
        
        if stats['total_processed'] > 0:
            stats['success_rate'] = stats['valid_images'] / stats['total_processed']
            stats['quality_failure_rate'] = stats['quality_failures'] / stats['total_processed']
            stats['parsing_error_rate'] = stats['parsing_errors'] / stats['total_processed']
        else:
            stats['success_rate'] = 0.0
            stats['quality_failure_rate'] = 0.0
            stats['parsing_error_rate'] = 0.0
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏
        process = psutil.Process()
        memory_info = process.memory_info()
        stats['memory_usage_mb'] = memory_info.rss / 1024 / 1024
        
        return stats

    def save_index_to_cache(self, cache_filename: str = "chronological_index.json"):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –∫—ç—à
        
        Args:
            cache_filename: –ò–º—è —Ñ–∞–π–ª–∞ –∫—ç—à–∞
        """
        if self.chronological_index is None:
            logger.warning("–ù–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        cache_path = self.cache_dir / cache_filename
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            cache_data = {
                'images_by_date': {
                    date.isoformat(): [
                        {
                            'filepath': img.filepath,
                            'filename': img.filename,
                            'date': img.date.isoformat(),
                            'age_on_date': img.age_on_date,
                            'sequence': img.sequence,
                            'file_size': img.file_size,
                            'file_hash': img.file_hash,
                            'resolution': img.resolution,
                            'quality_score': img.quality_score,
                            'quality_flags': img.quality_flags,
                            'processing_status': img.processing_status,
                            'error_message': img.error_message
                        }
                        for img in images
                    ]
                    for date, images in self.chronological_index.images_by_date.items()
                },
                'total_images': self.chronological_index.total_images,
                'date_range': [
                    self.chronological_index.date_range[0].isoformat(),
                    self.chronological_index.date_range[1].isoformat()
                ],
                'age_range': self.chronological_index.age_range,
                'quality_stats': self.chronological_index.quality_stats,
                'gaps_analysis': [
                    {
                        'start_date': gap['start_date'].isoformat(),
                        'end_date': gap['end_date'].isoformat(),
                        'duration_days': gap['duration_days'],
                        'duration_months': gap['duration_months'],
                        'severity': gap['severity']
                    }
                    for gap in self.chronological_index.gaps_analysis
                ],
                'processing_stats': self.processing_stats,
                'cache_timestamp': datetime.datetime.now().isoformat()
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à: {cache_path}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –≤ –∫—ç—à: {e}")

    def load_index_from_cache(self, cache_filename: str = "chronological_index.json") -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –∫—ç—à–∞
        
        Args:
            cache_filename: –ò–º—è —Ñ–∞–π–ª–∞ –∫—ç—à–∞
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        cache_path = self.cache_dir / cache_filename
        
        if not cache_path.exists():
            logger.info("–§–∞–π–ª –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            images_by_date = OrderedDict()
            
            for date_str, images_data in cache_data['images_by_date'].items():
                date_obj = datetime.date.fromisoformat(date_str)
                images_list = []
                
                for img_data in images_data:
                    metadata = ImageMetadata(
                        filepath=img_data['filepath'],
                        filename=img_data['filename'],
                        date=datetime.date.fromisoformat(img_data['date']),
                        age_on_date=img_data['age_on_date'],
                        sequence=img_data['sequence'],
                        file_size=img_data['file_size'],
                        file_hash=img_data['file_hash'],
                        resolution=tuple(img_data['resolution']),
                        quality_score=img_data['quality_score'],
                        quality_flags=img_data['quality_flags'],
                        processing_status=img_data['processing_status'],
                        error_message=img_data.get('error_message')
                    )
                    images_list.append(metadata)
                
                images_by_date[date_obj] = images_list
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ gaps_analysis
            gaps_analysis = []
            for gap_data in cache_data['gaps_analysis']:
                gap = {
                    'start_date': datetime.date.fromisoformat(gap_data['start_date']),
                    'end_date': datetime.date.fromisoformat(gap_data['end_date']),
                    'duration_days': gap_data['duration_days'],
                    'duration_months': gap_data['duration_months'],
                    'severity': gap_data['severity']
                }
                gaps_analysis.append(gap)
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ images_by_year
            images_by_year = defaultdict(list)
            for images_list in images_by_date.values():
                for img in images_list:
                    images_by_year[img.date.year].append(img)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
            self.chronological_index = ChronologicalIndex(
                images_by_date=images_by_date,
                images_by_year=dict(images_by_year),
                total_images=cache_data['total_images'],
                date_range=(
                    datetime.date.fromisoformat(cache_data['date_range'][0]),
                    datetime.date.fromisoformat(cache_data['date_range'][1])
                ),
                age_range=tuple(cache_data['age_range']),
                quality_stats=cache_data['quality_stats'],
                gaps_analysis=gaps_analysis
            )
            
            self.processing_stats = cache_data.get('processing_stats', {})
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞: {cache_path}")
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.chronological_index.total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –∫—ç—à–∞: {e}")
            return False

    def generate_quality_report(self) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç—á–µ—Ç–æ–º –æ –∫–∞—á–µ—Å—Ç–≤–µ
        """
        if self.chronological_index is None:
            return {"error": "–ò–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω"}
        
        all_images = []
        for images_list in self.chronological_index.images_by_date.values():
            all_images.extend(images_list)
        
        if not all_images:
            return {"error": "–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        quality_scores = [img.quality_score for img in all_images]
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ñ–ª–∞–≥–∞–º –∫–∞—á–µ—Å—Ç–≤–∞
        flag_counts = defaultdict(int)
        for img in all_images:
            for flag in img.quality_flags:
                flag_counts[flag] += 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ–¥–∞–º
        quality_by_year = defaultdict(list)
        for img in all_images:
            quality_by_year[img.date.year].append(img.quality_score)
        
        year_stats = {}
        for year, scores in quality_by_year.items():
            year_stats[year] = {
                'count': len(scores),
                'mean_quality': np.mean(scores),
                'min_quality': np.min(scores),
                'max_quality': np.max(scores),
                'std_quality': np.std(scores)
            }
        
        report = {
            'total_images': len(all_images),
            'overall_quality_stats': {
                'mean': np.mean(quality_scores),
                'median': np.median(quality_scores),
                'std': np.std(quality_scores),
                'min': np.min(quality_scores),
                'max': np.max(quality_scores)
            },
            'quality_distribution': {
                'excellent': len([s for s in quality_scores if s >= 0.9]),
                'good': len([s for s in quality_scores if 0.7 <= s < 0.9]),
                'fair': len([s for s in quality_scores if 0.5 <= s < 0.7]),
                'poor': len([s for s in quality_scores if s < 0.5])
            },
            'common_issues': dict(flag_counts),
            'quality_by_year': year_stats,
            'processing_stats': self.get_processing_statistics()
        }
        
        return report

# === –§–£–ù–ö–¶–ò–ò –°–ê–ú–û–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===

def self_test():
    """–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è data_manager"""
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è data_manager...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        dm = DataManager()
        
        # –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã
        test_cases = [
            ("01_01_00.jpg", datetime.date(2000, 1, 1), None),
            ("15_06_99.jpg", datetime.date(1999, 6, 15), None),
            ("31_12_23-2.jpg", datetime.date(2023, 12, 31), 2),
            ("invalid.jpg", None, None)
        ]
        
        for filename, expected_date, expected_seq in test_cases:
            result = dm.parse_date_from_filename(filename)
            if expected_date is None:
                assert result is None, f"–û–∂–∏–¥–∞–ª—Å—è None –¥–ª—è {filename}"
            else:
                assert result is not None, f"–û–∂–∏–¥–∞–ª—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {filename}"
                date, seq = result
                assert date == expected_date, f"–ù–µ–≤–µ—Ä–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è {filename}"
                assert seq == expected_seq, f"–ù–µ–≤–µ—Ä–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è {filename}"
        
        # –¢–µ—Å—Ç —Ä–∞—Å—á–µ—Ç–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞
        test_date = datetime.date(2000, 1, 1)
        age = dm.calculate_putin_age_on_date(test_date)
        expected_age = (test_date - PUTIN_BIRTH_DATE).days / 365.25
        assert abs(age - expected_age) < 0.01, "–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞"
        
        # –¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Å–æ–±—ã—Ç–∏—è–º–∏
        events = dm.correlate_with_historical_events(datetime.date(2000, 1, 1), 60)
        assert len(events) > 0, "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω—ã —Å–æ–±—ã—Ç–∏—è"
        
        logger.info("–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ data_manager –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è
    success = self_test()
    if success:
        print("‚úÖ –ú–æ–¥—É–ª—å data_manager —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        dm = DataManager()
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(dm.historical_events)}")
        print(f"üîß –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {SUPPORTED_FORMATS}")
        print(f"üìè –ü–æ—Ä–æ–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        print(f"üíæ –ö—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dm.cache_dir}")
    else:
        print("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ –º–æ–¥—É–ª–µ data_manager")
        exit(1)
