# data_processing.py
import os
import json
import logging
import asyncio
import hashlib
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field, asdict
from collections import OrderedDict, defaultdict
import numpy as np
import cv2
from PIL import Image
import torch
import psutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import pickle
import msgpack
from functools import lru_cache
import time

from core_config import get_config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# === –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

# –†–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
TARGET_SIZE = (800, 800)
INSIGHTFACE_SIZE = (112, 112)

# –ü–æ—Ä–æ–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
QUALITY_THRESHOLDS = {
    'min_variance_of_laplacian': 100.0,  # –ü–æ—Ä–æ–≥ —Ä–∞–∑–º—ã—Ç–∏—è
    'min_shannon_entropy': 5.0,          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
    'max_shannon_entropy': 8.5,          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
    'min_brightness': 50,                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å
    'max_brightness': 200,               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å
    'min_contrast': 0.3,                 # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
    'noise_threshold': 0.15              # –ü–æ—Ä–æ–≥ —à—É–º–∞
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏
BATCH_SIZE = 8
MAX_CONCURRENT_FILES = 16
CACHE_SIZE_LIMIT_MB = 1024

# === –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===

@dataclass
class AnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    image_id: str
    filepath: str
    filename: str
    date: datetime.date
    age_on_date: float
    processing_status: str
    quality_score: float
    quality_flags: List[str]
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    landmarks_3d: Optional[np.ndarray] = None
    pose_angles: Optional[Dict[str, float]] = None
    identity_metrics: Optional[Dict[str, float]] = None
    embedding_vector: Optional[np.ndarray] = None
    texture_metrics: Optional[Dict[str, Any]] = None
    temporal_score: Optional[float] = None
    authenticity_score: Optional[float] = None
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    processing_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # –ö—ç—à-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    cache_hit: bool = False
    cache_key: Optional[str] = None

@dataclass
class BatchProcessingStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    total_files: int = 0
    processed_files: int = 0
    successful_files: int = 0
    failed_files: int = 0
    skipped_files: int = 0
    
    total_processing_time_ms: float = 0.0
    average_processing_time_ms: float = 0.0
    peak_memory_usage_mb: float = 0.0
    
    quality_distribution: Dict[str, int] = field(default_factory=dict)
    error_counts: Dict[str, int] = field(default_factory=dict)
    
    start_time: Optional[datetime.datetime] = None
    end_time: Optional[datetime.datetime] = None

# === –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–• ===

class DataProcessing:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self):
        self.config = get_config()
        self.cache_dir = Path("./cache/data_processing")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.results_cache: Dict[str, AnalysisResult] = {}
        self.cache_size_mb = 0.0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.batch_stats = BatchProcessingStats()
        
        # –ü—É–ª—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        self.process_pool = ProcessPoolExecutor(max_workers=2)
        
        # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        self.file_semaphore = asyncio.Semaphore(MAX_CONCURRENT_FILES)
        
        logger.info("DataProcessing –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def preprocessing_pipeline(self, image_path: str, target_size: Tuple[int, int] = TARGET_SIZE) -> Optional[Dict[str, Any]]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            target_size: –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 800x800)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            start_time = time.time()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            if not os.path.exists(image_path):
                logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {image_path}")
                return None
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            original_image = cv2.imread(image_path)
            if original_image is None:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
                return None
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ø–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏–∑–±–µ–≥–∞–µ–º –º—É—Ç–∞—Ü–∏–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
            image = original_image.copy()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
            original_height, original_width = image.shape[:2]
            
            # –†–µ—Å–∞–π–∑ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ
            if (original_width, original_height) != target_size:
                image = cv2.resize(image, target_size, interpolation=cv2.INTER_CUBIC)
                logger.debug(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–æ —Å {original_width}x{original_height} –¥–æ {target_size}")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –¥–ª—è InsightFace (112x112)
            insightface_image = cv2.resize(image, INSIGHTFACE_SIZE, interpolation=cv2.INTER_AREA)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            insightface_rgb = cv2.cvtColor(insightface_image, cv2.COLOR_BGR2RGB)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
            image_normalized = image_rgb.astype(np.float32) / 255.0
            insightface_normalized = insightface_rgb.astype(np.float32) / 255.0
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–µ–≤—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # CLAHE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced_image = clahe.apply(gray_image)
            contrast_factor = np.std(enhanced_image) / np.std(gray_image) if np.std(gray_image) > 0 else 1.0
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = {
                'original_image': original_image,
                'processed_image': image,
                'image_rgb': image_rgb,
                'image_normalized': image_normalized,
                'insightface_image': insightface_image,
                'insightface_rgb': insightface_rgb,
                'insightface_normalized': insightface_normalized,
                'gray_image': gray_image,
                'enhanced_image': enhanced_image,
                'contrast_factor': contrast_factor,
                'original_size': (original_width, original_height),
                'target_size': target_size,
                'processing_time_ms': (time.time() - start_time) * 1000
            }
            
            logger.debug(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {result['processing_time_ms']:.1f}–º—Å: {image_path}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
            return None

    def assess_image_quality(self, image: np.ndarray, enhanced_image: Optional[np.ndarray] = None) -> Tuple[float, List[str]]:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            image: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ü–≤–µ—Ç–Ω–æ–µ)
            enhanced_image: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–æ–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞, —Å–ø–∏—Å–æ–∫ —Ñ–ª–∞–≥–æ–≤ –ø—Ä–æ–±–ª–µ–º)
        """
        try:
            flags = []
            scores = {}
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image.copy()
            
            # 1. –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏ (Variance of Laplacian)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            if laplacian_var < QUALITY_THRESHOLDS['min_variance_of_laplacian']:
                flags.append("blurry")
                scores['sharpness'] = laplacian_var / QUALITY_THRESHOLDS['min_variance_of_laplacian']
            else:
                scores['sharpness'] = min(1.0, laplacian_var / 300.0)
            
            # 2. –û—Ü–µ–Ω–∫–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ)
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            hist_norm = hist / hist.sum()
            hist_norm = hist_norm[hist_norm > 0]  # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏ –¥–ª—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∞
            entropy = -np.sum(hist_norm * np.log2(hist_norm))
            
            if entropy < QUALITY_THRESHOLDS['min_shannon_entropy']:
                flags.append("low_entropy")
                scores['entropy'] = entropy / QUALITY_THRESHOLDS['min_shannon_entropy']
            elif entropy > QUALITY_THRESHOLDS['max_shannon_entropy']:
                flags.append("high_entropy")
                scores['entropy'] = 1.0 - (entropy - QUALITY_THRESHOLDS['max_shannon_entropy']) / 2.0
            else:
                scores['entropy'] = 1.0
            
            # 3. –û—Ü–µ–Ω–∫–∞ —è—Ä–∫–æ—Å—Ç–∏
            mean_brightness = np.mean(gray)
            if mean_brightness < QUALITY_THRESHOLDS['min_brightness']:
                flags.append("too_dark")
                scores['brightness'] = mean_brightness / QUALITY_THRESHOLDS['min_brightness']
            elif mean_brightness > QUALITY_THRESHOLDS['max_brightness']:
                flags.append("too_bright")
                scores['brightness'] = 1.0 - (mean_brightness - QUALITY_THRESHOLDS['max_brightness']) / 55
            else:
                scores['brightness'] = 1.0
            
            # 4. –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
            contrast = np.std(gray) / 255.0
            if contrast < QUALITY_THRESHOLDS['min_contrast']:
                flags.append("low_contrast")
                scores['contrast'] = contrast / QUALITY_THRESHOLDS['min_contrast']
            else:
                scores['contrast'] = min(1.0, contrast / 0.5)
            
            # 5. –û—Ü–µ–Ω–∫–∞ —à—É–º–∞
            if enhanced_image is not None:
                noise_level = np.std(cv2.GaussianBlur(gray, (5, 5), 0) - gray) / 255.0
                if noise_level > QUALITY_THRESHOLDS['noise_threshold']:
                    flags.append("noisy")
                    scores['noise'] = max(0.0, 1.0 - noise_level / QUALITY_THRESHOLDS['noise_threshold'])
                else:
                    scores['noise'] = 1.0
            else:
                scores['noise'] = 1.0
            
            # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞
            overall_score = np.mean(list(scores.values()))
            
            logger.debug(f"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {overall_score:.3f}, —Ñ–ª–∞–≥–∏: {flags}")
            return overall_score, flags
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return 0.0, ["quality_assessment_error"]

    def generate_cache_key(self, filepath: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            SHA-256 —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
        """
        try:
            with open(filepath, 'rb') as f:
                file_content = f.read()
            return hashlib.sha256(file_content).hexdigest()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è {filepath}: {e}")
            return hashlib.sha256(filepath.encode()).hexdigest()

    def save_to_cache(self, cache_key: str, result: AnalysisResult):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à
        
        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        try:
            cache_file = self.cache_dir / f"{cache_key}.msgpack"
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            cache_data = asdict(result)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy –º–∞—Å—Å–∏–≤–æ–≤ –≤ —Å–ø–∏—Å–∫–∏
            for key, value in cache_data.items():
                if isinstance(value, np.ndarray):
                    cache_data[key] = value.tolist()
                elif key == 'date' and hasattr(value, 'isoformat'):
                    cache_data[key] = value.isoformat()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ msgpack
            with open(cache_file, 'wb') as f:
                msgpack.pack(cache_data, f)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫—ç—à–∞
            file_size_mb = cache_file.stat().st_size / (1024 * 1024)
            self.cache_size_mb += file_size_mb
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
            self.results_cache[cache_key] = result
            
            logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à: {cache_key}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à {cache_key}: {e}")

    def load_from_cache(self, cache_key: str) -> Optional[AnalysisResult]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫—ç—à–∞
        
        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ –∏–ª–∏ None
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ø–∞–º—è—Ç–∏
            if cache_key in self.results_cache:
                logger.debug(f"–ö—ç—à-–ø–æ–ø–∞–¥–∞–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏: {cache_key}")
                return self.results_cache[cache_key]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∏—Å–∫–µ
            cache_file = self.cache_dir / f"{cache_key}.msgpack"
            if not cache_file.exists():
                return None
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
            with open(cache_file, 'rb') as f:
                cache_data = msgpack.unpack(f)
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            if 'date' in cache_data and isinstance(cache_data['date'], str):
                cache_data['date'] = datetime.date.fromisoformat(cache_data['date'])
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ numpy –º–∞—Å—Å–∏–≤–æ–≤
            for key, value in cache_data.items():
                if isinstance(value, list) and key.endswith(('_3d', '_vector', '_metrics')):
                    cache_data[key] = np.array(value) if value else None
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = AnalysisResult(**cache_data)
            result.cache_hit = True
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
            self.results_cache[cache_key] = result
            
            logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞: {cache_key}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫—ç—à–∞ {cache_key}: {e}")
            return None

    def cleanup_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞"""
        try:
            if self.cache_size_mb <= CACHE_SIZE_LIMIT_MB:
                return
            
            logger.info(f"–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞: —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä {self.cache_size_mb:.1f}–ú–ë")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∫—ç—à–∞ —Å –≤—Ä–µ–º–µ–Ω–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
            cache_files = []
            for cache_file in self.cache_dir.glob("*.msgpack"):
                mtime = cache_file.stat().st_mtime
                size_mb = cache_file.stat().st_size / (1024 * 1024)
                cache_files.append((cache_file, mtime, size_mb))
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–º–∏)
            cache_files.sort(key=lambda x: x[1])
            
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
            removed_size = 0.0
            target_size = CACHE_SIZE_LIMIT_MB * 0.8  # –û—á–∏—â–∞–µ–º –¥–æ 80% –æ—Ç –ª–∏–º–∏—Ç–∞
            
            for cache_file, mtime, size_mb in cache_files:
                if self.cache_size_mb - removed_size <= target_size:
                    break
                
                try:
                    cache_file.unlink()
                    removed_size += size_mb
                    
                    # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –ø–∞–º—è—Ç–∏
                    cache_key = cache_file.stem
                    if cache_key in self.results_cache:
                        del self.results_cache[cache_key]
                        
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –∫—ç—à–∞ {cache_file}: {e}")
            
            self.cache_size_mb -= removed_size
            logger.info(f"–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {removed_size:.1f}–ú–ë")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    async def process_single_file_async(self, filepath: str, data_manager=None) -> Optional[AnalysisResult]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            data_manager: –≠–∫–∑–µ–º–ø–ª—è—Ä DataManager –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        async with self.file_semaphore:
            try:
                start_time = time.time()
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞
                cache_key = self.generate_cache_key(filepath)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
                cached_result = self.load_from_cache(cache_key)
                if cached_result is not None:
                    cached_result.cache_hit = True
                    return cached_result
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞
                filename = os.path.basename(filepath)
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏ –≤–æ–∑—Ä–∞—Å—Ç–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω data_manager)
                if data_manager:
                    date_result = data_manager.parse_date_from_filename(filename)
                    if date_result:
                        date_obj, sequence = date_result
                        age = data_manager.calculate_putin_age_on_date(date_obj)
                    else:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –∏–∑ {filename}")
                        date_obj = datetime.date.today()
                        age = 0.0
                else:
                    date_obj = datetime.date.today()
                    age = 0.0
                
                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                processed_data = self.preprocessing_pipeline(filepath)
                if processed_data is None:
                    return self._create_failed_result(filepath, filename, date_obj, age, 
                                                    "preprocessing_failed", cache_key)
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score, quality_flags = self.assess_image_quality(
                    processed_data['image_rgb'], 
                    processed_data['enhanced_image']
                )
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                if quality_score < 0.5:
                    processing_status = "quality_failed"
                    logger.warning(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ—à–ª–æ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞: {filepath} (–±–∞–ª–ª: {quality_score:.3f})")
                else:
                    processing_status = "ready_for_analysis"
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                result = AnalysisResult(
                    image_id=cache_key,
                    filepath=filepath,
                    filename=filename,
                    date=date_obj,
                    age_on_date=age,
                    processing_status=processing_status,
                    quality_score=quality_score,
                    quality_flags=quality_flags,
                    processing_time_ms=(time.time() - start_time) * 1000,
                    memory_usage_mb=psutil.Process().memory_info().rss / (1024 * 1024),
                    cache_key=cache_key,
                    cache_hit=False
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
                self.save_to_cache(cache_key, result)
                
                logger.debug(f"–§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {filename} (—Å—Ç–∞—Ç—É—Å: {processing_status})")
                return result
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                return self._create_failed_result(filepath, os.path.basename(filepath), 
                                                datetime.date.today(), 0.0, "processing_error", 
                                                cache_key, str(e))

    def _create_failed_result(self, filepath: str, filename: str, date: datetime.date, 
                            age: float, status: str, cache_key: str, 
                            error_message: str = None) -> AnalysisResult:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –Ω–µ—É–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        return AnalysisResult(
            image_id=cache_key,
            filepath=filepath,
            filename=filename,
            date=date,
            age_on_date=age,
            processing_status=status,
            quality_score=0.0,
            quality_flags=["processing_failed"],
            processing_time_ms=0.0,
            memory_usage_mb=0.0,
            error_message=error_message,
            cache_key=cache_key,
            cache_hit=False
        )

    async def process_batch_async(self, file_paths: List[str], data_manager=None) -> List[AnalysisResult]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            data_manager: –≠–∫–∑–µ–º–ø–ª—è—Ä DataManager
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.batch_stats = BatchProcessingStats(
                total_files=len(file_paths),
                start_time=datetime.datetime.now()
            )
            
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫—É {len(file_paths)} —Ñ–∞–π–ª–æ–≤")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            tasks = []
            for filepath in file_paths:
                task = asyncio.create_task(self.process_single_file_async(filepath, data_manager))
                tasks.append(task)
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–¥–∞—á
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_paths[i]}: {result}")
                    self.batch_stats.failed_files += 1
                    self.batch_stats.error_counts[str(type(result).__name__)] = \
                        self.batch_stats.error_counts.get(str(type(result).__name__), 0) + 1
                elif result is None:
                    self.batch_stats.failed_files += 1
                    self.batch_stats.error_counts["null_result"] = \
                        self.batch_stats.error_counts.get("null_result", 0) + 1
                else:
                    valid_results.append(result)
                    self.batch_stats.processed_files += 1
                    
                    if result.processing_status in ["ready_for_analysis", "quality_failed"]:
                        self.batch_stats.successful_files += 1
                    else:
                        self.batch_stats.failed_files += 1
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                    if result.quality_score >= 0.8:
                        quality_category = "excellent"
                    elif result.quality_score >= 0.6:
                        quality_category = "good"
                    elif result.quality_score >= 0.4:
                        quality_category = "fair"
                    else:
                        quality_category = "poor"
                    
                    self.batch_stats.quality_distribution[quality_category] = \
                        self.batch_stats.quality_distribution.get(quality_category, 0) + 1
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏
                    self.batch_stats.total_processing_time_ms += result.processing_time_ms
                    self.batch_stats.peak_memory_usage_mb = max(
                        self.batch_stats.peak_memory_usage_mb, 
                        result.memory_usage_mb
                    )
            
            # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.batch_stats.end_time = datetime.datetime.now()
            if self.batch_stats.processed_files > 0:
                self.batch_stats.average_processing_time_ms = \
                    self.batch_stats.total_processing_time_ms / self.batch_stats.processed_files
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            self.cleanup_cache()
            
            logger.info(f"–ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {self.batch_stats.successful_files}/{len(file_paths)} —É—Å–ø–µ—à–Ω–æ")
            return valid_results
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return []

    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = asdict(self.batch_stats)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ
        stats['cache_info'] = {
            'cache_size_mb': self.cache_size_mb,
            'cache_entries': len(self.results_cache),
            'cache_hit_rate': self._calculate_cache_hit_rate()
        }
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        process = psutil.Process()
        memory_info = process.memory_info()
        stats['system_info'] = {
            'memory_usage_mb': memory_info.rss / (1024 * 1024),
            'cpu_percent': process.cpu_percent(),
            'thread_count': process.num_threads()
        }
        
        return stats

    def _calculate_cache_hit_rate(self) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à"""
        if not self.results_cache:
            return 0.0
        
        cache_hits = sum(1 for result in self.results_cache.values() if result.cache_hit)
        return cache_hits / len(self.results_cache) * 100

    def export_results_to_json(self, results: List[AnalysisResult], output_path: str):
        """
        –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON
        
        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        try:
            export_data = {
                'metadata': {
                    'export_time': datetime.datetime.now().isoformat(),
                    'total_results': len(results),
                    'processing_stats': self.get_processing_statistics()
                },
                'results': []
            }
            
            for result in results:
                result_dict = asdict(result)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤
                if 'date' in result_dict and hasattr(result_dict['date'], 'isoformat'):
                    result_dict['date'] = result_dict['date'].isoformat()
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy –º–∞—Å—Å–∏–≤–æ–≤
                for key, value in result_dict.items():
                    if isinstance(value, np.ndarray):
                        result_dict[key] = value.tolist()
                
                export_data['results'].append(result_dict)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_path}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")

    def clear_cache(self):
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            self.results_cache.clear()
            
            # –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∫—ç—à–∞
            for cache_file in self.cache_dir.glob("*.msgpack"):
                cache_file.unlink()
            
            self.cache_size_mb = 0.0
            logger.info("–ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—É–ª–æ–≤"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=False)
            if hasattr(self, 'process_pool'):
                self.process_pool.shutdown(wait=False)
        except:
            pass

# === –ö–õ–ê–°–° –ê–ì–†–ï–ì–ê–¢–û–†–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===

class ResultsAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    def __init__(self):
        self.results: List[AnalysisResult] = []
        self.results_by_status: Dict[str, List[AnalysisResult]] = defaultdict(list)
        self.results_by_date: Dict[datetime.date, List[AnalysisResult]] = defaultdict(list)
        
    def add_results(self, results: List[AnalysisResult]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä"""
        for result in results:
            self.results.append(result)
            self.results_by_status[result.processing_status].append(result)
            self.results_by_date[result.date].append(result)
    
    def get_summary_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.results:
            return {"error": "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        total_results = len(self.results)
        quality_scores = [r.quality_score for r in self.results]
        processing_times = [r.processing_time_ms for r in self.results if r.processing_time_ms > 0]
        
        return {
            'total_files': total_results,
            'status_distribution': {status: len(results) for status, results in self.results_by_status.items()},
            'quality_statistics': {
                'mean': np.mean(quality_scores),
                'median': np.median(quality_scores),
                'std': np.std(quality_scores),
                'min': np.min(quality_scores),
                'max': np.max(quality_scores)
            },
            'performance_statistics': {
                'mean_processing_time_ms': np.mean(processing_times) if processing_times else 0,
                'total_processing_time_ms': np.sum(processing_times) if processing_times else 0
            },
            'date_range': {
                'earliest': min(r.date for r in self.results).isoformat(),
                'latest': max(r.date for r in self.results).isoformat(),
                'unique_dates': len(self.results_by_date)
            }
        }
    
    def filter_by_quality(self, min_quality: float = 0.5) -> List[AnalysisResult]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
        return [r for r in self.results if r.quality_score >= min_quality]
    
    def filter_by_status(self, status: str) -> List[AnalysisResult]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ç–∞—Ç—É—Å—É"""
        return self.results_by_status.get(status, [])

# === –§–£–ù–ö–¶–ò–ò –°–ê–ú–û–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===

def self_test():
    """–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è data_processing"""
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è data_processing...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = DataProcessing()
        
        # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        test_image = np.random.randint(0, 255, (800, 800, 3), dtype=np.uint8)
        test_path = processor.cache_dir / "test_image.jpg"
        cv2.imwrite(str(test_path), test_image)
        
        # –¢–µ—Å—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed_data = processor.preprocessing_pipeline(str(test_path))
        assert processed_data is not None, "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å"
        assert processed_data['target_size'] == (800, 800), "–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
        
        # –¢–µ—Å—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score, flags = processor.assess_image_quality(processed_data['image_rgb'])
        assert 0.0 <= quality_score <= 1.0, "–ù–µ–≤–µ—Ä–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞"
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞ –∫—ç—à–∞
        cache_key = processor.generate_cache_key(str(test_path))
        assert len(cache_key) == 64, "–ù–µ–≤–µ—Ä–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–ª—é—á–∞ –∫—ç—à–∞"
        
        # –¢–µ—Å—Ç –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞
        aggregator = ResultsAggregator()
        test_result = AnalysisResult(
            image_id="test",
            filepath=str(test_path),
            filename="test_image.jpg",
            date=datetime.date.today(),
            age_on_date=70.0,
            processing_status="ready_for_analysis",
            quality_score=0.8,
            quality_flags=[]
        )
        
        aggregator.add_results([test_result])
        stats = aggregator.get_summary_statistics()
        assert stats['total_files'] == 1, "–ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞"
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        test_path.unlink()
        processor.clear_cache()
        
        logger.info("–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ data_processing –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è
    success = self_test()
    if success:
        print("‚úÖ –ú–æ–¥—É–ª—å data_processing —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        processor = DataProcessing()
        print(f"üìä –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {processor.cache_size_mb:.1f}–ú–ë")
        print(f"üîß –õ–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {MAX_CONCURRENT_FILES}")
        print(f"üìè –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {TARGET_SIZE}")
        print(f"üíæ –ö—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {processor.cache_dir}")
    else:
        print("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ –º–æ–¥—É–ª–µ data_processing")
        exit(1)
