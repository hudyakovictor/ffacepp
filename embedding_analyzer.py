# embedding_analyzer.py
import os
import json
import logging
import hashlib
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field, asdict
import numpy as np
import cv2
from PIL import Image
import torch
import torch.nn as nn
import onnxruntime as ort
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cosine, euclidean
from scipy.stats import zscore
import pickle
import time
import psutil
from functools import lru_cache
import threading
from collections import OrderedDict, defaultdict
import msgpack

from core_config import get_config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# === –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

# –†–∞–∑–º–µ—Ä—ã –¥–ª—è InsightFace
INSIGHTFACE_INPUT_SIZE = (112, 112)
EMBEDDING_DIMENSION = 512

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã DBSCAN –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
DBSCAN_EPS = 0.35
DBSCAN_MIN_SAMPLES = 3
DBSCAN_METRIC = 'cosine'

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
EMBEDDING_DISTANCE_THRESHOLD = 0.35
CLUSTER_CONFIDENCE_THRESHOLD = 0.7
OUTLIER_THRESHOLD = 0.5

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è InsightFace
ARCFACE_SRC = np.array([
    [30.2946, 51.6963],
    [65.5318, 51.5014],
    [48.0252, 71.7366],
    [33.5493, 92.3655],
    [62.7299, 92.2041]
], dtype=np.float32)

# === –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===

@dataclass
class EmbeddingPackage:
    """–ü–∞–∫–µ—Ç –¥–∞–Ω–Ω—ã—Ö —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –ª–∏—Ü–∞"""
    image_id: str
    filepath: str
    embedding_vector: np.ndarray  # 512-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    extraction_confidence: float
    
    # –ö–ª–∞—Å—Ç–µ—Ä–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    cluster_id: int = -1
    cluster_confidence: float = 0.0
    is_outlier: bool = False
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    alignment_quality: float = 0.0
    face_quality_score: float = 0.0
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
    nearest_neighbor_distance: float = float('inf')
    mean_cluster_distance: float = float('inf')
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    processing_time_ms: float = 0.0
    model_version: str = "w600k_r50"
    device_used: str = "cpu"
    
    # –§–ª–∞–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_flags: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

@dataclass
class ClusterManifest:
    """–ú–∞–Ω–∏—Ñ–µ—Å—Ç –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏"""
    cluster_id: int
    center_embedding: np.ndarray
    member_count: int
    first_appearance: datetime.date
    last_appearance: datetime.date
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
    intra_cluster_variance: float
    cluster_radius: float
    stability_score: float
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    appearance_gaps: List[int]  # –¥–Ω–∏ –º–µ–∂–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è–º–∏
    total_timespan_days: int
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    silhouette_score: float
    cohesion_score: float
    
    # –°–ø–∏—Å–æ–∫ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
    member_image_ids: List[str] = field(default_factory=list)

@dataclass
class IdentityTimeline:
    """–í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –ø–æ—è–≤–ª–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π"""
    clusters: Dict[int, ClusterManifest]
    total_identities: int
    date_range: Tuple[datetime.date, datetime.date]
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π
    identity_switches: List[Dict[str, Any]]
    switch_frequency: float
    
    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    dominant_identity: int
    identity_distribution: Dict[int, float]

# === –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===

class EmbeddingAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ª–∏—Ü"""
    
    def __init__(self):
        self.config = get_config()
        self.cache_dir = Path("./cache/embedding_analyzer")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –ú–æ–¥–µ–ª–∏
        self.insightface_session = None
        self.scaler = StandardScaler()
        
        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.embedding_cache: Dict[str, EmbeddingPackage] = {}
        self.cluster_cache: Dict[str, ClusterManifest] = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.processing_stats = {
            'total_processed': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'cache_hits': 0,
            'clusters_created': 0
        }
        
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.extraction_lock = threading.Lock()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self._initialize_insightface_model()
        
        logger.info("EmbeddingAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _initialize_insightface_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ InsightFace"""
        try:
            model_path = self.config.get_model_path("insightface")
            
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å InsightFace –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–ª—è ONNX Runtime
            providers = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ MPS (Apple Silicon)
            if torch.backends.mps.is_available():
                providers.append('CPUExecutionProvider')  # MPS –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è ONNX Runtime
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –ø—Ä–æ–≤–∞–π–¥–µ—Ä (MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è ONNX Runtime)")
            else:
                providers.append('CPUExecutionProvider')
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ ONNX Runtime
            self.insightface_session = ort.InferenceSession(
                model_path,
                providers=providers
            )
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            input_shape = self.insightface_session.get_inputs()[0].shape
            output_shape = self.insightface_session.get_outputs()[0].shape
            
            logger.info(f"InsightFace –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {input_shape} -> {output_shape}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ InsightFace: {e}")
            raise

    def extract_512d_face_embedding(self, image: np.ndarray, 
                                  landmarks: np.ndarray) -> Optional[EmbeddingPackage]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ 512-–º–µ—Ä–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ª–∏—Ü–∞
        
        Args:
            image: –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            landmarks: 68-—Ç–æ—á–µ—á–Ω—ã–µ –ª–∞–Ω–¥–º–∞—Ä–∫–∏ –ª–∏—Ü–∞
            
        Returns:
            –ü–∞–∫–µ—Ç —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            start_time = time.time()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_bytes = cv2.imencode('.jpg', image)[1].tobytes()
            image_id = hashlib.sha256(image_bytes).hexdigest()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if image_id in self.embedding_cache:
                self.processing_stats['cache_hits'] += 1
                cached_result = self.embedding_cache[image_id]
                cached_result.processing_time_ms = (time.time() - start_time) * 1000
                return cached_result
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ 5 –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –∏–∑ 68-—Ç–æ—á–µ—á–Ω–æ–π –º–æ–¥–µ–ª–∏
            key_points = self._extract_5_keypoints(landmarks)
            
            # –ê—Ñ—Ñ–∏–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∏—Ü–∞
            aligned_face, alignment_quality = self._align_face(image, key_points)
            
            if aligned_face is None:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–æ–≤–Ω—è—Ç—å –ª–∏—Ü–æ")
                self.processing_stats['failed_extractions'] += 1
                return None
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è InsightFace
            preprocessed = self._preprocess_for_insightface(aligned_face)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding_vector = self._run_insightface_inference(preprocessed)
            
            if embedding_vector is None:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —ç–º–±–µ–¥–¥–∏–Ω–≥")
                self.processing_stats['failed_extractions'] += 1
                return None
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding_vector = self._normalize_embedding(embedding_vector)
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞
            face_quality_score = self._assess_face_quality(aligned_face)
            
            # –†–∞—Å—á–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            extraction_confidence = self._calculate_extraction_confidence(
                alignment_quality, face_quality_score
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞
            package = EmbeddingPackage(
                image_id=image_id,
                filepath="",  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –≤—ã–∑—ã–≤–∞—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–µ–π
                embedding_vector=embedding_vector,
                extraction_confidence=extraction_confidence,
                alignment_quality=alignment_quality,
                face_quality_score=face_quality_score,
                processing_time_ms=(time.time() - start_time) * 1000,
                device_used="cpu"
            )
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            self._validate_embedding_package(package)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            self.embedding_cache[image_id] = package
            
            self.processing_stats['successful_extractions'] += 1
            self.processing_stats['total_processed'] += 1
            
            logger.debug(f"–≠–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑–≤–ª–µ—á–µ–Ω –∑–∞ {package.processing_time_ms:.1f}–º—Å")
            return package
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            self.processing_stats['failed_extractions'] += 1
            self.processing_stats['total_processed'] += 1
            return None

    def _extract_5_keypoints(self, landmarks_68: np.ndarray) -> np.ndarray:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ 5 –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –∏–∑ 68-—Ç–æ—á–µ—á–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è 5 –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ InsightFace
            # –õ–µ–≤—ã–π –≥–ª–∞–∑, –ø—Ä–∞–≤—ã–π –≥–ª–∞–∑, –Ω–æ—Å, –ª–µ–≤—ã–π —É–≥–æ–ª —Ä—Ç–∞, –ø—Ä–∞–≤—ã–π —É–≥–æ–ª —Ä—Ç–∞
            left_eye = np.mean(landmarks_68[42:48, :2], axis=0)  # –õ–µ–≤—ã–π –≥–ª–∞–∑
            right_eye = np.mean(landmarks_68[36:42, :2], axis=0)  # –ü—Ä–∞–≤—ã–π –≥–ª–∞–∑
            nose = landmarks_68[30, :2]  # –ö–æ–Ω—á–∏–∫ –Ω–æ—Å–∞
            left_mouth = landmarks_68[48, :2]  # –õ–µ–≤—ã–π —É–≥–æ–ª —Ä—Ç–∞
            right_mouth = landmarks_68[54, :2]  # –ü—Ä–∞–≤—ã–π —É–≥–æ–ª —Ä—Ç–∞
            
            keypoints = np.array([
                left_eye, right_eye, nose, left_mouth, right_mouth
            ], dtype=np.float32)
            
            return keypoints
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫: {e}")
            raise

    def _align_face(self, image: np.ndarray, keypoints: np.ndarray) -> Tuple[Optional[np.ndarray], float]:
        """–ê—Ñ—Ñ–∏–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∏—Ü–∞ –ø–æ 5 –∫–ª—é—á–µ–≤—ã–º —Ç–æ—á–∫–∞–º"""
        try:
            # –†–∞—Å—á–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
            tform = cv2.estimateAffinePartial2D(keypoints, ARCFACE_SRC)[0]
            
            if tform is None:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É")
                return None, 0.0
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            aligned = cv2.warpAffine(
                image, tform, INSIGHTFACE_INPUT_SIZE, 
                borderMode=cv2.BORDER_CONSTANT, borderValue=0
            )
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
            alignment_quality = self._assess_alignment_quality(keypoints, tform)
            
            return aligned, alignment_quality
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ª–∏—Ü–∞: {e}")
            return None, 0.0

    def _assess_alignment_quality(self, keypoints: np.ndarray, tform: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è"""
        try:
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫ –∫–ª—é—á–µ–≤—ã–º —Ç–æ—á–∫–∞–º
            keypoints_homogeneous = np.hstack([keypoints, np.ones((len(keypoints), 1))])
            transformed_keypoints = (tform @ keypoints_homogeneous.T).T
            
            # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
            distances = np.linalg.norm(transformed_keypoints - ARCFACE_SRC, axis=1)
            mean_distance = np.mean(distances)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ (—á–µ–º –º–µ–Ω—å—à–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, —Ç–µ–º –ª—É—á—à–µ)
            quality = max(0.0, 1.0 - mean_distance / 10.0)
            
            return float(quality)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: {e}")
            return 0.0

    def _preprocess_for_insightface(self, image: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è InsightFace"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1]
            image = image.astype(np.float32) / 255.0
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞–Ω–∞–ª–∞–º (ImageNet —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
            mean = np.array([0.5, 0.5, 0.5])
            std = np.array([0.5, 0.5, 0.5])
            
            image = (image - mean) / std
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è ONNX (NCHW)
            image = np.transpose(image, (2, 0, 1))
            image = np.expand_dims(image, axis=0)
            
            return image
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            raise

    def _run_insightface_inference(self, preprocessed_image: np.ndarray) -> Optional[np.ndarray]:
        """–ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ InsightFace"""
        try:
            if self.insightface_session is None:
                logger.error("–ú–æ–¥–µ–ª—å InsightFace –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                return None
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
            input_name = self.insightface_session.get_inputs()[0].name
            
            # –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            outputs = self.insightface_session.run(None, {input_name: preprocessed_image})
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding = outputs[0][0]  # –ü–µ—Ä–≤—ã–π –±–∞—Ç—á, –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
            
            return embedding
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ InsightFace: {e}")
            return None

    def _normalize_embedding(self, embedding: np.ndarray) -> np.ndarray:
        """L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        try:
            norm = np.linalg.norm(embedding)
            if norm > 0:
                return embedding / norm
            else:
                logger.warning("–ù—É–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
                return embedding
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return embedding

    def _assess_face_quality(self, face_image: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ
            if len(face_image.shape) == 3:
                gray = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)
            else:
                gray = face_image
            
            # –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏ (Laplacian variance)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(1.0, laplacian_var / 100.0)
            
            # –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
            contrast_score = np.std(gray) / 255.0
            
            # –û—Ü–µ–Ω–∫–∞ —è—Ä–∫–æ—Å—Ç–∏
            brightness = np.mean(gray) / 255.0
            brightness_score = 1.0 - abs(brightness - 0.5) * 2
            
            # –û–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞
            quality_score = (sharpness_score + contrast_score + brightness_score) / 3.0
            
            return float(max(0.0, min(1.0, quality_score)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞: {e}")
            return 0.0

    def _calculate_extraction_confidence(self, alignment_quality: float, 
                                       face_quality: float) -> float:
        """–†–∞—Å—á–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        try:
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
            confidence = 0.6 * alignment_quality + 0.4 * face_quality
            
            return float(max(0.0, min(1.0, confidence)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return 0.0

    def _validate_embedding_package(self, package: EmbeddingPackage):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        warnings = []
        quality_flags = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            if len(package.embedding_vector) != EMBEDDING_DIMENSION:
                warnings.append(f"–ù–µ–≤–µ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(package.embedding_vector)}")
                quality_flags.append("wrong_dimension")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            norm = np.linalg.norm(package.embedding_vector)
            if abs(norm - 1.0) > 0.01:
                warnings.append(f"–≠–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω: –Ω–æ—Ä–º–∞ = {norm:.3f}")
                quality_flags.append("not_normalized")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            if package.extraction_confidence < CLUSTER_CONFIDENCE_THRESHOLD:
                warnings.append(f"–ù–∏–∑–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {package.extraction_confidence:.3f}")
                quality_flags.append("low_confidence")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
            if package.alignment_quality < 0.5:
                warnings.append(f"–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: {package.alignment_quality:.3f}")
                quality_flags.append("poor_alignment")
            
            package.warnings = warnings
            package.quality_flags = quality_flags
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞–∫–µ—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            package.warnings = [f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}"]
            package.quality_flags = ["validation_error"]

    def calculate_embedding_distances_matrix(self, embeddings: List[EmbeddingPackage]) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        
        Args:
            embeddings: –°–ø–∏—Å–æ–∫ –ø–∞–∫–µ—Ç–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        """
        try:
            if len(embeddings) < 2:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π")
                return np.array([[]])
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
            vectors = np.array([pkg.embedding_vector for pkg in embeddings])
            
            # –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            n = len(vectors)
            distance_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(i + 1, n):
                    dist = cosine(vectors[i], vectors[j])
                    distance_matrix[i, j] = dist
                    distance_matrix[j, i] = dist
            
            logger.debug(f"–†–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π {n}x{n}")
            return distance_matrix
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π: {e}")
            return np.array([[]])

    def perform_identity_clustering(self, embeddings: List[EmbeddingPackage]) -> Dict[int, ClusterManifest]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π
        
        Args:
            embeddings: –°–ø–∏—Å–æ–∫ –ø–∞–∫–µ—Ç–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ ID
        """
        try:
            if len(embeddings) < DBSCAN_MIN_SAMPLES:
                logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {len(embeddings)}")
                return {}
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
            vectors = np.array([pkg.embedding_vector for pkg in embeddings])
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è DBSCAN
            clustering = DBSCAN(
                eps=DBSCAN_EPS,
                min_samples=DBSCAN_MIN_SAMPLES,
                metric='cosine'
            )
            
            cluster_labels = clustering.fit_predict(vectors)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            for i, package in enumerate(embeddings):
                package.cluster_id = int(cluster_labels[i])
                package.is_outlier = (cluster_labels[i] == -1)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            clusters = {}
            unique_labels = set(cluster_labels)
            
            for label in unique_labels:
                if label == -1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
                    continue
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ —á–ª–µ–Ω–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞
                cluster_members = [pkg for pkg, lbl in zip(embeddings, cluster_labels) if lbl == label]
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
                manifest = self._create_cluster_manifest(label, cluster_members)
                clusters[label] = manifest
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–µ –≤ –ø–∞–∫–µ—Ç–∞—Ö
                for member in cluster_members:
                    member.cluster_confidence = manifest.cohesion_score
                    member.mean_cluster_distance = self._calculate_mean_distance_to_cluster(
                        member, cluster_members
                    )
            
            # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
            self._calculate_nearest_neighbor_distances(embeddings)
            
            self.processing_stats['clusters_created'] = len(clusters)
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return clusters
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {}

    def _create_cluster_manifest(self, cluster_id: int, 
                               members: List[EmbeddingPackage]) -> ClusterManifest:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        try:
            # –†–∞—Å—á–µ—Ç —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            vectors = np.array([member.embedding_vector for member in members])
            center_embedding = np.mean(vectors, axis=0)
            center_embedding = center_embedding / np.linalg.norm(center_embedding)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞
            distances_to_center = [cosine(vec, center_embedding) for vec in vectors]
            intra_cluster_variance = float(np.var(distances_to_center))
            cluster_radius = float(np.max(distances_to_center))
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–∑–∞–≥–ª—É—à–∫–∏, –±—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –≤ temporal_analyzer)
            first_appearance = datetime.date.today()
            last_appearance = datetime.date.today()
            
            # –†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            silhouette_score = self._calculate_silhouette_score(members, vectors)
            cohesion_score = 1.0 - intra_cluster_variance  # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å–ø–ª–æ—á–µ–Ω–Ω–æ—Å—Ç–∏
            
            # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞
            stability_score = self._calculate_cluster_stability(vectors)
            
            manifest = ClusterManifest(
                cluster_id=cluster_id,
                center_embedding=center_embedding,
                member_count=len(members),
                first_appearance=first_appearance,
                last_appearance=last_appearance,
                intra_cluster_variance=intra_cluster_variance,
                cluster_radius=cluster_radius,
                stability_score=stability_score,
                appearance_gaps=[],  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –≤ temporal_analyzer
                total_timespan_days=0,  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –≤ temporal_analyzer
                silhouette_score=silhouette_score,
                cohesion_score=cohesion_score,
                member_image_ids=[member.image_id for member in members]
            )
            
            return manifest
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
            raise

    def _calculate_silhouette_score(self, members: List[EmbeddingPackage], 
                                  vectors: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç silhouette score –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        try:
            if len(vectors) < 2:
                return 1.0  # –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            
            # –í–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            intra_distances = []
            for i, vec in enumerate(vectors):
                other_vectors = np.delete(vectors, i, axis=0)
                distances = [cosine(vec, other_vec) for other_vec in other_vectors]
                intra_distances.append(np.mean(distances))
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π silhouette score (–±–µ–∑ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π)
            mean_intra_distance = np.mean(intra_distances)
            silhouette = 1.0 - mean_intra_distance
            
            return float(max(0.0, min(1.0, silhouette)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ silhouette score: {e}")
            return 0.0

    def _calculate_cluster_stability(self, vectors: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        try:
            if len(vectors) < 2:
                return 1.0
            
            # –†–∞—Å—á–µ—Ç —Ü–µ–Ω—Ç—Ä–∞
            center = np.mean(vectors, axis=0)
            center = center / np.linalg.norm(center)
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —Ü–µ–Ω—Ç—Ä–∞
            distances = [cosine(vec, center) for vec in vectors]
            
            # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            stability = 1.0 / (1.0 + np.std(distances))
            
            return float(stability)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
            return 0.0

    def _calculate_mean_distance_to_cluster(self, target: EmbeddingPackage, 
                                          cluster_members: List[EmbeddingPackage]) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        try:
            distances = []
            for member in cluster_members:
                if member.image_id != target.image_id:
                    dist = cosine(target.embedding_vector, member.embedding_vector)
                    distances.append(dist)
            
            return float(np.mean(distances)) if distances else 0.0
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
            return 0.0

    def _calculate_nearest_neighbor_distances(self, embeddings: List[EmbeddingPackage]):
        """–†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π"""
        try:
            for i, target in enumerate(embeddings):
                min_distance = float('inf')
                
                for j, other in enumerate(embeddings):
                    if i != j:
                        dist = cosine(target.embedding_vector, other.embedding_vector)
                        min_distance = min(min_distance, dist)
                
                target.nearest_neighbor_distance = min_distance
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π: {e}")

    def build_identity_timeline(self, clusters: Dict[int, ClusterManifest], 
                              data_manager) -> IdentityTimeline:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π
        
        Args:
            clusters: –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            data_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞—Ç
            
        Returns:
            –í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π
        """
        try:
            if not clusters:
                logger.warning("–ù–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏")
                return IdentityTimeline(
                    clusters={},
                    total_identities=0,
                    date_range=(datetime.date.today(), datetime.date.today()),
                    identity_switches=[],
                    switch_frequency=0.0,
                    dominant_identity=-1,
                    identity_distribution={}
                )
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            updated_clusters = {}
            all_dates = []
            
            for cluster_id, manifest in clusters.items():
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞—Ç –¥–ª—è —á–ª–µ–Ω–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞
                member_dates = []
                for image_id in manifest.member_image_ids:
                    # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–æ–≤ –∫ data_manager –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞—Ç—ã
                    # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
                    member_dates.append(datetime.date.today())
                
                if member_dates:
                    member_dates.sort()
                    all_dates.extend(member_dates)
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                    manifest.first_appearance = member_dates[0]
                    manifest.last_appearance = member_dates[-1]
                    manifest.total_timespan_days = (member_dates[-1] - member_dates[0]).days
                    
                    # –†–∞—Å—á–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –º–µ–∂–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è–º–∏
                    gaps = []
                    for i in range(1, len(member_dates)):
                        gap = (member_dates[i] - member_dates[i-1]).days
                        gaps.append(gap)
                    manifest.appearance_gaps = gaps
                
                updated_clusters[cluster_id] = manifest
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if all_dates:
                all_dates.sort()
                date_range = (all_dates[0], all_dates[-1])
            else:
                date_range = (datetime.date.today(), datetime.date.today())
            
            # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π
            identity_switches = self._analyze_identity_switches(updated_clusters)
            
            # –†–∞—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π
            total_days = (date_range[1] - date_range[0]).days
            switch_frequency = len(identity_switches) / max(total_days, 1)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
            dominant_identity = max(updated_clusters.keys(), 
                                  key=lambda x: updated_clusters[x].member_count)
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π
            total_members = sum(manifest.member_count for manifest in updated_clusters.values())
            identity_distribution = {
                cluster_id: manifest.member_count / total_members
                for cluster_id, manifest in updated_clusters.items()
            }
            
            timeline = IdentityTimeline(
                clusters=updated_clusters,
                total_identities=len(updated_clusters),
                date_range=date_range,
                identity_switches=identity_switches,
                switch_frequency=switch_frequency,
                dominant_identity=dominant_identity,
                identity_distribution=identity_distribution
            )
            
            logger.info(f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è —Å {len(updated_clusters)} –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—è–º–∏")
            return timeline
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏: {e}")
            return IdentityTimeline(
                clusters={},
                total_identities=0,
                date_range=(datetime.date.today(), datetime.date.today()),
                identity_switches=[],
                switch_frequency=0.0,
                dominant_identity=-1,
                identity_distribution={}
            )

    def _analyze_identity_switches(self, clusters: Dict[int, ClusterManifest]) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π –º–µ–∂–¥—É –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—è–º–∏"""
        try:
            switches = []
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—è–≤–ª–µ–Ω–∏–π
            appearances = []
            for cluster_id, manifest in clusters.items():
                appearances.append({
                    'date': manifest.first_appearance,
                    'cluster_id': cluster_id,
                    'type': 'first_appearance'
                })
                appearances.append({
                    'date': manifest.last_appearance,
                    'cluster_id': cluster_id,
                    'type': 'last_appearance'
                })
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
            appearances.sort(key=lambda x: x['date'])
            
            # –ü–æ–∏—Å–∫ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π
            current_identity = None
            for appearance in appearances:
                if appearance['type'] == 'first_appearance':
                    if current_identity is not None and current_identity != appearance['cluster_id']:
                        switches.append({
                            'date': appearance['date'],
                            'from_identity': current_identity,
                            'to_identity': appearance['cluster_id'],
                            'switch_type': 'transition'
                        })
                    current_identity = appearance['cluster_id']
            
            return switches
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π: {e}")
            return []

    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = self.processing_stats.copy()
        
        if stats['total_processed'] > 0:
            stats['success_rate'] = stats['successful_extractions'] / stats['total_processed']
            stats['cache_hit_rate'] = stats['cache_hits'] / stats['total_processed']
        else:
            stats['success_rate'] = 0.0
            stats['cache_hit_rate'] = 0.0
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        stats['model_info'] = {
            'model_loaded': self.insightface_session is not None,
            'embedding_dimension': EMBEDDING_DIMENSION,
            'dbscan_eps': DBSCAN_EPS,
            'dbscan_min_samples': DBSCAN_MIN_SAMPLES
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ
        stats['cache_info'] = {
            'embedding_cache_size': len(self.embedding_cache),
            'cluster_cache_size': len(self.cluster_cache)
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
        process = psutil.Process()
        memory_info = process.memory_info()
        stats['memory_usage_mb'] = memory_info.rss / (1024 * 1024)
        
        return stats

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            self.embedding_cache.clear()
            self.cluster_cache.clear()
            logger.info("–ö—ç—à EmbeddingAnalyzer –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    def save_cache(self, cache_filename: str = "embedding_cache.pkl"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –Ω–∞ –¥–∏—Å–∫"""
        try:
            cache_path = self.cache_dir / cache_filename
            
            cache_data = {
                'embedding_cache': self.embedding_cache,
                'cluster_cache': self.cluster_cache,
                'processing_stats': self.processing_stats
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            logger.info(f"–ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {cache_path}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    def load_cache(self, cache_filename: str = "embedding_cache.pkl") -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —Å –¥–∏—Å–∫–∞"""
        try:
            cache_path = self.cache_dir / cache_filename
            
            if cache_path.exists():
                with open(cache_path, 'rb') as f:
                    cache_data = pickle.load(f)
                
                self.embedding_cache = cache_data.get('embedding_cache', {})
                self.cluster_cache = cache_data.get('cluster_cache', {})
                self.processing_stats.update(cache_data.get('processing_stats', {}))
                
                logger.info(f"–ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω: {cache_path}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
            return False

# === –§–£–ù–ö–¶–ò–ò –°–ê–ú–û–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===

def self_test():
    """–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è embedding_analyzer"""
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è embedding_analyzer...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        analyzer = EmbeddingAnalyzer()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤
        test_image = np.random.randint(0, 255, (112, 112, 3), dtype=np.uint8)
        test_landmarks = np.random.rand(68, 3) * 100
        
        # –¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
        keypoints = analyzer._extract_5_keypoints(test_landmarks)
        assert keypoints.shape == (5, 2), "–ù–µ–≤–µ—Ä–Ω–∞—è —Ñ–æ—Ä–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫"
        
        # –¢–µ—Å—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        preprocessed = analyzer._preprocess_for_insightface(test_image)
        assert preprocessed.shape == (1, 3, 112, 112), "–ù–µ–≤–µ—Ä–Ω–∞—è —Ñ–æ—Ä–º–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
        
        # –¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        test_embedding = np.random.rand(512)
        normalized = analyzer._normalize_embedding(test_embedding)
        assert abs(np.linalg.norm(normalized) - 1.0) < 0.01, "–≠–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω"
        
        # –¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = analyzer.get_processing_statistics()
        assert 'success_rate' in stats, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
        
        logger.info("–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ embedding_analyzer –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è
    success = self_test()
    if success:
        print("‚úÖ –ú–æ–¥—É–ª—å embedding_analyzer —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        analyzer = EmbeddingAnalyzer()
        print(f"üìä –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –∫—ç—à–µ: {len(analyzer.embedding_cache)}")
        print(f"üîß –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –∫—ç—à–µ: {len(analyzer.cluster_cache)}")
        print(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {EMBEDDING_DIMENSION}")
        print(f"üéØ DBSCAN eps: {DBSCAN_EPS}")
        print(f"üíæ –ö—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {analyzer.cache_dir}")
    else:
        print("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ –º–æ–¥—É–ª–µ embedding_analyzer")
        exit(1)