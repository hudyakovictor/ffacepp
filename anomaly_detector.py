# anomaly_detector.py
import os
import json
import logging
import hashlib
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field, asdict
import numpy as np
import pandas as pd
from scipy import stats
from scipy.spatial.distance import euclidean, cosine
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import pickle
import time
import psutil
from functools import lru_cache
import threading
from collections import OrderedDict, defaultdict
import msgpack

from core_config import get_config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# === –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

# –í–µ—Å–∞ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏ (0.3 + 0.3 + 0.2 + 0.2 = 1.0)
AUTHENTICITY_WEIGHTS = {
    'geometry': 0.3,
    'embedding': 0.3,
    'texture': 0.2,
    'temporal': 0.2
}

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
AUTHENTICITY_THRESHOLDS = {
    'fake_threshold': 0.3,      # < 0.3 = –º–∞—Å–∫–∞/–¥–≤–æ–π–Ω–∏–∫
    'suspicious_threshold': 0.7  # 0.3-0.7 = —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ, > 0.7 = –ø–æ–¥–ª–∏–Ω–Ω–æ–µ
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
BAYESIAN_PARAMS = {
    'prior_same_person': 0.5,
    'prior_different_person': 0.5,
    'likelihood_threshold': 0.1,
    'convergence_threshold': 0.01,
    'max_iterations': 100
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—Å–∫–∞–¥–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
CASCADE_PARAMS = {
    'geometry_critical_threshold': 0.2,
    'embedding_critical_threshold': 0.25,
    'texture_critical_threshold': 0.3,
    'temporal_critical_threshold': 0.3,
    'cross_source_threshold': 0.5
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã IsolationForest –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
ISOLATION_FOREST_PARAMS = {
    'contamination': 0.1,
    'random_state': 42,
    'n_estimators': 100,
    'max_samples': 'auto'
}

# === –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===

@dataclass
class AuthenticityScore:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏"""
    image_id: str
    filepath: str
    
    # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –±–∞–ª–ª—ã
    geometry_score: float = 0.0
    embedding_score: float = 0.0
    texture_score: float = 0.0
    temporal_score: float = 0.0
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
    overall_authenticity: float = 0.0
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    classification: str = "unknown"  # authentic, suspicious, fake
    confidence_level: float = 0.0
    
    # –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    posterior_same_person: float = 0.5
    posterior_different_person: float = 0.5
    
    # –§–ª–∞–≥–∏ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    critical_flags: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    anomaly_flags: List[str] = field(default_factory=list)
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    processing_time_ms: float = 0.0
    analysis_method: str = "cascade_bayesian"
    model_version: str = "v1.0"

@dataclass
class BayesianEvidence:
    """–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
    evidence_type: str  # geometry, embedding, texture, temporal
    likelihood_same: float
    likelihood_different: float
    confidence: float
    source_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CascadeResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞—Å–∫–∞–¥–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    stage: str
    passed: bool
    score: float
    threshold: float
    critical_failure: bool = False
    details: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CrossSourceAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∫—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    date: datetime.date
    sources_count: int
    consistency_score: float
    max_distance: float
    mean_distance: float
    outlier_sources: List[str] = field(default_factory=list)
    critical_inconsistency: bool = False

# === –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –î–ï–¢–ï–ö–¢–û–†–ê –ê–ù–û–ú–ê–õ–ò–ô ===

class AnomalyDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –º–∞—Å–æ–∫ –∏ –¥–≤–æ–π–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.config = get_config()
        self.cache_dir = Path("./cache/anomaly_detector")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.authenticity_cache: Dict[str, AuthenticityScore] = {}
        self.bayesian_cache: Dict[str, Dict[str, float]] = {}
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
        self.isolation_forest = None
        self.scaler = StandardScaler()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.processing_stats = {
            'total_analyzed': 0,
            'authentic_detected': 0,
            'suspicious_detected': 0,
            'fake_detected': 0,
            'critical_failures': 0,
            'bayesian_updates': 0
        }
        
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.analysis_lock = threading.Lock()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_anomaly_models()
        
        logger.info("AnomalyDetector –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _initialize_anomaly_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è IsolationForest
            self.isolation_forest = IsolationForest(
                contamination=ISOLATION_FOREST_PARAMS['contamination'],
                random_state=ISOLATION_FOREST_PARAMS['random_state'],
                n_estimators=ISOLATION_FOREST_PARAMS['n_estimators'],
                max_samples=ISOLATION_FOREST_PARAMS['max_samples']
            )
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ –µ—Å—Ç—å
            self._load_pretrained_models()
            
            logger.debug("–ú–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            raise

    def _load_pretrained_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            models_path = self.cache_dir / "pretrained_models.pkl"
            if models_path.exists():
                with open(models_path, 'rb') as f:
                    models_data = pickle.load(f)
                
                if 'isolation_forest' in models_data:
                    self.isolation_forest = models_data['isolation_forest']
                if 'scaler' in models_data:
                    self.scaler = models_data['scaler']
                
                logger.info("–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
                
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: {e}")

    def calculate_identity_authenticity_score(self, 
                                            geometry_metrics=None,
                                            embedding_package=None, 
                                            texture_package=None,
                                            temporal_analysis=None) -> Optional[AuthenticityScore]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á–µ—Ç–∞ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏
        
        Args:
            geometry_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –ª–∏—Ü–∞
            embedding_package: –ü–∞–∫–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            texture_package: –ü–∞–∫–µ—Ç —Ç–µ–∫—Å—Ç—É—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            temporal_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –û–±—ä–µ–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏
        """
        try:
            start_time = time.time()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not any([geometry_metrics, embedding_package, texture_package, temporal_analysis]):
                logger.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return None
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ image_id
            image_id = self._extract_image_id(geometry_metrics, embedding_package, 
                                            texture_package, temporal_analysis)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if image_id in self.authenticity_cache:
                cached_result = self.authenticity_cache[image_id]
                cached_result.processing_time_ms = (time.time() - start_time) * 1000
                return cached_result
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = AuthenticityScore(
                image_id=image_id,
                filepath=self._extract_filepath(geometry_metrics, embedding_package, 
                                              texture_package, temporal_analysis)
            )
            
            # –†–∞—Å—á–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –±–∞–ª–ª–æ–≤
            result.geometry_score = self._calculate_geometry_score(geometry_metrics)
            result.embedding_score = self._calculate_embedding_score(embedding_package)
            result.texture_score = self._calculate_texture_score(texture_package)
            result.temporal_score = self._calculate_temporal_score(temporal_analysis)
            
            # –ö–∞—Å–∫–∞–¥–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
            cascade_results = self.perform_cascade_verification(result)
            
            # –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ –±–∞–ª–ª–∞
            result.overall_authenticity = self._calculate_weighted_score(result)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result.classification = self._classify_authenticity(result.overall_authenticity)
            
            # –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            bayesian_result = self.apply_bayesian_identity_analysis(result, 
                                                                   geometry_metrics,
                                                                   embedding_package,
                                                                   texture_package,
                                                                   temporal_analysis)
            
            result.posterior_same_person = bayesian_result.get('posterior_same_person', 0.5)
            result.posterior_different_person = bayesian_result.get('posterior_different_person', 0.5)
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            anomaly_flags = self._detect_statistical_anomalies(result)
            result.anomaly_flags.extend(anomaly_flags)
            
            # –†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            result.confidence_level = self._calculate_confidence_level(result, cascade_results)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            result.processing_time_ms = (time.time() - start_time) * 1000
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            self.authenticity_cache[image_id] = result
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._update_processing_stats(result)
            
            logger.debug(f"–ê–Ω–∞–ª–∏–∑ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {result.processing_time_ms:.1f}–º—Å")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏: {e}")
            return None

    def _extract_image_id(self, *packages) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ image_id –∏–∑ –ª—é–±–æ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞"""
        for package in packages:
            if package and hasattr(package, 'image_id'):
                return package.image_id
        return hashlib.sha256(str(datetime.datetime.now()).encode()).hexdigest()

    def _extract_filepath(self, *packages) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ filepath –∏–∑ –ª—é–±–æ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞"""
        for package in packages:
            if package and hasattr(package, 'filepath'):
                return package.filepath
        return ""

    def _calculate_geometry_score(self, geometry_metrics) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –≥–µ–æ–º–µ—Ç—Ä–∏–∏"""
        try:
            if geometry_metrics is None:
                return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–ª–ª –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö
            
            # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤
            base_score = getattr(geometry_metrics, 'confidence_score', 0.5)
            
            # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –æ—à–∏–±–∫–∏ —Ñ–æ—Ä–º—ã
            shape_error = getattr(geometry_metrics, 'shape_error', 0.0)
            eye_region_error = getattr(geometry_metrics, 'eye_region_error', 0.0)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫
            shape_penalty = min(shape_error / 100.0, 0.3)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ 30%
            eye_penalty = min(eye_region_error / 50.0, 0.2)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ 20%
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
            geometry_score = base_score - shape_penalty - eye_penalty
            
            return float(max(0.0, min(1.0, geometry_score)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–ª–∞: {e}")
            return 0.0

    def _calculate_embedding_score(self, embedding_package) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            if embedding_package is None:
                return 0.5
            
            # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            base_score = getattr(embedding_package, 'extraction_confidence', 0.5)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
            cluster_confidence = getattr(embedding_package, 'cluster_confidence', 0.5)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞—É—Ç–ª–∞–µ—Ä—ã
            is_outlier = getattr(embedding_package, 'is_outlier', False)
            outlier_penalty = 0.3 if is_outlier else 0.0
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
            embedding_score = (base_score + cluster_confidence) / 2.0 - outlier_penalty
            
            return float(max(0.0, min(1.0, embedding_score)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –±–∞–ª–ª–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return 0.0

    def _calculate_texture_score(self, texture_package) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ —Ç–µ–∫—Å—Ç—É—Ä—ã"""
        try:
            if texture_package is None:
                return 0.5
            
            # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –º–∞—Ç–µ—Ä–∏–∞–ª–∞
            material_score = getattr(texture_package, 'material_authenticity_score', 0.5)
            
            # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            has_seam_artifacts = getattr(texture_package, 'seam_artifacts_detected', False)
            has_texture_transitions = getattr(texture_package, 'texture_transitions_detected', False)
            
            seam_penalty = 0.4 if has_seam_artifacts else 0.0
            transition_penalty = 0.3 if has_texture_transitions else 0.0
            
            # –ê–Ω–∞–ª–∏–∑ —ç–Ω—Ç—Ä–æ–ø–∏–∏
            entropy_score = getattr(texture_package, 'entropy_score', 0.5)
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
            texture_score = (material_score + entropy_score) / 2.0 - seam_penalty - transition_penalty
            
            return float(max(0.0, min(1.0, texture_score)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –±–∞–ª–ª–∞ —Ç–µ–∫—Å—Ç—É—Ä—ã: {e}")
            return 0.0

    def _calculate_temporal_score(self, temporal_analysis) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        try:
            if temporal_analysis is None:
                return 0.5
            
            # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            consistency_score = getattr(temporal_analysis, 'consistency_score', 0.5)
            
            # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –∞–Ω–æ–º–∞–ª–∏–∏
            temporal_anomalies = getattr(temporal_analysis, 'anomalies_detected', [])
            anomaly_penalty = min(len(temporal_anomalies) * 0.1, 0.4)
            
            # –ê–Ω–∞–ª–∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
            medical_validity = getattr(temporal_analysis, 'medical_validity_score', 0.5)
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
            temporal_score = (consistency_score + medical_validity) / 2.0 - anomaly_penalty
            
            return float(max(0.0, min(1.0, temporal_score)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –±–∞–ª–ª–∞: {e}")
            return 0.0

    def _calculate_weighted_score(self, result: AuthenticityScore) -> float:
        """–†–∞—Å—á–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –±–∞–ª–ª–∞ –ø–æ —Ñ–æ—Ä–º—É–ª–µ 0.3+0.3+0.2+0.2"""
        try:
            weighted_score = (
                result.geometry_score * AUTHENTICITY_WEIGHTS['geometry'] +
                result.embedding_score * AUTHENTICITY_WEIGHTS['embedding'] +
                result.texture_score * AUTHENTICITY_WEIGHTS['texture'] +
                result.temporal_score * AUTHENTICITY_WEIGHTS['temporal']
            )
            
            return float(max(0.0, min(1.0, weighted_score)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –±–∞–ª–ª–∞: {e}")
            return 0.0

    def _classify_authenticity(self, overall_score: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ –ø–æ—Ä–æ–≥–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º"""
        if overall_score < AUTHENTICITY_THRESHOLDS['fake_threshold']:
            return "fake"
        elif overall_score < AUTHENTICITY_THRESHOLDS['suspicious_threshold']:
            return "suspicious"
        else:
            return "authentic"

    def perform_cascade_verification(self, result: AuthenticityScore) -> List[CascadeResult]:
        """
        –ö–∞—Å–∫–∞–¥–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ä–æ–≥–æ–≤
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞—Å–∫–∞–¥–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        try:
            cascade_results = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–æ–º–µ—Ç—Ä–∏–∏
            geometry_result = CascadeResult(
                stage="geometry",
                passed=result.geometry_score >= CASCADE_PARAMS['geometry_critical_threshold'],
                score=result.geometry_score,
                threshold=CASCADE_PARAMS['geometry_critical_threshold'],
                critical_failure=result.geometry_score < CASCADE_PARAMS['geometry_critical_threshold']
            )
            cascade_results.append(geometry_result)
            
            if geometry_result.critical_failure:
                result.critical_flags.append("geometry_critical_failure")
                logger.warning(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≤–∞–ª –≥–µ–æ–º–µ—Ç—Ä–∏–∏: {result.geometry_score}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            embedding_result = CascadeResult(
                stage="embedding",
                passed=result.embedding_score >= CASCADE_PARAMS['embedding_critical_threshold'],
                score=result.embedding_score,
                threshold=CASCADE_PARAMS['embedding_critical_threshold'],
                critical_failure=result.embedding_score < CASCADE_PARAMS['embedding_critical_threshold']
            )
            cascade_results.append(embedding_result)
            
            if embedding_result.critical_failure:
                result.critical_flags.append("embedding_critical_failure")
                logger.warning(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≤–∞–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {result.embedding_score}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç—É—Ä—ã
            texture_result = CascadeResult(
                stage="texture",
                passed=result.texture_score >= CASCADE_PARAMS['texture_critical_threshold'],
                score=result.texture_score,
                threshold=CASCADE_PARAMS['texture_critical_threshold'],
                critical_failure=result.texture_score < CASCADE_PARAMS['texture_critical_threshold']
            )
            cascade_results.append(texture_result)
            
            if texture_result.critical_failure:
                result.critical_flags.append("texture_critical_failure")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            temporal_result = CascadeResult(
                stage="temporal",
                passed=result.temporal_score >= CASCADE_PARAMS['temporal_critical_threshold'],
                score=result.temporal_score,
                threshold=CASCADE_PARAMS['temporal_critical_threshold'],
                critical_failure=result.temporal_score < CASCADE_PARAMS['temporal_critical_threshold']
            )
            cascade_results.append(temporal_result)
            
            if temporal_result.critical_failure:
                result.critical_flags.append("temporal_critical_failure")
            
            # –ü–æ–¥—Å—á–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–∞–ª–æ–≤
            critical_failures = sum(1 for r in cascade_results if r.critical_failure)
            if critical_failures >= 2:
                result.critical_flags.append("multiple_critical_failures")
                result.overall_authenticity = min(result.overall_authenticity, 0.2)
            
            return cascade_results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–∞—Å–∫–∞–¥–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return []

    def apply_bayesian_identity_analysis(self, result: AuthenticityScore, 
                                       geometry_metrics=None,
                                       embedding_package=None,
                                       texture_package=None,
                                       temporal_analysis=None) -> Dict[str, float]:
        """
        –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏
            geometry_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏
            embedding_package: –ü–∞–∫–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤  
            texture_package: –ü–∞–∫–µ—Ç —Ç–µ–∫—Å—Ç—É—Ä—ã
            temporal_analysis: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –±–∞–π–µ—Å–æ–≤—Å–∫–∏–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        """
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è prior –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            image_id = result.image_id
            if image_id not in self.bayesian_cache:
                self.bayesian_cache[image_id] = {
                    'prior_same_person': BAYESIAN_PARAMS['prior_same_person'],
                    'prior_different_person': BAYESIAN_PARAMS['prior_different_person']
                }
            
            priors = self.bayesian_cache[image_id]
            
            # –°–±–æ—Ä –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
            evidence_list = []
            
            # –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            if geometry_metrics:
                geometry_evidence = self._calculate_geometry_likelihood(geometry_metrics)
                evidence_list.append(geometry_evidence)
            
            # –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            if embedding_package:
                embedding_evidence = self._calculate_embedding_likelihood(embedding_package)
                evidence_list.append(embedding_evidence)
            
            # –¢–µ–∫—Å—Ç—É—Ä–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            if texture_package:
                texture_evidence = self._calculate_texture_likelihood(texture_package)
                evidence_list.append(texture_evidence)
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            if temporal_analysis:
                temporal_evidence = self._calculate_temporal_likelihood(temporal_analysis)
                evidence_list.append(temporal_evidence)
            
            # –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            posterior_same = priors['prior_same_person']
            posterior_different = priors['prior_different_person']
            
            for evidence in evidence_list:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–∞–≤–∏–ª—É –ë–∞–π–µ—Å–∞
                numerator_same = posterior_same * evidence.likelihood_same
                numerator_different = posterior_different * evidence.likelihood_different
                
                denominator = numerator_same + numerator_different
                
                if denominator > 0:
                    posterior_same = numerator_same / denominator
                    posterior_different = numerator_different / denominator
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            total = posterior_same + posterior_different
            if total > 0:
                posterior_same /= total
                posterior_different /= total
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞
            self.bayesian_cache[image_id].update({
                'posterior_same_person': posterior_same,
                'posterior_different_person': posterior_different
            })
            
            self.processing_stats['bayesian_updates'] += 1
            
            return {
                'posterior_same_person': float(posterior_same),
                'posterior_different_person': float(posterior_different)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {
                'posterior_same_person': 0.5,
                'posterior_different_person': 0.5
            }

    def _calculate_geometry_likelihood(self, geometry_metrics) -> BayesianEvidence:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –¥–ª—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            confidence = getattr(geometry_metrics, 'confidence_score', 0.5)
            shape_error = getattr(geometry_metrics, 'shape_error', 0.0)
            
            # –í—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏ –Ω–∏–∑–∫–∞—è –æ—à–∏–±–∫–∞ -> –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å same_person
            if confidence > 0.8 and shape_error < 20.0:
                likelihood_same = 0.9
                likelihood_different = 0.1
            elif confidence < 0.3 or shape_error > 80.0:
                likelihood_same = 0.1
                likelihood_different = 0.9
            else:
                # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
                likelihood_same = confidence * (1.0 - min(shape_error / 100.0, 0.8))
                likelihood_different = 1.0 - likelihood_same
            
            return BayesianEvidence(
                evidence_type="geometry",
                likelihood_same=likelihood_same,
                likelihood_different=likelihood_different,
                confidence=confidence,
                source_data={'shape_error': shape_error}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è: {e}")
            return BayesianEvidence("geometry", 0.5, 0.5, 0.0)

    def _calculate_embedding_likelihood(self, embedding_package) -> BayesianEvidence:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            cluster_confidence = getattr(embedding_package, 'cluster_confidence', 0.5)
            is_outlier = getattr(embedding_package, 'is_outlier', False)
            
            if is_outlier:
                likelihood_same = 0.2
                likelihood_different = 0.8
            elif cluster_confidence > 0.8:
                likelihood_same = 0.9
                likelihood_different = 0.1
            else:
                likelihood_same = cluster_confidence
                likelihood_different = 1.0 - cluster_confidence
            
            return BayesianEvidence(
                evidence_type="embedding",
                likelihood_same=likelihood_same,
                likelihood_different=likelihood_different,
                confidence=cluster_confidence,
                source_data={'is_outlier': is_outlier}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return BayesianEvidence("embedding", 0.5, 0.5, 0.0)

    def _calculate_texture_likelihood(self, texture_package) -> BayesianEvidence:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç—É—Ä—ã"""
        try:
            material_score = getattr(texture_package, 'material_authenticity_score', 0.5)
            has_artifacts = getattr(texture_package, 'seam_artifacts_detected', False)
            
            if has_artifacts:
                likelihood_same = 0.1
                likelihood_different = 0.9
            elif material_score > 0.8:
                likelihood_same = 0.9
                likelihood_different = 0.1
            else:
                likelihood_same = material_score
                likelihood_different = 1.0 - material_score
            
            return BayesianEvidence(
                evidence_type="texture",
                likelihood_same=likelihood_same,
                likelihood_different=likelihood_different,
                confidence=material_score,
                source_data={'has_artifacts': has_artifacts}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ–∫—Å—Ç—É—Ä–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è: {e}")
            return BayesianEvidence("texture", 0.5, 0.5, 0.0)

    def _calculate_temporal_likelihood(self, temporal_analysis) -> BayesianEvidence:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            consistency_score = getattr(temporal_analysis, 'consistency_score', 0.5)
            anomalies_count = len(getattr(temporal_analysis, 'anomalies_detected', []))
            
            if anomalies_count > 3:
                likelihood_same = 0.2
                likelihood_different = 0.8
            elif consistency_score > 0.8:
                likelihood_same = 0.9
                likelihood_different = 0.1
            else:
                likelihood_same = consistency_score
                likelihood_different = 1.0 - consistency_score
            
            return BayesianEvidence(
                evidence_type="temporal",
                likelihood_same=likelihood_same,
                likelihood_different=likelihood_different,
                confidence=consistency_score,
                source_data={'anomalies_count': anomalies_count}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è: {e}")
            return BayesianEvidence("temporal", 0.5, 0.5, 0.0)

    def _detect_statistical_anomalies(self, result: AuthenticityScore) -> List[str]:
        """–î–µ—Ç–µ–∫—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∞–Ω–æ–º–∞–ª–∏–π"""
        try:
            anomaly_flags = []
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = np.array([[
                result.geometry_score,
                result.embedding_score,
                result.texture_score,
                result.temporal_score
            ]])
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if hasattr(self.scaler, 'mean_'):
                features_scaled = self.scaler.transform(features)
            else:
                features_scaled = features
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π —á–µ—Ä–µ–∑ IsolationForest
            if self.isolation_forest is not None:
                try:
                    anomaly_score = self.isolation_forest.decision_function(features_scaled)[0]
                    is_anomaly = self.isolation_forest.predict(features_scaled)[0] == -1
                    
                    if is_anomaly:
                        anomaly_flags.append("statistical_anomaly")
                        if anomaly_score < -0.5:
                            anomaly_flags.append("severe_anomaly")
                            
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ IsolationForest: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            scores = [result.geometry_score, result.embedding_score, 
                     result.texture_score, result.temporal_score]
            
            if any(score < 0.1 for score in scores):
                anomaly_flags.append("extreme_low_score")
            
            if all(score > 0.95 for score in scores):
                anomaly_flags.append("suspiciously_perfect")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            score_std = np.std(scores)
            if score_std > 0.4:
                anomaly_flags.append("high_score_variance")
            
            return anomaly_flags
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
            return []

    def _calculate_confidence_level(self, result: AuthenticityScore, 
                                  cascade_results: List[CascadeResult]) -> float:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ë–∞–∑–æ–≤–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            available_scores = sum(1 for score in [result.geometry_score, result.embedding_score,
                                                 result.texture_score, result.temporal_score] if score > 0)
            
            base_confidence = available_scores / 4.0
            
            # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–∞–ª—ã
            critical_failures = len(result.critical_flags)
            critical_penalty = min(critical_failures * 0.2, 0.6)
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            scores = [result.geometry_score, result.embedding_score, 
                     result.texture_score, result.temporal_score]
            consistency_bonus = 1.0 - np.std(scores) if np.std(scores) < 0.3 else 0.0
            consistency_bonus *= 0.2
            
            # –ò—Ç–æ–≥–æ–≤–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
            confidence = base_confidence - critical_penalty + consistency_bonus
            
            return float(max(0.0, min(1.0, confidence)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return 0.5

    def perform_cross_source_verification(self, embeddings_by_source: Dict[str, List]) -> CrossSourceAnalysis:
        """
        –ö—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        
        Args:
            embeddings_by_source: –°–ª–æ–≤–∞—Ä—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∫—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            if len(embeddings_by_source) < 2:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                return CrossSourceAnalysis(
                    date=datetime.date.today(),
                    sources_count=len(embeddings_by_source),
                    consistency_score=1.0,
                    max_distance=0.0,
                    mean_distance=0.0
                )
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
            sources = list(embeddings_by_source.keys())
            embeddings = [np.mean(embeddings_by_source[source], axis=0) 
                         for source in sources]
            
            distances = []
            for i in range(len(embeddings)):
                for j in range(i + 1, len(embeddings)):
                    dist = cosine(embeddings[i], embeddings[j])
                    distances.append(dist)
            
            if not distances:
                return CrossSourceAnalysis(
                    date=datetime.date.today(),
                    sources_count=len(embeddings_by_source),
                    consistency_score=1.0,
                    max_distance=0.0,
                    mean_distance=0.0
                )
            
            max_distance = max(distances)
            mean_distance = np.mean(distances)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—É—Ç–ª–∞–µ—Ä–æ–≤
            outlier_sources = []
            if max_distance > CASCADE_PARAMS['cross_source_threshold']:
                # –ü–æ–∏—Å–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–∏–±–æ–ª–µ–µ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                source_distances = []
                for i, source in enumerate(sources):
                    other_distances = []
                    for j, other_embedding in enumerate(embeddings):
                        if i != j:
                            dist = cosine(embeddings[i], other_embedding)
                            other_distances.append(dist)
                    source_distances.append((source, np.mean(other_distances)))
                
                # –ò—Å—Ç–æ—á–Ω–∏–∫ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º
                outlier_source = max(source_distances, key=lambda x: x[1])
                if outlier_source[1] > CASCADE_PARAMS['cross_source_threshold']:
                    outlier_sources.append(outlier_source[0])
            
            # –†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            consistency_score = 1.0 - min(mean_distance / 0.5, 1.0)
            
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            critical_inconsistency = max_distance > CASCADE_PARAMS['cross_source_threshold']
            
            return CrossSourceAnalysis(
                date=datetime.date.today(),
                sources_count=len(embeddings_by_source),
                consistency_score=consistency_score,
                max_distance=max_distance,
                mean_distance=mean_distance,
                outlier_sources=outlier_sources,
                critical_inconsistency=critical_inconsistency
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫—Ä–æ—Å—Å-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return CrossSourceAnalysis(
                date=datetime.date.today(),
                sources_count=0,
                consistency_score=0.0,
                max_distance=1.0,
                mean_distance=1.0,
                critical_inconsistency=True
            )

    def _update_processing_stats(self, result: AuthenticityScore):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            self.processing_stats['total_analyzed'] += 1
            
            if result.classification == "authentic":
                self.processing_stats['authentic_detected'] += 1
            elif result.classification == "suspicious":
                self.processing_stats['suspicious_detected'] += 1
            elif result.classification == "fake":
                self.processing_stats['fake_detected'] += 1
            
            if result.critical_flags:
                self.processing_stats['critical_failures'] += 1
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

    def train_anomaly_models(self, training_data: List[AuthenticityScore]):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
        
        Args:
            training_data: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            if len(training_data) < 10:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
                return
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = []
            for result in training_data:
                features.append([
                    result.geometry_score,
                    result.embedding_score,
                    result.texture_score,
                    result.temporal_score
                ])
            
            features = np.array(features)
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            self.scaler.fit(features)
            features_scaled = self.scaler.transform(features)
            
            # –û–±—É—á–µ–Ω–∏–µ IsolationForest
            self.isolation_forest.fit(features_scaled)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            self._save_trained_models()
            
            logger.info(f"–ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ {len(training_data)} –æ–±—Ä–∞–∑—Ü–∞—Ö")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")

    def _save_trained_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            models_data = {
                'isolation_forest': self.isolation_forest,
                'scaler': self.scaler,
                'training_timestamp': datetime.datetime.now().isoformat()
            }
            
            models_path = self.cache_dir / "pretrained_models.pkl"
            with open(models_path, 'wb') as f:
                pickle.dump(models_data, f)
            
            logger.info("–û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")

    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = self.processing_stats.copy()
        
        if stats['total_analyzed'] > 0:
            stats['authentic_rate'] = stats['authentic_detected'] / stats['total_analyzed']
            stats['suspicious_rate'] = stats['suspicious_detected'] / stats['total_analyzed']
            stats['fake_rate'] = stats['fake_detected'] / stats['total_analyzed']
            stats['critical_failure_rate'] = stats['critical_failures'] / stats['total_analyzed']
        else:
            stats['authentic_rate'] = 0.0
            stats['suspicious_rate'] = 0.0
            stats['fake_rate'] = 0.0
            stats['critical_failure_rate'] = 0.0
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
        stats['models_info'] = {
            'isolation_forest_trained': self.isolation_forest is not None,
            'scaler_fitted': hasattr(self.scaler, 'mean_'),
            'bayesian_cache_size': len(self.bayesian_cache)
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
        process = psutil.Process()
        memory_info = process.memory_info()
        stats['memory_usage_mb'] = memory_info.rss / (1024 * 1024)
        
        return stats

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            self.authenticity_cache.clear()
            self.bayesian_cache.clear()
            logger.info("–ö—ç—à AnomalyDetector –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

# === –§–£–ù–ö–¶–ò–ò –°–ê–ú–û–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===

def self_test():
    """–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è anomaly_detector"""
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è anomaly_detector...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        detector = AnomalyDetector()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        class MockMetrics:
            def __init__(self):
                self.image_id = "test_image"
                self.filepath = "test.jpg"
                self.confidence_score = 0.8
                self.shape_error = 15.0
                self.eye_region_error = 8.0
        
        class MockEmbedding:
            def __init__(self):
                self.image_id = "test_image"
                self.extraction_confidence = 0.9
                self.cluster_confidence = 0.85
                self.is_outlier = False
        
        test_geometry = MockMetrics()
        test_embedding = MockEmbedding()
        
        # –¢–µ—Å—Ç —Ä–∞—Å—á–µ—Ç–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏
        result = detector.calculate_identity_authenticity_score(
            geometry_metrics=test_geometry,
            embedding_package=test_embedding
        )
        
        assert result is not None, "–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω"
        assert 0.0 <= result.overall_authenticity <= 1.0, "–ù–µ–≤–µ—Ä–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏"
        assert result.classification in ["authentic", "suspicious", "fake"], "–ù–µ–≤–µ—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
        
        # –¢–µ—Å—Ç –∫–∞—Å–∫–∞–¥–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
        cascade_results = detector.perform_cascade_verification(result)
        assert len(cascade_results) > 0, "–ö–∞—Å–∫–∞–¥–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"
        
        # –¢–µ—Å—Ç –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        bayesian_result = detector.apply_bayesian_identity_analysis(
            result, test_geometry, test_embedding
        )
        assert 'posterior_same_person' in bayesian_result, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
        
        # –¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = detector.get_processing_statistics()
        assert 'total_analyzed' in stats, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
        
        logger.info("–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ anomaly_detector –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è
    success = self_test()
    if success:
        print("‚úÖ –ú–æ–¥—É–ª—å anomaly_detector —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        detector = AnomalyDetector()
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫—ç—à–µ: {len(detector.authenticity_cache)}")
        print(f"üîß –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫—ç—à: {len(detector.bayesian_cache)}")
        print(f"üìè –í–µ—Å–∞ —Ñ–æ—Ä–º—É–ª—ã: {AUTHENTICITY_WEIGHTS}")
        print(f"üéØ –ü–æ—Ä–æ–≥–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {AUTHENTICITY_THRESHOLDS}")
        print(f"üíæ –ö—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {detector.cache_dir}")
    else:
        print("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ –º–æ–¥—É–ª–µ anomaly_detector")
        exit(1)