# metrics_calculator.py
import os
import json
import logging
import hashlib
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field, asdict
import numpy as np
import cv2
from scipy.spatial.distance import euclidean, cosine
from scipy.stats import zscore
import pickle
import time
import psutil
from functools import lru_cache
import threading
from collections import OrderedDict, defaultdict

from core_config import get_config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# === –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

# –ò–Ω–¥–µ–∫—Å—ã –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
SKULL_LANDMARKS = {
    'forehead_width': [19, 24],  # –®–∏—Ä–∏–Ω–∞ –ª–±–∞
    'temple_width': [0, 16],     # –®–∏—Ä–∏–Ω–∞ –≤–∏—Å–∫–æ–≤
    'cheekbone_width': [1, 15],  # –®–∏—Ä–∏–Ω–∞ —Å–∫—É–ª
    'jaw_width': [5, 11],        # –®–∏—Ä–∏–Ω–∞ —á–µ–ª—é—Å—Ç–∏
    'chin_width': [6, 10]        # –®–∏—Ä–∏–Ω–∞ –ø–æ–¥–±–æ—Ä–æ–¥–∫–∞
}

FACIAL_PROPORTIONS = {
    'eye_distance': [36, 45],           # –ú–µ–∂–∑—Ä–∞—á–∫–æ–≤–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    'nose_width': [31, 35],             # –®–∏—Ä–∏–Ω–∞ –Ω–æ—Å–∞
    'mouth_width': [48, 54],            # –®–∏—Ä–∏–Ω–∞ —Ä—Ç–∞
    'face_height': [27, 8],             # –í—ã—Å–æ—Ç–∞ –ª–∏—Ü–∞
    'nose_height': [27, 33],            # –í—ã—Å–æ—Ç–∞ –Ω–æ—Å–∞
    'upper_face_height': [19, 33],      # –í—ã—Å–æ—Ç–∞ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ –ª–∏—Ü–∞
    'lower_face_height': [33, 8]        # –í—ã—Å–æ—Ç–∞ –Ω–∏–∂–Ω–µ–π —á–∞—Å—Ç–∏ –ª–∏—Ü–∞
}

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ 15 –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
SKULL_METRICS = [
    'forehead_width_ratio',
    'temple_width_ratio', 
    'cheekbone_width_ratio',
    'jaw_width_ratio',
    'chin_width_ratio',
    'skull_length_ratio',
    'cranial_vault_ratio'
]

PROPORTION_METRICS = [
    'eye_distance_ratio',
    'nose_width_ratio',
    'mouth_width_ratio',
    'face_height_ratio',
    'nose_height_ratio',
    'upper_face_ratio',
    'lower_face_ratio',
    'facial_index'
]

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫
METRIC_RANGES = {
    'forehead_width_ratio': (0.8, 1.2),
    'temple_width_ratio': (0.85, 1.15),
    'cheekbone_width_ratio': (0.9, 1.1),
    'jaw_width_ratio': (0.85, 1.15),
    'chin_width_ratio': (0.8, 1.2),
    'skull_length_ratio': (0.9, 1.1),
    'cranial_vault_ratio': (0.85, 1.15),
    'eye_distance_ratio': (0.9, 1.1),
    'nose_width_ratio': (0.8, 1.2),
    'mouth_width_ratio': (0.85, 1.15),
    'face_height_ratio': (0.9, 1.1),
    'nose_height_ratio': (0.85, 1.15),
    'upper_face_ratio': (0.9, 1.1),
    'lower_face_ratio': (0.9, 1.1),
    'facial_index': (0.8, 1.2)
}

# === –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===

@dataclass
class IdentityMetrics:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è 15 –º–µ—Ç—Ä–∏–∫ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏"""
    image_id: str
    filepath: str
    
    # –ö–æ—Å—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (7 —à—Ç—É–∫)
    forehead_width_ratio: float = 0.0
    temple_width_ratio: float = 0.0
    cheekbone_width_ratio: float = 0.0
    jaw_width_ratio: float = 0.0
    chin_width_ratio: float = 0.0
    skull_length_ratio: float = 0.0
    cranial_vault_ratio: float = 0.0
    
    # –ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (8 —à—Ç—É–∫)
    eye_distance_ratio: float = 0.0
    nose_width_ratio: float = 0.0
    mouth_width_ratio: float = 0.0
    face_height_ratio: float = 0.0
    nose_height_ratio: float = 0.0
    upper_face_ratio: float = 0.0
    lower_face_ratio: float = 0.0
    facial_index: float = 0.0
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    pose_category: str = "frontal"
    confidence_score: float = 0.0
    normalization_factor: float = 1.0
    baseline_reference: Optional[str] = None
    
    # –§–ª–∞–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_flags: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    processing_time_ms: float = 0.0
    calculation_method: str = "standard"
    
    def to_array(self) -> np.ndarray:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –≤ numpy –º–∞—Å—Å–∏–≤"""
        return np.array([
            self.forehead_width_ratio, self.temple_width_ratio, self.cheekbone_width_ratio,
            self.jaw_width_ratio, self.chin_width_ratio, self.skull_length_ratio,
            self.cranial_vault_ratio, self.eye_distance_ratio, self.nose_width_ratio,
            self.mouth_width_ratio, self.face_height_ratio, self.nose_height_ratio,
            self.upper_face_ratio, self.lower_face_ratio, self.facial_index
        ])
    
    def get_skull_metrics(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫–æ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        return {
            'forehead_width_ratio': self.forehead_width_ratio,
            'temple_width_ratio': self.temple_width_ratio,
            'cheekbone_width_ratio': self.cheekbone_width_ratio,
            'jaw_width_ratio': self.jaw_width_ratio,
            'chin_width_ratio': self.chin_width_ratio,
            'skull_length_ratio': self.skull_length_ratio,
            'cranial_vault_ratio': self.cranial_vault_ratio
        }
    
    def get_proportion_metrics(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        return {
            'eye_distance_ratio': self.eye_distance_ratio,
            'nose_width_ratio': self.nose_width_ratio,
            'mouth_width_ratio': self.mouth_width_ratio,
            'face_height_ratio': self.face_height_ratio,
            'nose_height_ratio': self.nose_height_ratio,
            'upper_face_ratio': self.upper_face_ratio,
            'lower_face_ratio': self.lower_face_ratio,
            'facial_index': self.facial_index
        }

@dataclass
class BaselineMetrics:
    """–ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
    reference_period: str
    mean_values: Dict[str, float]
    std_values: Dict[str, float]
    sample_count: int
    creation_date: datetime.datetime
    confidence_level: float

@dataclass
class MetricsComparison:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
    similarity_score: float
    distance_euclidean: float
    distance_cosine: float
    outlier_metrics: List[str]
    consistency_score: float
    temporal_trend: Optional[str] = None

# === –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ö–ê–õ–¨–ö–£–õ–Ø–¢–û–†–ê –ú–ï–¢–†–ò–ö ===

class MetricsCalculator:
    """–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è 15 –º–µ—Ç—Ä–∏–∫ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ª–∏—Ü–∞"""
    
    def __init__(self):
        self.config = get_config()
        self.cache_dir = Path("./cache/metrics")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.metrics_cache: Dict[str, IdentityMetrics] = {}
        self.baseline_cache: Dict[str, BaselineMetrics] = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.processing_stats = {
            'total_calculated': 0,
            'successful_calculations': 0,
            'failed_calculations': 0,
            'cache_hits': 0,
            'baseline_normalizations': 0
        }
        
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.calculation_lock = threading.Lock()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ baseline –º–µ—Ç—Ä–∏–∫
        self._load_baseline_metrics()
        
        logger.info("MetricsCalculator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _load_baseline_metrics(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        try:
            baseline_path = self.cache_dir / "baseline_metrics.pkl"
            if baseline_path.exists():
                with open(baseline_path, 'rb') as f:
                    self.baseline_cache = pickle.load(f)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.baseline_cache)} baseline –º–µ—Ç—Ä–∏–∫")
            else:
                logger.info("Baseline –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ baseline –º–µ—Ç—Ä–∏–∫: {e}")
            self.baseline_cache = {}

    def calculate_identity_signature_metrics(self, landmarks_package) -> Optional[IdentityMetrics]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á–µ—Ç–∞ 15 –º–µ—Ç—Ä–∏–∫ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
        
        Args:
            landmarks_package: –ü–∞–∫–µ—Ç —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –ª–∞–Ω–¥–º–∞—Ä–∫–∞–º–∏
            
        Returns:
            –û–±—ä–µ–∫—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            start_time = time.time()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if landmarks_package is None or landmarks_package.normalized_landmarks is None:
                logger.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ª–∞–Ω–¥–º–∞—Ä–∫–∏")
                self.processing_stats['failed_calculations'] += 1
                return None
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            cache_key = landmarks_package.image_id
            if cache_key in self.metrics_cache:
                self.processing_stats['cache_hits'] += 1
                cached_result = self.metrics_cache[cache_key]
                cached_result.processing_time_ms = (time.time() - start_time) * 1000
                return cached_result
            
            landmarks = landmarks_package.normalized_landmarks
            pose_category = landmarks_package.pose_category
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤
            if len(landmarks) < 68:
                logger.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤: {len(landmarks)}")
                self.processing_stats['failed_calculations'] += 1
                return None
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –º–µ—Ç—Ä–∏–∫
            metrics = IdentityMetrics(
                image_id=landmarks_package.image_id,
                filepath=landmarks_package.filepath,
                pose_category=pose_category
            )
            
            # –†–∞—Å—á–µ—Ç –∫–æ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (7 —à—Ç—É–∫)
            self._calculate_skull_metrics(landmarks, metrics)
            
            # –†–∞—Å—á–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (8 —à—Ç—É–∫)
            self._calculate_proportion_metrics(landmarks, metrics)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ baseline
            self._normalize_by_baseline(metrics, pose_category)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self._validate_metrics(metrics)
            
            # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            metrics.confidence_score = self._calculate_confidence_score(metrics)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            metrics.processing_time_ms = (time.time() - start_time) * 1000
            metrics.calculation_method = "normalized_landmarks"
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            self.metrics_cache[cache_key] = metrics
            
            self.processing_stats['successful_calculations'] += 1
            self.processing_stats['total_calculated'] += 1
            
            logger.debug(f"–ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∑–∞ {metrics.processing_time_ms:.1f}–º—Å")
            return metrics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫: {e}")
            self.processing_stats['failed_calculations'] += 1
            self.processing_stats['total_calculated'] += 1
            return None

    def _calculate_skull_metrics(self, landmarks: np.ndarray, metrics: IdentityMetrics):
        """–†–∞—Å—á–µ—Ç 7 –∫–æ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        try:
            # 1. –®–∏—Ä–∏–Ω–∞ –ª–±–∞ (forehead_width_ratio)
            forehead_points = [landmarks[i] for i in SKULL_LANDMARKS['forehead_width']]
            forehead_width = self._calculate_distance_2d(forehead_points[0], forehead_points[1])
            
            # 2. –®–∏—Ä–∏–Ω–∞ –≤–∏—Å–∫–æ–≤ (temple_width_ratio)
            temple_points = [landmarks[i] for i in SKULL_LANDMARKS['temple_width']]
            temple_width = self._calculate_distance_2d(temple_points[0], temple_points[1])
            
            # 3. –®–∏—Ä–∏–Ω–∞ —Å–∫—É–ª (cheekbone_width_ratio)
            cheekbone_points = [landmarks[i] for i in SKULL_LANDMARKS['cheekbone_width']]
            cheekbone_width = self._calculate_distance_2d(cheekbone_points[0], cheekbone_points[1])
            
            # 4. –®–∏—Ä–∏–Ω–∞ —á–µ–ª—é—Å—Ç–∏ (jaw_width_ratio)
            jaw_points = [landmarks[i] for i in SKULL_LANDMARKS['jaw_width']]
            jaw_width = self._calculate_distance_2d(jaw_points[0], jaw_points[1])
            
            # 5. –®–∏—Ä–∏–Ω–∞ –ø–æ–¥–±–æ—Ä–æ–¥–∫–∞ (chin_width_ratio)
            chin_points = [landmarks[i] for i in SKULL_LANDMARKS['chin_width']]
            chin_width = self._calculate_distance_2d(chin_points[0], chin_points[1])
            
            # 6. –î–ª–∏–Ω–∞ —á–µ—Ä–µ–ø–∞ (skull_length_ratio)
            skull_length = self._calculate_distance_2d(landmarks[27], landmarks[8])  # –ü–µ—Ä–µ–Ω–æ—Å–∏—Ü–∞ - –ø–æ–¥–±–æ—Ä–æ–¥–æ–∫
            
            # 7. –°–≤–æ–¥ —á–µ—Ä–µ–ø–∞ (cranial_vault_ratio)
            cranial_vault = self._calculate_cranial_vault_ratio(landmarks)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–µ–∂–∑—Ä–∞—á–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            iod = self._calculate_distance_2d(landmarks[36], landmarks[45])
            if iod > 0:
                metrics.forehead_width_ratio = forehead_width / iod
                metrics.temple_width_ratio = temple_width / iod
                metrics.cheekbone_width_ratio = cheekbone_width / iod
                metrics.jaw_width_ratio = jaw_width / iod
                metrics.chin_width_ratio = chin_width / iod
                metrics.skull_length_ratio = skull_length / iod
                metrics.cranial_vault_ratio = cranial_vault
                metrics.normalization_factor = iod
            else:
                logger.warning("–ù—É–ª–µ–≤–æ–µ –º–µ–∂–∑—Ä–∞—á–∫–æ–≤–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {e}")
            metrics.warnings.append(f"–û—à–∏–±–∫–∞ –∫–æ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {str(e)}")

    def _calculate_proportion_metrics(self, landmarks: np.ndarray, metrics: IdentityMetrics):
        """–†–∞—Å—á–µ—Ç 8 –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            iod = self._calculate_distance_2d(landmarks[36], landmarks[45])  # –ú–µ–∂–∑—Ä–∞—á–∫–æ–≤–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            
            if iod <= 0:
                logger.warning("–ù—É–ª–µ–≤–æ–µ –º–µ–∂–∑—Ä–∞—á–∫–æ–≤–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–π")
                return
            
            # 1. –ú–µ–∂–∑—Ä–∞—á–∫–æ–≤–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (eye_distance_ratio) - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫ 1.0
            metrics.eye_distance_ratio = 1.0
            
            # 2. –®–∏—Ä–∏–Ω–∞ –Ω–æ—Å–∞ (nose_width_ratio)
            nose_points = [landmarks[i] for i in FACIAL_PROPORTIONS['nose_width']]
            nose_width = self._calculate_distance_2d(nose_points[0], nose_points[1])
            metrics.nose_width_ratio = nose_width / iod
            
            # 3. –®–∏—Ä–∏–Ω–∞ —Ä—Ç–∞ (mouth_width_ratio)
            mouth_points = [landmarks[i] for i in FACIAL_PROPORTIONS['mouth_width']]
            mouth_width = self._calculate_distance_2d(mouth_points[0], mouth_points[1])
            metrics.mouth_width_ratio = mouth_width / iod
            
            # 4. –í—ã—Å–æ—Ç–∞ –ª–∏—Ü–∞ (face_height_ratio)
            face_points = [landmarks[i] for i in FACIAL_PROPORTIONS['face_height']]
            face_height = self._calculate_distance_2d(face_points[0], face_points[1])
            metrics.face_height_ratio = face_height / iod
            
            # 5. –í—ã—Å–æ—Ç–∞ –Ω–æ—Å–∞ (nose_height_ratio)
            nose_height_points = [landmarks[i] for i in FACIAL_PROPORTIONS['nose_height']]
            nose_height = self._calculate_distance_2d(nose_height_points[0], nose_height_points[1])
            metrics.nose_height_ratio = nose_height / iod
            
            # 6. –í—ã—Å–æ—Ç–∞ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ –ª–∏—Ü–∞ (upper_face_ratio)
            upper_face_points = [landmarks[i] for i in FACIAL_PROPORTIONS['upper_face_height']]
            upper_face_height = self._calculate_distance_2d(upper_face_points[0], upper_face_points[1])
            metrics.upper_face_ratio = upper_face_height / iod
            
            # 7. –í—ã—Å–æ—Ç–∞ –Ω–∏–∂–Ω–µ–π —á–∞—Å—Ç–∏ –ª–∏—Ü–∞ (lower_face_ratio)
            lower_face_points = [landmarks[i] for i in FACIAL_PROPORTIONS['lower_face_height']]
            lower_face_height = self._calculate_distance_2d(lower_face_points[0], lower_face_points[1])
            metrics.lower_face_ratio = lower_face_height / iod
            
            # 8. –õ–∏—Ü–µ–≤–æ–π –∏–Ω–¥–µ–∫—Å (facial_index)
            metrics.facial_index = face_height / cheekbone_width if hasattr(metrics, 'cheekbone_width_ratio') and metrics.cheekbone_width_ratio > 0 else face_height / iod
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {e}")
            metrics.warnings.append(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {str(e)}")

    def _calculate_distance_2d(self, point1: np.ndarray, point2: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç 2D —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏"""
        try:
            return float(np.linalg.norm(point1[:2] - point2[:2]))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è: {e}")
            return 0.0

    def _calculate_cranial_vault_ratio(self, landmarks: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å–≤–æ–¥–∞ —á–µ—Ä–µ–ø–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–∫–∏ –±—Ä–æ–≤–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—ã—Å–æ—Ç—ã —Å–≤–æ–¥–∞
            left_brow = np.mean(landmarks[22:27, :2], axis=0)
            right_brow = np.mean(landmarks[17:22, :2], axis=0)
            brow_center = (left_brow + right_brow) / 2
            
            # –¢–æ—á–∫–∞ –ø–æ–¥–±–æ—Ä–æ–¥–∫–∞
            chin = landmarks[8, :2]
            
            # –í—ã—Å–æ—Ç–∞ –æ—Ç –±—Ä–æ–≤–µ–π –¥–æ –ø–æ–¥–±–æ—Ä–æ–¥–∫–∞
            vault_height = np.linalg.norm(brow_center - chin)
            
            # –®–∏—Ä–∏–Ω–∞ –ª–∏—Ü–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∫—É–ª
            cheek_width = np.linalg.norm(landmarks[1, :2] - landmarks[15, :2])
            
            if cheek_width > 0:
                return float(vault_height / cheek_width)
            else:
                return 1.0
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å–≤–æ–¥–∞ —á–µ—Ä–µ–ø–∞: {e}")
            return 1.0

    def _normalize_by_baseline(self, metrics: IdentityMetrics, pose_category: str):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ baseline"""
        try:
            baseline_key = f"baseline_{pose_category}"
            
            if baseline_key in self.baseline_cache:
                baseline = self.baseline_cache[baseline_key]
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
                for metric_name in SKULL_METRICS + PROPORTION_METRICS:
                    current_value = getattr(metrics, metric_name, 0.0)
                    baseline_mean = baseline.mean_values.get(metric_name, current_value)
                    baseline_std = baseline.std_values.get(metric_name, 1.0)
                    
                    if baseline_std > 0:
                        # Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                        normalized_value = (current_value - baseline_mean) / baseline_std
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ ratio (–¥–æ–±–∞–≤–ª—è–µ–º 1.0 –¥–ª—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è)
                        setattr(metrics, metric_name, 1.0 + normalized_value * 0.1)
                    
                metrics.baseline_reference = baseline_key
                self.processing_stats['baseline_normalizations'] += 1
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ baseline: {e}")
            metrics.warnings.append(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")

    def _validate_metrics(self, metrics: IdentityMetrics):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        try:
            for metric_name in SKULL_METRICS + PROPORTION_METRICS:
                value = getattr(metrics, metric_name, 0.0)
                min_val, max_val = METRIC_RANGES.get(metric_name, (0.0, 2.0))
                
                if not (min_val <= value <= max_val):
                    metrics.quality_flags.append(f"out_of_range_{metric_name}")
                    metrics.warnings.append(f"–ú–µ—Ç—Ä–∏–∫–∞ {metric_name} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {value:.3f}")
                
                if np.isnan(value) or np.isinf(value):
                    metrics.quality_flags.append(f"invalid_{metric_name}")
                    metrics.warnings.append(f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {metric_name}: {value}")
                    setattr(metrics, metric_name, 1.0)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
            metrics.warnings.append(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}")

    def _calculate_confidence_score(self, metrics: IdentityMetrics) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫"""
        try:
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            total_metrics = len(SKULL_METRICS + PROPORTION_METRICS)
            invalid_metrics = len([flag for flag in metrics.quality_flags if 'invalid_' in flag])
            out_of_range_metrics = len([flag for flag in metrics.quality_flags if 'out_of_range_' in flag])
            
            # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª
            base_score = (total_metrics - invalid_metrics) / total_metrics
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ã—Ö–æ–¥ –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            range_penalty = out_of_range_metrics / total_metrics * 0.5
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
            confidence = max(0.0, base_score - range_penalty)
            
            return float(confidence)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return 0.5

    def compare_metrics(self, metrics1: IdentityMetrics, metrics2: IdentityMetrics) -> MetricsComparison:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –Ω–∞–±–æ—Ä–æ–≤ –º–µ—Ç—Ä–∏–∫
        
        Args:
            metrics1: –ü–µ—Ä–≤—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫
            metrics2: –í—Ç–æ—Ä–æ–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–∞—Å—Å–∏–≤—ã
            array1 = metrics1.to_array()
            array2 = metrics2.to_array()
            
            # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            euclidean_dist = float(euclidean(array1, array2))
            cosine_dist = float(cosine(array1, array2))
            
            # –ü–æ–∏—Å–∫ –≤—ã–±—Ä–æ—Å–æ–≤
            diff = np.abs(array1 - array2)
            outlier_threshold = np.mean(diff) + 2 * np.std(diff)
            outlier_indices = np.where(diff > outlier_threshold)[0]
            
            metric_names = SKULL_METRICS + PROPORTION_METRICS
            outlier_metrics = [metric_names[i] for i in outlier_indices if i < len(metric_names)]
            
            # –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ (–æ–±—Ä–∞—Ç–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è)
            similarity = 1.0 / (1.0 + euclidean_dist)
            
            # –†–∞—Å—á–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            consistency = 1.0 - min(1.0, len(outlier_metrics) / len(metric_names))
            
            return MetricsComparison(
                similarity_score=similarity,
                distance_euclidean=euclidean_dist,
                distance_cosine=cosine_dist,
                outlier_metrics=outlier_metrics,
                consistency_score=consistency
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")
            return MetricsComparison(
                similarity_score=0.0,
                distance_euclidean=float('inf'),
                distance_cosine=1.0,
                outlier_metrics=[],
                consistency_score=0.0
            )

    def analyze_metrics_consistency(self, metrics_list: List[IdentityMetrics]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
        
        Args:
            metrics_list: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            if len(metrics_list) < 2:
                return {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–∞—Ç—Ä–∏—Ü—É
            metrics_matrix = np.array([m.to_array() for m in metrics_list])
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
            means = np.mean(metrics_matrix, axis=0)
            stds = np.std(metrics_matrix, axis=0)
            cvs = stds / means  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
            
            metric_names = SKULL_METRICS + PROPORTION_METRICS
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            stable_metrics = []
            unstable_metrics = []
            
            for i, (name, cv) in enumerate(zip(metric_names, cvs)):
                if cv < 0.1:  # –ú–µ–Ω–µ–µ 10% –≤–∞—Ä–∏–∞—Ü–∏–∏
                    stable_metrics.append(name)
                elif cv > 0.3:  # –ë–æ–ª–µ–µ 30% –≤–∞—Ä–∏–∞—Ü–∏–∏
                    unstable_metrics.append(name)
            
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            overall_consistency = 1.0 - np.mean(cvs)
            
            # –ü–æ–∏—Å–∫ —Ç—Ä–µ–Ω–¥–æ–≤
            trends = self._analyze_temporal_trends(metrics_matrix, metric_names)
            
            return {
                'total_samples': len(metrics_list),
                'overall_consistency': float(overall_consistency),
                'stable_metrics': stable_metrics,
                'unstable_metrics': unstable_metrics,
                'mean_values': {name: float(mean) for name, mean in zip(metric_names, means)},
                'std_values': {name: float(std) for name, std in zip(metric_names, stds)},
                'cv_values': {name: float(cv) for name, cv in zip(metric_names, cvs)},
                'trends': trends
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            return {'error': str(e)}

    def _analyze_temporal_trends(self, metrics_matrix: np.ndarray, metric_names: List[str]) -> Dict[str, str]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö"""
        try:
            trends = {}
            
            for i, name in enumerate(metric_names):
                values = metrics_matrix[:, i]
                
                # –ü—Ä–æ—Å—Ç–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
                x = np.arange(len(values))
                correlation = np.corrcoef(x, values)[0, 1]
                
                if correlation > 0.3:
                    trends[name] = 'increasing'
                elif correlation < -0.3:
                    trends[name] = 'decreasing'
                else:
                    trends[name] = 'stable'
            
            return trends
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return {}

    def create_baseline_from_metrics(self, metrics_list: List[IdentityMetrics], 
                                   baseline_name: str, pose_category: str = "frontal"):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ baseline –º–µ—Ç—Ä–∏–∫ –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
        
        Args:
            metrics_list: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è baseline
            baseline_name: –ò–º—è baseline
            pose_category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–∑—ã
        """
        try:
            if len(metrics_list) < 5:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ baseline")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–∞—Ç—Ä–∏—Ü—É
            metrics_matrix = np.array([m.to_array() for m in metrics_list])
            
            # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            means = np.mean(metrics_matrix, axis=0)
            stds = np.std(metrics_matrix, axis=0)
            
            metric_names = SKULL_METRICS + PROPORTION_METRICS
            
            # –°–æ–∑–¥–∞–Ω–∏–µ baseline –æ–±—ä–µ–∫—Ç–∞
            baseline = BaselineMetrics(
                reference_period=baseline_name,
                mean_values={name: float(mean) for name, mean in zip(metric_names, means)},
                std_values={name: float(std) for name, std in zip(metric_names, stds)},
                sample_count=len(metrics_list),
                creation_date=datetime.datetime.now(),
                confidence_level=min(1.0, len(metrics_list) / 20.0)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ 20+ –æ–±—Ä–∞–∑—Ü–∞—Ö
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            baseline_key = f"baseline_{pose_category}"
            self.baseline_cache[baseline_key] = baseline
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫
            self._save_baseline_metrics()
            
            logger.info(f"–°–æ–∑–¥–∞–Ω baseline '{baseline_name}' –¥–ª—è –ø–æ–∑—ã '{pose_category}' –∏–∑ {len(metrics_list)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è baseline: {e}")

    def _save_baseline_metrics(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ baseline –º–µ—Ç—Ä–∏–∫ –Ω–∞ –¥–∏—Å–∫"""
        try:
            baseline_path = self.cache_dir / "baseline_metrics.pkl"
            with open(baseline_path, 'wb') as f:
                pickle.dump(self.baseline_cache, f)
            logger.debug("Baseline –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è baseline –º–µ—Ç—Ä–∏–∫: {e}")

    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = self.processing_stats.copy()
        
        if stats['total_calculated'] > 0:
            stats['success_rate'] = stats['successful_calculations'] / stats['total_calculated']
            stats['cache_hit_rate'] = stats['cache_hits'] / stats['total_calculated']
        else:
            stats['success_rate'] = 0.0
            stats['cache_hit_rate'] = 0.0
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ
        stats['cache_info'] = {
            'metrics_cached': len(self.metrics_cache),
            'baselines_loaded': len(self.baseline_cache)
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
        process = psutil.Process()
        memory_info = process.memory_info()
        stats['memory_usage_mb'] = memory_info.rss / (1024 * 1024)
        
        return stats

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –º–µ—Ç—Ä–∏–∫"""
        try:
            self.metrics_cache.clear()
            logger.info("–ö—ç—à –º–µ—Ç—Ä–∏–∫ –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    def export_metrics_to_csv(self, metrics_list: List[IdentityMetrics], output_path: str):
        """
        –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –≤ CSV —Ñ–∞–π–ª
        
        Args:
            metrics_list: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV
        """
        try:
            import pandas as pd
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            data = []
            for metrics in metrics_list:
                row = {
                    'image_id': metrics.image_id,
                    'filepath': metrics.filepath,
                    'pose_category': metrics.pose_category,
                    'confidence_score': metrics.confidence_score,
                    'processing_time_ms': metrics.processing_time_ms
                }
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
                for metric_name in SKULL_METRICS + PROPORTION_METRICS:
                    row[metric_name] = getattr(metrics, metric_name, 0.0)
                
                data.append(row)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            df = pd.DataFrame(data)
            df.to_csv(output_path, index=False)
            
            logger.info(f"–ú–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_path}")
            
        except ImportError:
            logger.error("Pandas –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV: {e}")

# === –§–£–ù–ö–¶–ò–ò –°–ê–ú–û–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===

def self_test():
    """–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è metrics_calculator"""
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è metrics_calculator...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞
        calculator = MetricsCalculator()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤
        test_landmarks = np.random.rand(68, 3) * 100
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–∞–∫–µ—Ç–∞ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤
        class MockLandmarksPackage:
            def __init__(self):
                self.image_id = "test_image"
                self.filepath = "test.jpg"
                self.normalized_landmarks = test_landmarks
                self.pose_category = "frontal"
        
        test_package = MockLandmarksPackage()
        
        # –¢–µ—Å—Ç —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
        metrics = calculator.calculate_identity_signature_metrics(test_package)
        assert metrics is not None, "–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã"
        assert len(metrics.to_array()) == 15, "–ù–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫"
        
        # –¢–µ—Å—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        metrics2 = calculator.calculate_identity_signature_metrics(test_package)
        comparison = calculator.compare_metrics(metrics, metrics2)
        assert comparison.similarity_score > 0.9, "–ò–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏"
        
        # –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        metrics_list = [metrics, metrics2]
        consistency = calculator.analyze_metrics_consistency(metrics_list)
        assert 'overall_consistency' in consistency, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏"
        
        # –¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = calculator.get_processing_statistics()
        assert 'success_rate' in stats, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
        
        logger.info("–°–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ metrics_calculator –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è
    success = self_test()
    if success:
        print("‚úÖ –ú–æ–¥—É–ª—å metrics_calculator —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        calculator = MetricsCalculator()
        print(f"üìä –ú–µ—Ç—Ä–∏–∫ –≤ –∫—ç—à–µ: {len(calculator.metrics_cache)}")
        print(f"üîß Baseline –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(calculator.baseline_cache)}")
        print(f"üìè –ö–æ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {len(SKULL_METRICS)}")
        print(f"üìê –ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {len(PROPORTION_METRICS)}")
        print(f"üíæ –ö—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {calculator.cache_dir}")
    else:
        print("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ –º–æ–¥—É–ª–µ metrics_calculator")
        exit(1)